# Query: from transformers
# Including: /inspire/hdd/project/embodied-multimodality/public/syfei/train_real_world/transformers_4573
# ContextLines: 1

10587 results - 3105 files

train_real_world/transformers_4573/AGENTS.md:
  23  
  24: - "Copied from" syntax. Functions or entire classes can have a comment at the top like this: `# Copied from transformers.models.llama.modeling_llama.rotate_half` or `# Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->MT5`
  25    These comments are actively checked by the style tools, and copies will automatically be updated when the base code is updated. If you need to update a copied function, you should

train_real_world/transformers_4573/awesome-transformers.md:
  156  
  157: [transformers_4573.js](https://github.com/huggingface/transformers_4573.js/) is a JavaScript library targeted at running models from transformers_4573 directly within the browser.
  158  

train_real_world/transformers_4573/conftest.py:
   26  
   27: from transformers_4573.testing_utils import (
   28      HfDoctestModule,

   33  )
   34: from transformers_4573.utils import enable_tf32
   35  

  104  def pytest_addoption(parser):
  105:     from transformers_4573.testing_utils import pytest_addoption_shared
  106  

  110  def pytest_terminal_summary(terminalreporter):
  111:     from transformers_4573.testing_utils import pytest_terminal_summary_main
  112  

train_real_world/transformers_4573/Makefile:
  60  quality:
  61: 	@python -c "from transformers_4573 import *" || (echo 'ðŸš¨ import failed, this means you introduced unprotected imports! ðŸš¨'; exit 1)
  62  	ruff check $(check_dirs) setup.py conftest.py

train_real_world/transformers_4573/MIGRATION_GUIDE_V5.md:
   90  ```python
   91: from transformers_4573 import TokenizersBackend, generate_merges
   92  from tokenizers import pre_tokenizers, Tokenizer

  177  ```py
  178: from transformers_4573 import LlamaTokenizer
  179  

  187  ```py
  188: from transformers_4573 import LlamaTokenizer
  189  

  206  ```python
  207: from transformers_4573 import AutoTokenizer
  208  tokenizer = AutoTokenizer.from_pretrained("t5-small") 

  468  ```python
  469: from transformers_4573 import AutoModelForCausalLM, BitsAndBytesConfig
  470  

train_real_world/transformers_4573/README.md:
  126  ```py
  127: from transformers_4573 import pipeline
  128  

  143  import torch
  144: from transformers_4573 import pipeline
  145  

  161  ```py
  162: from transformers_4573 import pipeline
  163  

  178  ```py
  179: from transformers_4573 import pipeline
  180  

  201  ```py
  202: from transformers_4573 import pipeline
  203  

train_real_world/transformers_4573/setup.py:
  173  #
  174: # python -c 'import sys; from transformers_4573.dependency_versions_table import deps; \
  175  # print(" ".join([ deps[x] for x in sys.argv[1:]]))' tokenizers datasets

  182  #
  183: # pip install -U $(python -c 'import sys; from transformers_4573.dependency_versions_table import deps; \
  184  # print(" ".join([deps[x] for x in sys.argv[1:]]))' tokenizers datasets)

train_real_world/transformers_4573/.circleci/config.yml:
  156                    path: ~/transformers/installed.txt
  157:             - run: python -c "from transformers import *" || (echo 'ðŸš¨ import failed, this means you introduced unprotected imports! ðŸš¨'; exit 1)
  158              - run: ruff check examples tests src utils

train_real_world/transformers_4573/.github/copilot-instructions.md:
  23  
  24: - "Copied from" syntax. Functions or entire classes can have a comment at the top like this: `# Copied from transformers.models.llama.modeling_llama.rotate_half` or `# Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->MT5`
  25    These comments are actively checked by the style tools, and copies will automatically be updated when the base code is updated. If you need to update a copied function, you should

train_real_world/transformers_4573/.github/workflows/release.yml:
  27  
  28:       - run: python -c "from transformers import *"
  29  
  30        - run: pip install -e .[torch]
  31:       - run: python -c "from transformers import pipeline; classifier = pipeline('text-classification'); assert classifier('What a nice release')[0]['score'] > 0"
  32  

train_real_world/transformers_4573/benchmark/benches/llama.py:
  32  
  33:     from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, StaticCache
  34  

train_real_world/transformers_4573/benchmark_v2/framework/benchmark_config.py:
  7  
  8: from transformers.generation.configuration_utils import CompileConfig
  9: from transformers.utils.import_utils import is_flash_attn_2_available, is_kernels_available
  10  

train_real_world/transformers_4573/benchmark_v2/framework/benchmark_runner.py:
  17  
  18: from transformers import (
  19      AutoModelForCausalLM,

  23  )
  24: from transformers.generation.streamers import BaseStreamer
  25  

train_real_world/transformers_4573/docs/README.md:
  324  
  325:     >>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
  326      >>> from datasets import load_dataset

train_real_world/transformers_4573/docs/source/ar/accelerate.md:
  52  + from accelerate import Accelerator
  53:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  54  

train_real_world/transformers_4573/docs/source/ar/autoclass_tutorial.md:
   21  ```py
   22: >>> from transformers import AutoTokenizer
   23  

   42  ```py
   43: >>> from transformers import AutoImageProcessor
   44  

   68  ```py
   69: >>> from transformers import AutoImageProcessor, AutoBackbone
   70  >>> import torch

   96  ```py
   97: >>> from transformers import AutoFeatureExtractor
   98  

  110  ```py
  111: >>> from transformers import AutoProcessor
  112  

  120  ```py
  121: >>> from transformers import AutoModelForSequenceClassification
  122  

  128  ```py
  129: >>> from transformers import AutoModelForTokenClassification
  130  

train_real_world/transformers_4573/docs/source/ar/chat_templating.md:
   11  ```python
   12: >>> from transformers import AutoTokenizer
   13  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

   27  ```python
   28: >>> from transformers import AutoTokenizer
   29  >>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

   60  ```python
   61: from transformers import AutoModelForCausalLM, AutoTokenizer
   62  

  113  ```python
  114: from transformers import pipeline
  115  

  203  ```python
  204: from transformers import AutoTokenizer
  205  from datasets import Dataset

  301  import torch
  302: from transformers import AutoModelForCausalLM, AutoTokenizer
  303  

  374  ```python
  375: from transformers.utils import get_json_schema
  376  

  468  ```python
  469: from transformers import AutoTokenizer, AutoModelForCausalLM
  470  

train_real_world/transformers_4573/docs/source/ar/conversations.md:
   25  import torch
   26: from transformers import pipeline
   27  

  118  ```python
  119: from transformers import AutoModelForCausalLM, AutoTokenizer
  120  import torch

  171  ```python
  172: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  173  

  180  ```python
  181: from transformers import pipeline, BitsAndBytesConfig
  182  

train_real_world/transformers_4573/docs/source/ar/create_a_model.md:
   18  ```py
   19: >>> from transformers import DistilBertConfig
   20  

   86  ```py
   87: >>> from transformers import DistilBertModel
   88  

  113  ```py
  114: >>> from transformers import DistilBertForSequenceClassification
  115  

  121  ```py
  122: >>> from transformers import DistilBertForQuestionAnswering
  123  

  144  ```py
  145: >>> from transformers import DistilBertTokenizer
  146  

  152  ```py
  153: >>> from transformers import DistilBertTokenizer
  154  

  160  ```py
  161: >>> from transformers import DistilBertTokenizerFast
  162  

  176  ```py
  177: >>> from transformers import ViTImageProcessor
  178  

  208  ```py
  209: >>> from transformers import ViTImageProcessor
  210  

  246  ```py
  247: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  248  

  258  ```py
  259: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  260  

  267  ```py
  268: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, ResNetConfig
  269  

  282  ```python
  283: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  284  

  291  ```python
  292: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  293  

  300  ```python
  301: from transformers import TimmBackboneConfig, TimmBackbone
  302  

  308  # Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù…ÙˆØ¯ ÙÙ‚Ø±ÙŠ timm
  309: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  310  

  321  ```py
  322: >>> from transformers import Wav2Vec2FeatureExtractor
  323  

  343  ```py
  344: >>> from transformers import Wav2Vec2FeatureExtractor
  345  

  365  ```py
  366: >>> from transformers import Wav2Vec2FeatureExtractor
  367  

  373  ```py
  374: >>> from transformers import Wav2Vec2CTCTokenizer
  375  

  381  ```py
  382: >>> from transformers import Wav2Vec2Processor
  383  

train_real_world/transformers_4573/docs/source/ar/custom_models.md:
   22  ```python
   23: from transformers import PreTrainedConfig
   24  from typing import List

   93  ```py
   94: from transformers import PreTrainedModel
   95  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  193  ```py
  194: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  195  

  308  ```py
  309: from transformers import AutoModelForImageClassification
  310  

train_real_world/transformers_4573/docs/source/ar/fast_tokenizers.md:
  27  ```python
  28: >>> from transformers import PreTrainedTokenizerFast
  29  

  45  ```python
  46: >>> from transformers import PreTrainedTokenizerFast
  47  

train_real_world/transformers_4573/docs/source/ar/gguf.md:
  68  ```py
  69: from transformers import AutoTokenizer, AutoModelForCausalLM
  70  

train_real_world/transformers_4573/docs/source/ar/glossary.md:
   17  ```python
   18: >>> from transformers import BertTokenizer
   19  

  178  ```python
  179: >>> from transformers import BertTokenizer
  180  

  395  ```python
  396: >>> from transformers import BertTokenizer
  397  

train_real_world/transformers_4573/docs/source/ar/how_to_hack_models.md:
   32  import torch.nn as nn
   33: from transformers.models.sam.modeling_sam import SamVisionAttention
   34  

   99  ```python
  100: from transformers import SamModel
  101: from transformers.models.sam import modeling_sam
  102  

train_real_world/transformers_4573/docs/source/ar/installation.md:
   71  ```bash
   72: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   73  ```

   93  ```bash
   94: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
   95  ```

   99  ```bash
  100: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  101  ```

  171  ```py
  172: from transformers import T5Model
  173  

  192      ```py
  193:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  194  

  231  ```py
  232: >>> from transformers import AutoConfig
  233  

train_real_world/transformers_4573/docs/source/ar/llm_tutorial_optimization.md:
  58  ```python
  59: from transformers import AutoModelForCausalLM
  60  

  72  ```python
  73: from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
  74  import torch

train_real_world/transformers_4573/docs/source/ar/llm_tutorial.md:
   62  ```py
   63: >>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   64  

   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  115  ```py
  116: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  117  

  147  >>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
  148: >>> from transformers import set_seed
  149  >>> set_seed(42)

train_real_world/transformers_4573/docs/source/ar/model_memory_anatomy.md:
   70  ```py
   71: >>> from transformers import AutoModelForSequenceClassification
   72  

  129  ```py
  130: >>> from transformers import TrainingArgumentsØŒ TrainerØŒ logging
  131  

train_real_world/transformers_4573/docs/source/ar/model_sharing.md:
  115  ```py
  116: >>> from transformers import AutoModel
  117  

  120  ```py
  121: >>> from transformers import AutoModel
  122  

train_real_world/transformers_4573/docs/source/ar/multilingual.md:
   26  >>> import torch
   27: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   28  

  100  ```py
  101: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  102  

  136  ```py
  137: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  138  

train_real_world/transformers_4573/docs/source/ar/peft.md:
   47  ```py
   48: from transformers import AutoModelForCausalLM, AutoTokenizer
   49  

   62  ```py
   63: from transformers import AutoModelForCausalLM, AutoTokenizer
   64  

   82  ```py
   83: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   84  

   93  ```py
   94: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
   95  from peft import LoraConfig

  133  ```py
  134: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  135  from peft import PeftConfig

  216  ```py
  217: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  218  from peft import LoraConfig

train_real_world/transformers_4573/docs/source/ar/perplexity.md:
  39  ```python
  40: from transformers import GPT2LMHeadModel, GPT2TokenizerFast
  41  

train_real_world/transformers_4573/docs/source/ar/pipeline_tutorial.md:
   21  ```py
   22: >>> from transformers import pipeline
   23  

  179  # KeyDataset Ù‡ÙŠ Ø£Ø¯Ø§Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ø³ØªÙ‚ÙˆÙ… ÙÙ‚Ø· Ø¨Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø°ÙŠ Ù†Ù‡ØªÙ… Ø¨Ù‡.
  180: from transformers.pipelines.pt_utils import KeyDataset
  181  from datasets import load_dataset

  206  ```py
  207: >>> from transformers import pipeline
  208  

  222  ```py
  223: >>> from transformers import pipeline
  224  

  241  ```py
  242: >>> from transformers import pipeline
  243  

  273  import torch
  274: from transformers import pipeline
  275  

  284  import torch
  285: from transformers import pipeline
  286  

  303  ```py
  304: from transformers import pipeline
  305  import gradio as gr

train_real_world/transformers_4573/docs/source/ar/pipeline_webserver.md:
  21  from starlette.routing import Route
  22: from transformers import pipeline
  23  import asyncio

train_real_world/transformers_4573/docs/source/ar/preprocessing.md:
   38  ```py
   39: >>> from transformers import AutoTokenizer
   40  

  234  ```py
  235: >>> from transformers import AutoFeatureExtractor
  236  

  331  ```py
  332: >>> from transformers import AutoImageProcessor
  333  

  469  ```py
  470: >>> from transformers import AutoProcessor
  471  

train_real_world/transformers_4573/docs/source/ar/quicktour.md:
   51  ```py
   52: >>> from transformers import pipeline
   53  

   76  >>> import torch
   77: >>> from transformers import pipeline
   78  

  117  ```py
  118: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  119  

  147  ```py
  148: >>> from transformers import AutoTokenizer
  149  

  192  ```py
  193: >>> from transformers import AutoModelForSequenceClassification
  194  

  251  ```py
  252: >>> from transformers import AutoModel
  253  

  265  ```py
  266: >>> from transformers import AutoConfig
  267  

  273  ```py
  274: >>> from transformers import AutoModel
  275  

  289     ```py
  290:    >>> from transformers import AutoModelForSequenceClassification
  291  

  297     ```py
  298:    >>> from transformers import TrainingArguments
  299  

  311     ```py
  312:    >>> from transformers import AutoTokenizer
  313  

  340     ```py
  341:    >>> from transformers import DataCollatorWithPadding
  342  

  348  ```py
  349: >>> from transformers import Trainer
  350  

  382     ```py
  383:    >>> from transformers import TFAutoModelForSequenceClassification
  384  

  390     ```py
  391:    >>> from transformers import AutoTokenizer
  392  

train_real_world/transformers_4573/docs/source/ar/serialization.md:
  75  ```python
  76: >>> from transformers import AutoTokenizer
  77  >>> from optimum.onnxruntime import ORTModelForQuestionAnswering

  96  >>> from optimum.onnxruntime import ORTModelForSequenceClassification
  97: >>> from transformers import AutoTokenizer
  98  

train_real_world/transformers_4573/docs/source/ar/task_summary.md:
   22  ```py
   23: >>> from transformers import pipeline
   24  

   40  ```py
   41: >>> from transformers import pipeline
   42  

   66  ```py
   67: >>> from transformers import pipeline
   68  

   90  ```py
   91: >>> from transformers import pipeline
   92  

  113  ```py
  114: >>> from transformers import pipeline
  115  

  136  ```py
  137: >>> from transformers import pipeline
  138  

  156  ```py
  157: >>> from transformers import pipeline
  158  

  175  ```py
  176: >>> from transformers import pipeline
  177  

  210  ```py
  211: >>> from transformers import pipeline
  212  

  233  ```py
  234: >>> from transformers import pipeline
  235  

  249  ```py
  250: >>> from transformers import pipeline
  251  

  266  ```py
  267: >>> from transformers import pipeline
  268  

  306  ```py
  307: >>> from transformers import pipeline
  308  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/ar/tiktoken.md:
  15  ```py
  16: from transformers import AutoTokenizer
  17  

  28  
  29: from transformers.integrations.tiktoken import convert_tiktoken_to_fast
  30  from tiktoken import get_encoding

train_real_world/transformers_4573/docs/source/ar/tokenizer_summary.md:
  69  ```py
  70: >>> from transformers import BertTokenizer
  71  

  81  ```py
  82: >>> from transformers import XLNetTokenizer
  83  

train_real_world/transformers_4573/docs/source/ar/trainer.md:
   39  ```py
   40: from transformers import TrainingArguments
   41  

   59  ```py
   60: from transformers import Trainer
   61  

  113  from torch import nn
  114: from transformers import Trainer
  115  

  134  ```py
  135: from transformers import TrainerCallback
  136  

  150  ```py
  151: from transformers import Trainer
  152  

  231  ```py
  232: from transformers import TrainingArguments, Trainer
  233  

  250  ```python
  251: from transformers import TrainingArguments
  252  

  272  ```python
  273: from transformers import TrainingArguments
  274  training_args = TrainingArguments(..., optim="adamw_torch")

  288  
  289: from transformers import Trainer
  290  trainer = Trainer(..., optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs))

  421  import datasets
  422: from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer
  423  

train_real_world/transformers_4573/docs/source/ar/training.md:
   28  ```py
   29: >>> from transformers import AutoTokenizer
   30  

   69  ```py
   70: >>> from transformers import AutoModelForSequenceClassification
   71  

   87  ```py
   88: >>> from transformers import TrainingArguments
   89  

  115  ```py
  116: >>> from transformers import TrainingArguments, Trainer
  117  

  196  ```py
  197: >>> from transformers import AutoModelForSequenceClassification
  198  

  214  ```py
  215: >>> from transformers import get_scheduler
  216  

train_real_world/transformers_4573/docs/source/ar/troubleshooting.md:
   52  ```python
   53: >>> from transformers import TFPreTrainedModel
   54  >>> from tensorflow import keras

   62  ```python
   63: >>> from transformers import TFPreTrainedModel
   64  

  111  ```python
  112: >>> from transformers import AutoModelForSequenceClassification
  113  >>> import torch

  164  ```py
  165: >>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
  166  

train_real_world/transformers_4573/docs/source/ar/tasks/language_modeling.md:
   98  ```py
   99: >>> from transformers import AutoTokenizer
  100  

  187  ```py
  188: >>> from transformers import DataCollatorForLanguageModeling
  189  

  206  ```py
  207: >>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
  208  

  275  ```py
  276: >>> from transformers import pipeline
  277  

  285  ```py
  286: >>> from transformers import AutoTokenizer
  287  

  295  ```py
  296: >>> from transformers import AutoModelForCausalLM
  297  

train_real_world/transformers_4573/docs/source/ar/tasks/masked_language_modeling.md:
   94  ```py
   95: >>> from transformers import AutoTokenizer
   96  

  182  ```py
  183: >>> from transformers import DataCollatorForLanguageModeling
  184  

  200  ```py
  201: >>> from transformers import AutoModelForMaskedLM
  202  

  271  ```py
  272: >>> from transformers import pipeline
  273  

  292  ```py
  293: >>> from transformers import AutoTokenizer
  294  

  302  ```py
  303: >>> from transformers import AutoModelForMaskedLM
  304  

train_real_world/transformers_4573/docs/source/ar/tasks/multiple_choice.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  121  >>> from dataclasses import dataclass
  122: >>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
  123  >>> from typing import Optional, Union

  194  ```py
  195: >>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer
  196  

  260  ```py
  261: >>> from transformers import AutoTokenizer
  262  

  270  ```py
  271: >>> from transformers import AutoModelForMultipleChoice
  272  

train_real_world/transformers_4573/docs/source/ar/tasks/question_answering.md:
   93  ```py
   94: >>> from transformers import AutoTokenizer
   95  

  171  ```py
  172: >>> from transformers import DefaultDataCollator
  173  

  188  ```py
  189: >>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
  190  

  257  ```py
  258: >>> from transformers import pipeline
  259  

  273  ```py
  274: >>> from transformers import AutoTokenizer
  275  

  283  >>> import torch
  284: >>> from transformers import AutoModelForQuestionAnswering
  285  

train_real_world/transformers_4573/docs/source/ar/tasks/sequence_classification.md:
   74  ```py
   75: >>> from transformers import AutoTokenizer
   76  

   96  ```py
   97: >>> from transformers import DataCollatorWithPadding
   98  

  142  ```py
  143: >>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
  144  

  215  ```py
  216: >>> from transformers import pipeline
  217  

  227  ```py
  228: >>> from transformers import AutoTokenizer
  229  

  236  ```py
  237: >>> from transformers import AutoModelForSequenceClassification
  238  

train_real_world/transformers_4573/docs/source/ar/tasks/summarization.md:
   87  ```py
   88: >>> from transformers import AutoTokenizer
   89  

  122  ```py
  123: >>> from transformers import DataCollatorForSeq2Seq
  124  

  170  ```py
  171: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  172  

  235  ```py
  236: >>> from transformers import pipeline
  237  

  247  ```py
  248: >>> from transformers import AutoTokenizer
  249  

  256  ```py
  257: >>> from transformers import AutoModelForSeq2SeqLM
  258  

train_real_world/transformers_4573/docs/source/ar/tasks/token_classification.md:
   99  ```py
  100: >>> from transformers import AutoTokenizer
  101  

  154  ```py
  155: >>> from transformers import DataCollatorForTokenClassification
  156  

  246  ```py
  247: >>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
  248  

  313  ```py
  314: >>> from transformers import pipeline
  315  

  354  ```py
  355: >>> from transformers import AutoTokenizer
  356  

  363  ```py
  364: >>> from transformers import AutoModelForTokenClassification
  365  

train_real_world/transformers_4573/docs/source/ar/tasks/translation.md:
   83  ```py
   84: >>> from transformers import AutoTokenizer
   85  

  117  ```py
  118: >>> from transformers import DataCollatorForSeq2Seq
  119  

  177  ```py
  178: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  179  

  242  ```py
  243: >>> from transformers import pipeline
  244  

  257  ```py
  258: >>> from transformers import AutoTokenizer
  259  

  266  ```py
  267: >>> from transformers import AutoModelForSeq2SeqLM
  268  

train_real_world/transformers_4573/docs/source/de/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/de/add_new_model.md:
  494  ```python
  495: from transformers import BrandNewBertModel, BrandNewBertConfig
  496  

  788  ```python
  789: from transformers import BrandNewBertTokenizer
  790  

train_real_world/transformers_4573/docs/source/de/add_new_pipeline.md:
   33  ```python
   34: from transformers import Pipeline
   35  

  123  ```python
  124: from transformers.pipelines import PIPELINE_REGISTRY
  125  

  152  
  153: from transformers import Pipeline
  154  

  190  from pair_classification import PairClassificationPipeline
  191: from transformers.pipelines import PIPELINE_REGISTRY
  192: from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
  193  

  205  ```py
  206: from transformers import pipeline
  207  

  222  ```py
  223: from transformers import pipeline
  224  

train_real_world/transformers_4573/docs/source/de/autoclass_tutorial.md:
  40  ```py
  41: >>> from transformers import AutoTokenizer
  42  

  62  ```py
  63: >>> from transformers import AutoFeatureExtractor
  64  

  76  ```py
  77: >>> from transformers import AutoProcessor
  78  

  86  ```py
  87: >>> from transformers import AutoModelForSequenceClassification
  88  

  94  ```py
  95: >>> from transformers import AutoModelForTokenClassification
  96  

train_real_world/transformers_4573/docs/source/de/index.md:
  121  1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.
  122: 1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.
  123  1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.

train_real_world/transformers_4573/docs/source/de/installation.md:
   79  ```bash
   80: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   81  ```

  102  ```bash
  103: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  104  ```

  195      ```py
  196:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  197  

  234  ```py
  235: >>> from transformers import AutoConfig
  236  

train_real_world/transformers_4573/docs/source/de/llm_tutorial.md:
   80  ```py
   81: >>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   82  

   97  ```py
   98: >>> from transformers import AutoTokenizer
   99  

  121  ```py
  122: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  123  

  155  >>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
  156: >>> from transformers import set_seed
  157  >>> set_seed(0)

train_real_world/transformers_4573/docs/source/de/model_sharing.md:
  129  ```py
  130: >>> from transformers import AutoModel
  131  

train_real_world/transformers_4573/docs/source/de/peft.md:
   59  ```py
   60: from transformers import AutoModelForCausalLM, AutoTokenizer
   61  

   74  ```py
   75: from transformers import AutoModelForCausalLM, AutoTokenizer
   76  

   88  ```py
   89: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   90  

   99  ```py
  100: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  101  from peft import PeftConfig

  139  ```py
  140: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  141  from peft import PeftConfig

train_real_world/transformers_4573/docs/source/de/pipeline_tutorial.md:
   37  ```py
   38: >>> from transformers import pipeline
   39  

   76  ```py
   77: >>> from transformers import AutoTokenizer, AutoModelForCausalLM
   78  

   85  ```py
   86: >>> from transformers import pipeline
   87  

  117  ```py
  118: >>> from transformers import pipeline
  119  

  142  ```py
  143: >>> from transformers import pipeline
  144  

  167  ```py
  168: >>> from transformers import pipeline
  169  

train_real_world/transformers_4573/docs/source/de/pr_checks.md:
  158  ```py
  159: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  160  ```

  164  ```py
  165: # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights
  166  ```

  170  ```py
  171: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta
  172  ```

  178  ```py
  179: # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT
  180  ```

  192  ```py
  193: # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing
  194  ```

train_real_world/transformers_4573/docs/source/de/preprocessing.md:
   45  ```py
   46: >>> from transformers import AutoTokenizer
   47  

  247  ```py
  248: >>> from transformers import AutoFeatureExtractor
  249  

  331  ```py
  332: >>> from transformers import AutoImageProcessor
  333  

  453  ```py
  454: >>> from transformers import AutoProcessor
  455  

train_real_world/transformers_4573/docs/source/de/quicktour.md:
   76  ```py
   77: >>> from transformers import pipeline
   78  

  108  >>> import torch
  109: >>> from transformers import pipeline
  110  

  149  ```py
  150: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  151  

  179  ```py
  180: >>> from transformers import AutoTokenizer
  181  

  222  ```py
  223: >>> from transformers import AutoModelForSequenceClassification
  224  

  287  ```py
  288: >>> from transformers import AutoModel
  289  

  300  ```py
  301: >>> from transformers import AutoConfig
  302  

  308  ```py
  309: >>> from transformers import AutoModel
  310  

train_real_world/transformers_4573/docs/source/de/testing.md:
   203      >>> import torch
   204:     >>> from transformers import WhisperModel, WhisperFeatureExtractor
   205      >>> from datasets import load_dataset

   501  ```python
   502: from transformers.testing_utils import get_gpu_count
   503  

   731  ```python
   732: from transformers.testing_utils import TestCasePlus
   733  

   743  ```python
   744: from transformers.testing_utils import TestCasePlus
   745  

   766  ```python
   767: from transformers.testing_utils import TestCasePlus
   768  

   827  import os
   828: from transformers.testing_utils import ExtendSysPath
   829  

   935  ```python no-style
   936: from transformers.testing_utils import slow
   937  @slow

  1079  ```python
  1080: from transformers.testing_utils import CaptureStdout
  1081  

  1089  ```python
  1090: from transformers.testing_utils import CaptureStdout
  1091  

  1101  ```python
  1102: from transformers.testing_utils import CaptureStderr
  1103  

  1111  ```python
  1112: from transformers.testing_utils import CaptureStd
  1113  

  1127  ```python
  1128: from transformers import logging
  1129: from transformers.testing_utils import CaptureLogger
  1130  

  1144  ```python
  1145: from transformers.testing_utils import mockenv
  1146  

  1157  ```python
  1158: from transformers.testing_utils import TestCasePlus
  1159  

train_real_world/transformers_4573/docs/source/de/training.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   84  ```py
   85: >>> from transformers import AutoModelForSequenceClassification
   86  

  103  ```py
  104: >>> from transformers import TrainingArguments
  105  

  131  ```py
  132: >>> from transformers import TrainingArguments, Trainer
  133  

  214  ```py
  215: >>> from transformers import AutoModelForSequenceClassification
  216  

  232  ```py
  233: >>> from transformers import get_scheduler
  234  

train_real_world/transformers_4573/docs/source/en/accelerate.md:
  68  ```py
  69: from transformers import TrainingArguments, Trainer
  70  

train_real_world/transformers_4573/docs/source/en/add_new_model.md:
  303  
  304: At this point, your code doesn't have to be clean or even fully correct, It is more efficient to quickly create a first draft and then iteratively improve on it. The most important thing is that your model can be instantiated from Transformers. The command below creates a model from the configuration with random weights, verifying that the `__init__` method works.
  305  
  306  ```py
  307: from transformers import BrandNewLlama, BrandNewLlamaConfig
  308  model = BrandNewLlama(BrandNewLlamaConfig())

  534  ```py
  535: from transformers import BrandNewLlamaTokenizer
  536  

train_real_world/transformers_4573/docs/source/en/add_new_pipeline.md:
   34  ```py
   35: from transformers import Pipeline
   36  

   89  ```py
   90: from transformers import pipeline
   91  

  107  ```py
  108: from transformers.pipelines import PIPELINE_REGISTRY
  109: from transformers import AutoModelForSequenceClassification
  110  

  133  import numpy as np
  134: from transformers import Pipeline
  135  

  168  from pair_classification import PairClassificationPipeline
  169: from transformers.pipelines import PIPELINE_REGISTRY
  170: from transformers import AutoModelForSequenceClassification
  171  

  194  ```py
  195: from transformers import pipeline
  196  

  203  ```py
  204: from transformers import pipeline
  205  

train_real_world/transformers_4573/docs/source/en/assisted_decoding.md:
   34  import torch
   35: from transformers import AutoModelForCausalLM, AutoTokenizer
   36  

   50  import torch
   51: from transformers import pipeline
   52  

   67  ```py
   68: from transformers import AutoModelForCausalLM, AutoTokenizer
   69  

   93  import torch
   94: from transformers import AutoModelForCausalLM, AutoTokenizer
   95  

  109  import torch
  110: from transformers import AutoModelForCausalLM, AutoTokenizer
  111  

  133  import torch
  134: from transformers import AutoModelForCausalLM, AutoTokenizer
  135  

  151  import torch
  152: from transformers import AutoModelForCausalLM, AutoTokenizer
  153  

train_real_world/transformers_4573/docs/source/en/attention_interface.md:
   37  import torch
   38: from transformers import AutoModelForCausalLM
   39  

   58  import torch
   59: from transformers import AutoModelForCausalLM
   60  

   74  from torch.nn.attention import SDPBackend, sdpa_kernel
   75: from transformers import AutoModelForCausalLM
   76  

   91  ```py
   92: from transformers import AutoModelForImageTextToText
   93  

  135  import torch
  136: from transformers import AutoModelForCausalLM, AttentionInterface
  137: from transformers.integrations.sdpa_attention import sdpa_attention_forward
  138  

  152  import torch
  153: from transformers import AutoModelForCausalLM, AttentionInterface
  154: from transformers.integrations.sdpa_attention import sdpa_attention_forward
  155  

  182  import torch
  183: from transformers import AttentionMaskInterface
  184: from transformers.masking_utils import sdpa_mask
  185  

train_real_world/transformers_4573/docs/source/en/auto_docstring.md:
   38  ```python
   39: from transformers.modeling_utils import PreTrainedModel
   40  from ...utils import auto_docstring

  284  
  285: 5. Using a templating system to allow predefined docstrings to include dynamic information from Transformers' [auto_modules](https://github.com/huggingface/transformers/tree/main/src/transformers/models/auto) such as `{{processor_class}}` and `{{config_class}}`.
  286  

train_real_world/transformers_4573/docs/source/en/backbones.md:
   27  ```py
   28: from transformers import AutoBackbone
   29  

   59  ```py
   60: from transformers import AutoImageProcessor, AutoBackbone
   61  

   74  ```py
   75: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
   76  

   83  ```py
   84: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, ResNetConfig
   85  

  103  ```py
  104: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  105  

  112  ```py
  113: from transformers import TimmBackboneConfig
  114  

  120  ```py
  121: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation
  122  

  133  ```py
  134: from transformers import AutoImageProcessor, AutoBackbone
  135  import torch

train_real_world/transformers_4573/docs/source/en/cache_explanation.md:
  100  import torch
  101: from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
  102  from accelerate import Accelerator

  146  import torch
  147: from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
  148  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/chat_extras.md:
   65  import torch
   66: from transformers import AutoModelForCausalLM, AutoTokenizer
   67  

  140  ```py
  141: from transformers.utils import get_json_schema
  142  

train_real_world/transformers_4573/docs/source/en/chat_templating_multimodal.md:
   51  import torch
   52: from transformers import pipeline
   53  

   70  ```python
   71: from transformers import AutoProcessor, AutoModelForImageTextToText
   72  

  123  ```python
  124: from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
  125  

  171  # Make sure a video backend library (pyav, decord, or torchvision) is available.
  172: from transformers.video_utils import load_video
  173  

train_real_world/transformers_4573/docs/source/en/chat_templating.md:
   34  ```py
   35: from transformers import AutoTokenizer
   36  

   54  ```py
   55: from transformers import AutoTokenizer
   56  

   87  import torch
   88: from transformers import AutoModelForCausalLM, AutoTokenizer
   89  

  200  ```py
  201: from transformers import AutoTokenizer
  202  from datasets import Dataset

train_real_world/transformers_4573/docs/source/en/continuous_batching.md:
   29  import torch
   30: from transformers import AutoModelForCausalLM, AutoTokenizer
   31: from transformers.generation import GenerationConfig
   32  

   71  ```py
   72: from transformers.generation.continuous_batching import RequestStatus
   73  

  128  import torch
  129: from transformers import AutoModelForCausalLM
  130  

  145  ```py
  146: from transformers import AutoConfig
  147  

train_real_world/transformers_4573/docs/source/en/conversations.md:
   70  import torch
   71: from transformers import pipeline
   72  

  106  ```py
  107: from transformers import pipeline, BitsAndBytesConfig
  108  

train_real_world/transformers_4573/docs/source/en/custom_models.md:
   40  ```py
   41: from transformers import PreTrainedConfig
   42  from typing import List

  102  ```py
  103: from transformers import PreTrainedModel
  104  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  199  ```py
  200: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  201  

  217  > [!WARNING]
  218: > When copying a Transformers' model file, replace all relative imports at the top of the `modeling.py` file to import from Transformers instead.
  219  

train_real_world/transformers_4573/docs/source/en/debugging.md:
  175  ```py
  176: from transformers import TrainingArguments
  177  

  187  ```py
  188: from transformers.debug_utils import DebugUnderflowOverflow
  189  

  316  ```py
  317: from transformers.debug_utils import DebugUnderflowOverflow
  318  

train_real_world/transformers_4573/docs/source/en/deepspeed.md:
    57  ```bash
    58: $ python -c 'from transformers import AutoModel; \
    59  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

   251  ```py
   252: from transformers import T5ForConditionalGeneration, T5Config
   253  import deepspeed

   265  ```py
   266: from transformers import AutoModel, Trainer, TrainingArguments
   267  

   432  ```py
   433: from transformers import DataCollatorForLanguageModeling
   434  

   978  ```py
   979: from transformers.trainer_utils import get_last_checkpoint
   980  from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

  1008  ```py
  1009: from transformers.integrations import HfDeepSpeedConfig
  1010: from transformers import AutoModel
  1011  import deepspeed

  1026  ```py
  1027: from transformers.integrations import HfDeepSpeedConfig
  1028: from transformers import AutoModel, AutoConfig
  1029  import deepspeed

train_real_world/transformers_4573/docs/source/en/expert_parallelism.md:
  28  import torch
  29: from transformers import AutoModelForCausalLM, AutoTokenizer
  30: from transformers.distributed.configuration_utils import DistributedConfig
  31  

train_real_world/transformers_4573/docs/source/en/fast_tokenizers.md:
   28  ```py
   29: from transformers import AutoTokenizer
   30  

   59  ```py
   60: from transformers import AutoTokenizer
   61  

   72  ```py
   73: from transformers import AutoTokenizer
   74  

   86  ```py
   87: from transformers import GemmaTokenizer
   88  

   95  ```py
   96: from transformers import GemmaTokenizerFast
   97  

  104  ```py
  105: from transformers import GemmaTokenizerFast
  106  

  176  ```py
  177: from transformers import PreTrainedTokenizerFast
  178  

  184  ```py
  185: from transformers import PreTrainedTokenizerFast
  186  

  198  ```py
  199: from transformers import AutoTokenizer
  200  

  210  ```py
  211: from transformers.integrations.tiktoken import convert_tiktoken_to_fast
  212  from tiktoken import get_encoding

  231  ```py
  232: from transformers import AutoTokenizer
  233  

train_real_world/transformers_4573/docs/source/en/feature_extractors.md:
  25  ```py
  26: from transformers import AutoFeatureExtractor
  27  

  56  ```py
  57: from transformers import AutoFeatureExtractor
  58  

  69  ```py
  70: from transformers import WhisperFeatureExtractor
  71  

  91  from datasets import load_dataset, Audio
  92: from transformers import AutoFeatureExtractor
  93  

train_real_world/transformers_4573/docs/source/en/generation_features.md:
  36  ```py
  37: from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
  38  

  60  ```py
  61: from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig
  62  

train_real_world/transformers_4573/docs/source/en/generation_strategies.md:
   34  import torch
   35: from transformers import AutoModelForCausalLM, AutoTokenizer
   36  from accelerate import Accelerator

   57  import torch
   58: from transformers import AutoModelForCausalLM, AutoTokenizer
   59  from accelerate import Accelerator

   83  import torch
   84: from transformers import AutoModelForCausalLM, AutoTokenizer
   85  from accelerate import Accelerator

  112  ```py
  113: from transformers import AutoModelForCausalLM, AutoTokenizer
  114  

  132  ```py
  133: from transformers import AutoModelForCausalLM, AutoTokenizer
  134  

  199  ```py
  200: from transformers import AutoModelForCausalLM, AutoTokenizer
  201  

  313  > [!TIP]
  314: > If you publish a `custom_generate` repository, your `generate` implementation can itself define a callable and pass it to `model.generate()`. This lets you customize the decoding loop while still benefiting from Transformers' built-in input preparation logic.
  315  

train_real_world/transformers_4573/docs/source/en/gguf.md:
  36  import torch
  37: from transformers import AutoTokenizer, AutoModelForCausalLM
  38  

train_real_world/transformers_4573/docs/source/en/glossary.md:
   34  ```python
   35: >>> from transformers import BertTokenizer
   36  

  212  ```python
  213: >>> from transformers import BertTokenizer
  214  

  467  ```python
  468: >>> from transformers import BertTokenizer
  469  

train_real_world/transformers_4573/docs/source/en/how_to_hack_models.md:
   25  > ```py
   26: > from transformers import AutoModel
   27: > from transformers.utils.import_utils import clear_import_cache
   28  >

   45  import torch.nn as nn
   46: from transformers.models.sam.modeling_sam import SamVisionAttention
   47  

  110  ```py
  111: from transformers import SamModel
  112  

train_real_world/transformers_4573/docs/source/en/hpo_train.md:
  50  ```py
  51: from transformers import Trainer
  52  

train_real_world/transformers_4573/docs/source/en/image_processors.md:
   26  ```py
   27: from transformers import AutoImageProcessor
   28  

   63  ```py
   64: from transformers import AutoImageProcessor
   65  

   76  ```py
   77: from transformers import ViTImageProcessor
   78  

   84  ```py
   85: from transformers import ViTImageProcessorFast
   86  

   97  ```py
   98: from transformers import AutoImageProcessor
   99  

  106  from torchvision.io import read_image
  107: from transformers import DetrImageProcessorFast
  108  

train_real_world/transformers_4573/docs/source/en/installation.md:
   68  ```bash
   69: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))"
   70  [{'label': 'POSITIVE', 'score': 0.9998704791069031}]

   87  ```bash
   88: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))"
   89  [{'label': 'POSITIVE', 'score': 0.9998704791069031}]

  160  ```py
  161: from transformers import LlamaForCausalLM
  162  

train_real_world/transformers_4573/docs/source/en/kv_cache.md:
   42  import torch
   43: from transformers import AutoTokenizer, AutoModelForCausalLM
   44  

   57  import torch
   58: from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
   59  

   79  import torch
   80: from transformers import AutoTokenizer, AutoModelForCausalLM
   81  

  110  import torch
  111: from transformers import AutoTokenizer, AutoModelForCausalLM
  112  

  126  import torch
  127: from transformers import AutoTokenizer, AutoModelForCausalLM
  128  from accelerate import Accelerator

  172  import torch
  173: from transformers import AutoTokenizer, AutoModelForCausalLM, QuantizedCache
  174  

  187  import torch
  188: from transformers import AutoTokenizer, AutoModelForCausalLM
  189  

  222  import torch
  223: from transformers import AutoTokenizer,AutoModelForCausalLM, DynamicCache, StaticCache
  224  

  251  import torch
  252: from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache
  253  

train_real_world/transformers_4573/docs/source/en/llm_tutorial_optimization.md:
  70  ```python
  71: from transformers import AutoModelForCausalLM
  72  

  84  ```python
  85: from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
  86  import torch

train_real_world/transformers_4573/docs/source/en/llm_tutorial.md:
   47  ```py
   48: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   49  

   78  ```py
   79: from transformers import AutoModelForCausalLM
   80  

  108  ```py
  109: from transformers import AutoModelForCausalLM, GenerationConfig
  110  

  125  ```py
  126: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig
  127  

  261  ```py
  262: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  263  

train_real_world/transformers_4573/docs/source/en/model_memory_anatomy.md:
   97  ```py
   98: >>> from transformers import AutoModelForSequenceClassification
   99  

  163  ```py
  164: >>> from transformers import TrainingArguments, Trainer, logging
  165  

train_real_world/transformers_4573/docs/source/en/model_sharing.md:
  20  
  21: This guide will show you how to share a model to the Hub from Transformers.
  22  

  88  ```py
  89: from transformers import TrainingArguments, Trainer
  90  

train_real_world/transformers_4573/docs/source/en/models.md:
   26  ```py
   27: from transformers import AutoModelForCausalLM
   28  

   65  ```py
   66: from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering
   67  

   76  ```py
   77: from transformers import AutoModelForCausalLM
   78  

   92  ```py
   93: from transformers import LlamaModel, LlamaForCausalLM
   94  

  176  ```py
  177: from transformers import AutoModelForCausalLM
  178  

  198  import torch
  199: from transformers import AutoModelForCausalLM
  200  

  211  import torch
  212: from transformers import AutoConfig, AutoModel
  213  

  219  
  220: Custom models builds on Transformers' configuration and modeling classes, supports the [AutoClass](#autoclass) API, and are loaded with [`~PreTrainedModel.from_pretrained`]. The difference is that the modeling code is *not* from Transformers.
  221  

  226  ```py
  227: from transformers import AutoModelForImageClassification
  228  

train_real_world/transformers_4573/docs/source/en/modular_transformers.md:
  575  ```py
  576: We detected multiple prefix names when inheriting from transformers.models.llama.modeling_llama: ('Emu3Text', 'Emu3'). We will only use the most used 'Emu3' prefix when grabbing args and dependencies. Make sure to subclass the intermediate classes with the prefix you want (if different from 'Emu3') or use a single prefix in all the modular (best).
  577  ```

train_real_world/transformers_4573/docs/source/en/optimization_overview.md:
   45  import torch
   46: from transformers import AutoTokenizer, AutoModelForCausalLM
   47  

   65  ```py
   66: from transformers import AutoModelForCausalLM
   67  

   78  import torch
   79: from transformers import AutoModelForCausalLM
   80  

   93  import torch
   94: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   95  

  111  import torch
  112: from transformers import AutoTokenizer, AutoModelForCausalLM
  113  

  129  import torch
  130: from transformers import AutoModelForCausalLM, AutoTokenizer
  131  

  143  import torch
  144: from transformers import AutoModelForCausalLM, AutoTokenizer
  145: from transformers.generation import GenerationConfig
  146  

train_real_world/transformers_4573/docs/source/en/optimizers.md:
   24  import torch
   25: from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer
   26  

   53  import torch
   54: from transformers import TrainingArguments
   55  

   85  ```py
   86: from transformers import TrainingArguments
   87  

  107  import torch
  108: from transformers import TrainingArguments
  109  

train_real_world/transformers_4573/docs/source/en/peft.md:
   45  from peft import LoraConfig, TaskType, get_peft_model
   46: from transformers import AutoModelForCausalLM
   47  

   70  ```py
   71: from transformers import AutoModelForCausalLM
   72  from peft import LoraConfig

   95  ```py
   96: from transformers import AutoModelForCausalLM
   97  

  104  ```py
  105: from transformers import AutoModelForCausalLM
  106  

  118  ```py
  119: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  120  

train_real_world/transformers_4573/docs/source/en/perf_infer_gpu_multi.md:
   53  import torch
   54: from transformers import AutoModelForCausalLM, AutoTokenizer
   55  

   81  ```py
   82: from transformers import AutoModelForCausalLM
   83  

  234      ```python
  235:     from transformers.integrations.tensor_parallel import ParallelInterface
  236  

train_real_world/transformers_4573/docs/source/en/perf_torch_compile.md:
  22  ```py
  23: from transformers import AutoModelForCausalLM
  24  

  42  ```py
  43: from transformers import AutoModelForCausalLM
  44  

  53  ```py
  54: from transformers import AutoModelForCausalLM
  55  

train_real_world/transformers_4573/docs/source/en/perf_train_gpu_one.md:
   46  ```py
   47: from transformers import TrainingArguments
   48  

   67  ```py
   68: from transformers import TrainingArguments
   69  

   90  ```py
   91: from transformers import TrainingArguments
   92  

  113  ```py
  114: from transformers import TrainingArguments
  115  

  133  ```py
  134: from transformers import TrainingArguments
  135  

  159  ```py
  160: from transformers import TrainingArguments
  161  

  180  ```py
  181: from transformers import TrainingArguments
  182  

  205  ```py
  206: from transformers import TrainingArguments
  207  

  229  ```py
  230: from transformers import TrainingArguments
  231  

  250  ```py
  251: from transformers import TrainingArguments
  252  

  292  ```py
  293: from transformers import AutoModelForCausalLM
  294  

train_real_world/transformers_4573/docs/source/en/perplexity.md:
  74  ```python
  75: from transformers import GPT2LMHeadModel, GPT2TokenizerFast
  76  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/pipeline_gradio.md:
  31  ```py
  32: from transformers import pipeline
  33  import gradio as gr

train_real_world/transformers_4573/docs/source/en/pipeline_tutorial.md:
   31  ```py
   32: from transformers import pipeline
   33  

   41  ```py
   42: from transformers import pipeline
   43  from accelerate import Accelerator

   64  ```py
   65: from transformers import pipeline
   66  

   75  ```py
   76: from transformers import pipeline
   77  

   86  ```py
   87: from transformers import pipeline
   88  

  103  ```py
  104: from transformers import pipeline
  105  

  132  ```py
  133: from transformers import pipeline
  134  

  148  ```py
  149: from transformers import pipeline
  150  

  160  ```py
  161: from transformers import pipeline
  162  

  176  ```py
  177: from transformers import pipeline
  178  from accelerate import Accelerator

  192  ```py
  193: from transformers import pipeline
  194  from accelerate import Accelerator
  195: from transformers.pipelines.pt_utils import KeyDataset
  196  import datasets

  227  ```py
  228: from transformers import pipeline
  229  

  263  ```py
  264: from transformers import pipeline
  265  

  306  ```py
  307: from transformers.pipelines.pt_utils import KeyDataset
  308: from transformers import pipeline
  309  from accelerate import Accelerator

  351  import torch
  352: from transformers import pipeline, BitsAndBytesConfig
  353  

train_real_world/transformers_4573/docs/source/en/pipeline_webserver.md:
  42  from starlette.routing import Route
  43: from transformers import pipeline
  44  import asyncio

train_real_world/transformers_4573/docs/source/en/pr_checks.md:
  159  ```py
  160: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  161  ```

  165  ```py
  166: # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights
  167  ```

  171  ```py
  172: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta
  173  ```

  179  ```py
  180: # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT
  181  ```

  193  ```py
  194: # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing
  195  ```

train_real_world/transformers_4573/docs/source/en/processors.md:
   25  ```py
   26: from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
   27  from PIL import Image

   55  ```py
   56: from transformers import AutoProcessor
   57  

   66  ```py
   67: from transformers import WhisperProcessor
   68  

   74  ```py
   75: from transformers import WhisperTokenizerFast, WhisperFeatureExtractor, WhisperProcessor
   76  

  116  ```py
  117: from transformers import AutoProcessor
  118  

train_real_world/transformers_4573/docs/source/en/quicktour.md:
   89  ```py
   90: from transformers import AutoModelForCausalLM, AutoTokenizer
   91  

  129  ```py
  130: from transformers import pipeline
  131  from accelerate import Accelerator

  150  ```py
  151: from transformers import pipeline
  152  from accelerate import Accelerator

  178  ```py
  179: from transformers import pipeline
  180  from accelerate import Accelerator

  205  ```py
  206: from transformers import AutoModelForSequenceClassification, AutoTokenizer
  207  from datasets import load_dataset

  224  ```py
  225: from transformers import DataCollatorWithPadding
  226  

  232  ```py
  233: from transformers import TrainingArguments
  234  

  247  ```py
  248: from transformers import Trainer
  249  

train_real_world/transformers_4573/docs/source/en/serialization.md:
  74  ```python
  75: >>> from transformers import AutoTokenizer
  76  >>> from optimum.onnxruntime import ORTModelForQuestionAnswering

  89  >>> from optimum.onnxruntime import ORTModelForSequenceClassification
  90: >>> from transformers import AutoTokenizer
  91  

train_real_world/transformers_4573/docs/source/en/testing.md:
   194      >>> import torch
   195:     >>> from transformers import WhisperModel, WhisperFeatureExtractor
   196      >>> from datasets import load_dataset

   489  ```python
   490: from transformers.testing_utils import get_gpu_count
   491  

   738  ```python
   739: from transformers.testing_utils import TestCasePlus
   740  

   750  ```python
   751: from transformers.testing_utils import TestCasePlus
   752  

   773  ```python
   774: from transformers.testing_utils import TestCasePlus
   775  

   833  import os
   834: from transformers.testing_utils import ExtendSysPath
   835  

   941  ```python no-style
   942: from transformers.testing_utils import slow
   943  @slow

  1084  ```python
  1085: from transformers.testing_utils import CaptureStdout
  1086  

  1094  ```python
  1095: from transformers.testing_utils import CaptureStdout
  1096  

  1106  ```python
  1107: from transformers.testing_utils import CaptureStderr
  1108  

  1116  ```python
  1117: from transformers.testing_utils import CaptureStd
  1118  

  1131  ```python
  1132: from transformers import logging
  1133: from transformers.testing_utils import CaptureLogger
  1134  

  1148  ```python
  1149: from transformers.testing_utils import mockenv
  1150  

  1161  ```python
  1162: from transformers.testing_utils import TestCasePlus
  1163  

train_real_world/transformers_4573/docs/source/en/tokenizer_summary.md:
  109  ```py
  110: >>> from transformers import BertTokenizer
  111  

  123  ```py
  124: >>> from transformers import XLNetTokenizer
  125  

train_real_world/transformers_4573/docs/source/en/trainer.md:
   45  ```py
   46: from transformers import TrainingArguments
   47  

   67  ```py
   68: from transformers import Trainer
   69  

  186  from torch import nn
  187: from transformers import Trainer
  188  

  210  ```py
  211: from transformers import TrainerCallback, Trainer
  212  

  378  ```py
  379: from transformers import TrainingArguments
  380  

  476  ```py
  477: from transformers import TrainingArguments
  478  

  496  ```py
  497: from transformers import TrainingArguments
  498  

  525  ```py
  526: from transformers import TrainingArguments, Trainer
  527  

train_real_world/transformers_4573/docs/source/en/training.md:
   41  from datasets import load_dataset
   42: from transformers import AutoTokenizer
   43  

   69  ```py
   70: from transformers import AutoModelForSequenceClassification
   71  

  101  ```py
  102: from transformers import TrainingArguments
  103  

  113  ```py
  114: from transformers import Trainer
  115  

train_real_world/transformers_4573/docs/source/en/transformers_as_backend.md:
  103  
  104: from transformers import PreTrainedModel
  105  from torch import nn

  137  
  138: from transformers import PreTrainedConfig
  139  

train_real_world/transformers_4573/docs/source/en/troubleshooting.md:
  109  ```py
  110: >>> from transformers import AutoModelForSequenceClassification
  111  >>> import torch

  167  ```py
  168: >>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
  169  

train_real_world/transformers_4573/docs/source/en/video_processors.md:
  29  ```python
  30: from transformers import AutoVideoProcessor
  31  

  40  import torch
  41: from transformers.video_utils import load_video
  42: from transformers import AutoVideoProcessor
  43  

train_real_world/transformers_4573/docs/source/en/internal/generation_utils.md:
  29  ```python
  30: from transformers import GPT2Tokenizer, GPT2LMHeadModel
  31  

train_real_world/transformers_4573/docs/source/en/internal/import_utils.md:
  30  ```python
  31: >>> from transformers import DetrImageProcessorFast
  32  >>> print(DetrImageProcessorFast)

train_real_world/transformers_4573/docs/source/en/internal/model_debugging_utils.md:
  44  import requests
  45: from transformers import LlavaProcessor, LlavaForConditionalGeneration
  46: from transformers.model_debugging_utils import model_addition_debugger_context
  47  torch.random.manual_seed(673)

train_real_world/transformers_4573/docs/source/en/internal/rope_utils.md:
  41  ```python
  42: from transformers import LlamaConfig
  43  

  62  ```python
  63: from transformers import Gemma3Config
  64  

train_real_world/transformers_4573/docs/source/en/main_classes/backbones.md:
  20  
  21: * [`~utils.BackboneMixin`] enables initializing a backbone from Transformers or [timm](https://hf.co/docs/timm/index) and includes functions for returning the output features and indices.
  22  * [`~utils.BackboneConfigMixin`] sets the output features and indices of the backbone configuration.

train_real_world/transformers_4573/docs/source/en/main_classes/image_processor.md:
  24  ```python
  25: from transformers import AutoImageProcessor
  26  

  35  from torchvision.io import read_image
  36: from transformers import DetrImageProcessorFast
  37  

train_real_world/transformers_4573/docs/source/en/main_classes/logging.md:
  49  ```python
  50: from transformers.utils import logging
  51  

train_real_world/transformers_4573/docs/source/en/main_classes/output.md:
  25  ```python
  26: from transformers import BertTokenizer, BertForSequenceClassification
  27  import torch

train_real_world/transformers_4573/docs/source/en/main_classes/pipelines.md:
   65  import datasets
   66: from transformers import pipeline
   67: from transformers.pipelines.pt_utils import KeyDataset
   68  from tqdm.auto import tqdm

   84  ```python
   85: from transformers import pipeline
   86  

  114  ```python
  115: from transformers import pipeline
  116: from transformers.pipelines.pt_utils import KeyDataset
  117  import datasets

  137  ```python
  138: from transformers import pipeline
  139  from torch.utils.data import Dataset

train_real_world/transformers_4573/docs/source/en/main_classes/video_processor.md:
  29  ```python
  30: from transformers import AutoVideoProcessor
  31  

  40  import torch
  41: from transformers.video_utils import load_video
  42: from transformers import AutoVideoProcessor
  43  

  62  ```python
  63: from transformers import AutoVideoProcessor
  64  

  75  ```python
  76: from transformers import AutoVideoProcessor
  77: from transformers.video_utils import VideoMetadata
  78  

train_real_world/transformers_4573/docs/source/en/model_doc/afmoe.md:
  52  import torch
  53: from transformers import pipeline
  54  

  70  import torch
  71: from transformers import AutoTokenizer, AfmoeForCausalLM
  72  

train_real_world/transformers_4573/docs/source/en/model_doc/aimv2.md:
  37  from PIL import Image
  38: from transformers import AutoImageProcessor, AutoModel
  39  

  54  from PIL import Image
  55: from transformers import AutoProcessor, AutoModel
  56  

train_real_world/transformers_4573/docs/source/en/model_doc/albert.md:
  47  import torch
  48: from transformers import pipeline
  49  

  63  import torch
  64: from transformers import AutoModelForMaskedLM, AutoTokenizer
  65  

train_real_world/transformers_4573/docs/source/en/model_doc/align.md:
   40  import torch
   41: from transformers import pipeline
   42  

   65  from PIL import Image
   66: from transformers import AutoProcessor, AutoModelForZeroShotImageClassification
   67  

  103    # Example of using ALIGN for image-text similarity
  104:   from transformers import AlignProcessor, AlignModel
  105    import torch

train_real_world/transformers_4573/docs/source/en/model_doc/altclip.md:
  40  from PIL import Image
  41: from transformers import AltCLIPModel, AltCLIPProcessor
  42  

  71  from PIL import Image
  72: from transformers import AltCLIPModel, AltCLIPProcessor, TorchAoConfig
  73  

train_real_world/transformers_4573/docs/source/en/model_doc/apertus.md:
  42  import torch
  43: from transformers import pipeline
  44  

  58  import torch
  59: from transformers import AutoModelForCausalLM, AutoTokenizer
  60  

train_real_world/transformers_4573/docs/source/en/model_doc/arcee.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  import torch
  60: from transformers import AutoTokenizer, ArceeForCausalLM
  61  

train_real_world/transformers_4573/docs/source/en/model_doc/aria.md:
   41  import torch
   42: from transformers import pipeline
   43  

   60  import torch
   61: from transformers import AutoModelForCausalLM, AutoProcessor
   62  

  106  import torch
  107: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoProcessor
  108  

train_real_world/transformers_4573/docs/source/en/model_doc/audio-spectrogram-transformer.md:
  64  ```py
  65: from transformers import ASTForAudioClassification
  66  model = ASTForAudioClassification.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/audioflamingo3.md:
   56  ```python
   57: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
   58  

   88  ```python
   89: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
   90  

  133  ```python
  134: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  135  

  164  ```python
  165: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  166  

  195  ```python
  196: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  197  

  244  ```python
  245: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  246  

  299  ```python
  300: from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  301  

train_real_world/transformers_4573/docs/source/en/model_doc/auto.md:
  40  ```python
  41: from transformers import AutoConfig, AutoModel
  42  

train_real_world/transformers_4573/docs/source/en/model_doc/aya_vision.md:
   40  ```python
   41: from transformers import pipeline
   42  

   63  import torch
   64: from transformers import AutoProcessor, AutoModelForImageTextToText
   65  

  104  import torch
  105: from transformers import (
  106      AutoProcessor,

  151      import torch
  152:     from transformers import AutoProcessor, AutoModelForImageTextToText
  153          

  197      import torch
  198:     from transformers import AutoProcessor, AutoModelForImageTextToText
  199          

train_real_world/transformers_4573/docs/source/en/model_doc/bamba.md:
   43  import torch
   44: from transformers import pipeline
   45  

   60  import torch
   61: from transformers import AutoModelForCausalLM, AutoTokenizer
   62  

   85  import torch
   86: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
   87  

  118    ```python
  119:   from transformers import DataCollatorWithFlattening
  120  

train_real_world/transformers_4573/docs/source/en/model_doc/bark.md:
   45  ```python
   46: from transformers import BarkModel
   47  from accelerate import Accelerator

  101  ```python
  102: from transformers import BarkModel
  103  from accelerate import Accelerator

  122  ```python
  123: >>> from transformers import AutoProcessor, BarkModel
  124  

train_real_world/transformers_4573/docs/source/en/model_doc/bart.md:
  37  import torch
  38: from transformers import pipeline
  39  

  54  import torch
  55: from transformers import AutoModelForMaskedLM, AutoTokenizer
  56  

train_real_world/transformers_4573/docs/source/en/model_doc/barthez.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForMaskedLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/bartpho.md:
  40  import torch
  41: from transformers import pipeline
  42  

  62  import torch
  63: from transformers import BartForConditionalGeneration, AutoTokenizer
  64  

train_real_world/transformers_4573/docs/source/en/model_doc/beit.md:
  90  ```py
  91: from transformers import BeitForImageClassification
  92  model = BeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/bert-generation.md:
   41  import torch
   42: from transformers import pipeline
   43  

   57  import torch
   58: from transformers import EncoderDecoderModel, AutoTokenizer
   59  

   86  import torch
   87: from transformers import EncoderDecoderModel, AutoTokenizer, BitsAndBytesConfig
   88  

  114     ```python
  115:    from transformers import BertGenerationEncoder, BertGenerationDecoder, BertTokenizer, EncoderDecoderModel
  116     

train_real_world/transformers_4573/docs/source/en/model_doc/bert-japanese.md:
  41  >>> import torch
  42: >>> from transformers import AutoModel, AutoTokenizer
  43  

train_real_world/transformers_4573/docs/source/en/model_doc/bert.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForMaskedLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/bertweet.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForMaskedLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/big_bird.md:
  39  import torch
  40: from transformers import pipeline
  41  

  55  import torch
  56: from transformers import AutoModelForMaskedLM, AutoTokenizer
  57  

train_real_world/transformers_4573/docs/source/en/model_doc/bigbird_pegasus.md:
  41  import torch
  42: from transformers import pipeline
  43  

  60  import torch
  61: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  62  

  97  import torch
  98: from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  99  

train_real_world/transformers_4573/docs/source/en/model_doc/biogpt.md:
   41  import torch
   42: from transformers import pipeline
   43  

   58  import torch
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   94  import torch
   95: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   96  

  125     ```py
  126:    from transformers import AutoModelForCausalLM
  127  

train_real_world/transformers_4573/docs/source/en/model_doc/bitnet.md:
  80  import torch
  81: from transformers import AutoModelForCausalLM, AutoTokenizer
  82  

train_real_world/transformers_4573/docs/source/en/model_doc/blenderbot.md:
  53  ```python
  54: >>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
  55  

train_real_world/transformers_4573/docs/source/en/model_doc/blip.md:
  41  import torch
  42: from transformers import pipeline
  43  

  60  from PIL import Image
  61: from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering
  62  

train_real_world/transformers_4573/docs/source/en/model_doc/blt.md:
  63  import torch
  64: from transformers import AutoTokenizer, AutoModelForCausalLM
  65  

train_real_world/transformers_4573/docs/source/en/model_doc/bridgetower.md:
   58  ```python
   59: >>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
   60  >>> import requests

   81  ```python
   82: >>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
   83  >>> import requests

  104  ```python
  105: >>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
  106  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/byt5.md:
   38  import torch
   39: from transformers import pipeline
   40  

   54  import torch
   55: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
   56  

   90  import torch
   91: from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer
   92  

  115      import torch
  116:     from transformers import AutoModelForSeq2SeqLM
  117  

train_real_world/transformers_4573/docs/source/en/model_doc/camembert.md:
  47  import torch
  48: from transformers import pipeline
  49  

  59  import torch
  60: from transformers import AutoTokenizer, AutoModelForMaskedLM
  61  

  93  ```python
  94: from transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig
  95  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/canine.md:
  39  import torch
  40: from transformers import pipeline
  41  

  55  import torch
  56: from transformers import AutoModel
  57  

  82      ```py
  83:     from transformers import AutoTokenizer, AutoModel
  84  

train_real_world/transformers_4573/docs/source/en/model_doc/chameleon.md:
   70  ```python
   71: from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
   72  import torch

   95  ```python
   96: from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
   97  import torch

  146  ```python
  147: from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig
  148  

  163  ```python
  164: from transformers import ChameleonForConditionalGeneration
  165  

train_real_world/transformers_4573/docs/source/en/model_doc/chinese_clip.md:
  41  >>> import requests
  42: >>> from transformers import ChineseCLIPProcessor, ChineseCLIPModel
  43  

train_real_world/transformers_4573/docs/source/en/model_doc/clap.md:
  43  import torch
  44: from transformers import AutoTokenizer, AutoModel
  45  

train_real_world/transformers_4573/docs/source/en/model_doc/clip.md:
  41  import torch
  42: from transformers import pipeline
  43  

  60  from PIL import Image
  61: from transformers import AutoProcessor, AutoModel
  62  

train_real_world/transformers_4573/docs/source/en/model_doc/clvp.md:
  54  >>> import datasets
  55: >>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration
  56  

train_real_world/transformers_4573/docs/source/en/model_doc/code_llama.md:
   39  import torch
   40: from transformers import pipeline
   41  

   62  import torch
   63: from transformers import AutoModelForCausalLM, AutoTokenizer
   64  

  109  import torch
  110: from transformers import AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig
  111  

  130  ```py
  131: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  132  

  147      ```py
  148:     from transformers import LlamaForCausalLM, CodeLlamaTokenizer
  149  

train_real_world/transformers_4573/docs/source/en/model_doc/codegen.md:
  50  ```python
  51: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  52  

train_real_world/transformers_4573/docs/source/en/model_doc/cohere.md:
   40  import torch
   41: from transformers import pipeline
   42  

   56  import torch
   57: from transformers import AutoTokenizer, AutoModelForCausalLM
   58  

   91  import torch
   92: from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
   93  

  113  ```py
  114: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  115  

train_real_world/transformers_4573/docs/source/en/model_doc/cohere2_vision.md:
  44  
  45: from transformers import AutoProcessor, AutoModelForImageTextToText
  46  

  94  ```python
  95: from transformers import pipeline
  96  

train_real_world/transformers_4573/docs/source/en/model_doc/cohere2.md:
   42  import torch
   43: from transformers import pipeline
   44  

   62  import torch
   63: from transformers import AutoTokenizer, AutoModelForCausalLM
   64  

  102  import torch
  103: from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
  104  

train_real_world/transformers_4573/docs/source/en/model_doc/colpali.md:
   40  
   41: from transformers import ColPaliForRetrieval, ColPaliProcessor
   42  

  105  
  106: from transformers import BitsAndBytesConfig, ColPaliForRetrieval, ColPaliProcessor
  107  

train_real_world/transformers_4573/docs/source/en/model_doc/colqwen2.md:
   42  
   43: from transformers import ColQwen2ForRetrieval, ColQwen2Processor
   44: from transformers.utils.import_utils import is_flash_attn_2_available
   45  

  109  
  110: from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor
  111  from accelerate import Accelerator

  164  import torch
  165: from transformers import ColQwen2ForRetrieval, ColQwen2Processor
  166: from transformers.utils.import_utils import is_flash_attn_2_available
  167  

train_real_world/transformers_4573/docs/source/en/model_doc/csm.md:
   40  import torch
   41: from transformers import CsmForConditionalGeneration, AutoProcessor
   42  from accelerate import Accelerator

   75  import torch
   76: from transformers import CsmForConditionalGeneration, AutoProcessor
   77  from accelerate import Accelerator

  121  import torch
  122: from transformers import CsmForConditionalGeneration, AutoProcessor
  123  from accelerate import Accelerator

  179  import copy
  180: from transformers import CsmForConditionalGeneration, AutoProcessor
  181  from datasets import load_dataset

  311  ```python
  312: from transformers import CsmForConditionalGeneration, AutoProcessor
  313  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/cvt.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  from PIL import Image
  60: from transformers import AutoModelForImageClassification, AutoImageProcessor
  61  

train_real_world/transformers_4573/docs/source/en/model_doc/cwm.md:
  70  ```python
  71: from transformers import AutoModelForCausalLM, AutoTokenizer
  72  

train_real_world/transformers_4573/docs/source/en/model_doc/d_fine.md:
  36  >>> import torch
  37: >>> from transformers.image_utils import load_image
  38: >>> from transformers import DFineForObjectDetection, AutoImageProcessor
  39  

train_real_world/transformers_4573/docs/source/en/model_doc/dab-detr.md:
   58  from PIL import Image
   59: from transformers import AutoModelForObjectDetection, AutoImageProcessor
   60  

   95  ```py
   96: >>> from transformers import DabDetrForObjectDetection
   97  

  103  ```py
  104: >>> from transformers import DabDetrConfig, DabDetrForObjectDetection
  105  

train_real_world/transformers_4573/docs/source/en/model_doc/dac.md:
  50  >>> from datasets import load_dataset, Audio
  51: >>> from transformers import DacModel, AutoProcessor
  52  >>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

train_real_world/transformers_4573/docs/source/en/model_doc/data2vec.md:
  70  ```py
  71: from transformers import Data2VecVisionForImageClassification
  72  model = Data2VecVisionForImageClassification.from_pretrained("facebook/data2vec-vision-base", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/dbrx.md:
  46  ```python
  47: from transformers import DbrxForCausalLM, AutoTokenizer
  48  import torch

  68  ```python
  69: from transformers import DbrxForCausalLM, AutoTokenizer
  70  import torch

  91  ```python
  92: from transformers import DbrxForCausalLM, AutoTokenizer
  93  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/deberta-v2.md:
  41  import torch
  42: from transformers import pipeline
  43  

  58  import torch
  59: from transformers import AutoTokenizer, AutoModelForSequenceClassification
  60  

  95  ```py
  96: from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig
  97  

train_real_world/transformers_4573/docs/source/en/model_doc/deberta.md:
  43  import torch
  44: from transformers import pipeline
  45  

  62  import torch
  63: from transformers import AutoModelForSequenceClassification, AutoTokenizer
  64  

train_real_world/transformers_4573/docs/source/en/model_doc/deepseek_v3.md:
  43  # `run_deepseek_v1.py`
  44: from transformers import AutoModelForCausalLM, AutoTokenizer
  45  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/deepseek_vl_hybrid.md:
   40  import torch
   41: from transformers import pipeline
   42  

   71  import torch
   72: from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor
   73  

  127  import torch
  128: from transformers import TorchAoConfig, DeepseekVLHybridForConditionalGeneration, AutoProcessor
  129  

  148      import torch
  149:     from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor
  150  

train_real_world/transformers_4573/docs/source/en/model_doc/deepseek_vl.md:
   41  import torch
   42: from transformers import pipeline
   43  

   72  import torch
   73: from transformers import DeepseekVLForConditionalGeneration, AutoProcessor
   74  

  128  import torch
  129: from transformers import TorchAoConfig, DeepseekVLForConditionalGeneration, AutoProcessor
  130  

  149      import torch
  150:     from transformers import DeepseekVLForConditionalGeneration, AutoProcessor
  151  

train_real_world/transformers_4573/docs/source/en/model_doc/deformable_detr.md:
  45  ```python
  46: from transformers import pipeline
  47  import torch

  62  ```python
  63: from transformers import AutoImageProcessor, AutoModelForObjectDetection
  64  from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/deit.md:
  89  ```py
  90: from transformers import DeiTForImageClassification
  91  model = DeiTForImageClassification.from_pretrained("facebook/deit-base-distilled-patch16-224", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/deplot.md:
  41  ```python
  42: from transformers import AutoProcessor, Pix2StructForConditionalGeneration
  43  import requests

  60  ```python
  61: from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup
  62  

train_real_world/transformers_4573/docs/source/en/model_doc/depth_anything_v2.md:
  44  ```python
  45: >>> from transformers import pipeline
  46  >>> from PIL import Image

  64  ```python
  65: >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  66  >>> import torch

train_real_world/transformers_4573/docs/source/en/model_doc/depth_anything.md:
  39  import torch
  40: from transformers import pipeline
  41  

  53  from PIL import Image
  54: from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  55  

train_real_world/transformers_4573/docs/source/en/model_doc/depth_pro.md:
   48  >>> import torch
   49: >>> from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation
   50  from accelerate import Accelerator

  108  ```py
  109: >>> from transformers import DepthProForDepthEstimation
  110  >>> model = DepthProForDepthEstimation.from_pretrained("apple/DepthPro-hf", use_fov_model=False)

  115  ```py
  116: >>> from transformers import DepthProConfig, DepthProForDepthEstimation
  117  >>> config = DepthProConfig(use_fov_model=True)

  123  ```py
  124: >>> from transformers import DepthProConfig, DepthProForDepthEstimation
  125  >>> config = DepthProConfig()

  140  ```py
  141: from transformers import DepthProForDepthEstimation
  142  model = DepthProForDepthEstimation.from_pretrained("apple/DepthPro-hf", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/detr.md:
   40  ```python
   41: from transformers import pipeline
   42  import torch

   57  ```python
   58: from transformers import AutoImageProcessor, AutoModelForObjectDetection
   59  from PIL import Image

  117  ```python
  118: from transformers import DetrForObjectDetection
  119  

  125  ```python
  126: from transformers import DetrConfig, DetrForObjectDetection
  127  

train_real_world/transformers_4573/docs/source/en/model_doc/dia.md:
  44  ```python
  45: from transformers import AutoProcessor, DiaForConditionalGeneration
  46  from accelerate import Accelerator

  67  from datasets import load_dataset, Audio
  68: from transformers import AutoProcessor, DiaForConditionalGeneration
  69  from accelerate import Accelerator

  95  from datasets import load_dataset, Audio
  96: from transformers import AutoProcessor, DiaForConditionalGeneration
  97  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/dinov2.md:
   37  import torch
   38: from transformers import pipeline
   39  

   54  import requests
   55: from transformers import AutoImageProcessor, AutoModelForImageClassification
   56  from PIL import Image

   84  import requests
   85: from transformers import TorchAoConfig, AutoImageProcessor, AutoModelForImageClassification
   86  from torchao.quantization import Int4WeightOnlyConfig

  119    ```py
  120:   from transformers import AutoImageProcessor, AutoModel
  121    from PIL import Image

  151    import torch
  152:   from transformers import AutoImageProcessor, AutoModel
  153    from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/dinov3.md:
   37  import torch
   38: from transformers import pipeline
   39  

   53  import torch
   54: from transformers import AutoImageProcessor, AutoModel
   55: from transformers.image_utils import load_image
   56  

   85  import torch
   86: from transformers import TorchAoConfig, AutoImageProcessor, AutoModel
   87  from torchao.quantization import Int4WeightOnlyConfig
   88: from transformers.image_utils import load_image
   89  

  126    import torch
  127:   from transformers import AutoImageProcessor, AutoModel
  128:   from transformers.image_utils import load_image
  129  

train_real_world/transformers_4573/docs/source/en/model_doc/distilbert.md:
  41  ```py
  42: from transformers import pipeline
  43  

  61  import torch
  62: from transformers import AutoModelForSequenceClassification, AutoTokenizer
  63  

train_real_world/transformers_4573/docs/source/en/model_doc/dit.md:
  40  import torch
  41: from transformers import pipeline
  42  

  58  from PIL import Image
  59: from transformers import AutoModelForImageClassification, AutoImageProcessor
  60  

  89     ```py
  90:    from transformers import BeitForMaskedImageModeling
  91  

train_real_world/transformers_4573/docs/source/en/model_doc/doge.md:
  35  ```python
  36: from transformers import AutoTokenizer, AutoModelForCausalLM
  37  

  51  ```python
  52: from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer
  53  

train_real_world/transformers_4573/docs/source/en/model_doc/donut.md:
   41  import torch
   42: from transformers import pipeline
   43  from PIL import Image

   63  from datasets import load_dataset
   64: from transformers import AutoProcessor, AutoModelForImageTextToText
   65  

   94  from datasets import load_dataset
   95: from transformers import TorchAoConfig, AutoProcessor, AutoModelForImageTextToText
   96  

  121      >>> import re
  122:     >>> from transformers import DonutProcessor, VisionEncoderDecoderModel
  123      >>> from accelerate import Accelerator

  166      >>> from datasets import load_dataset
  167:     >>> from transformers import DonutProcessor, VisionEncoderDecoderModel
  168      >>> import torch

train_real_world/transformers_4573/docs/source/en/model_doc/dpt.md:
  46  ```python
  47: from transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation
  48  

train_real_world/transformers_4573/docs/source/en/model_doc/edgetam_video.md:
  51  ```python
  52: >>> from transformers import EdgeTamVideoModel, Sam2VideoProcessor
  53  from accelerate import Accelerator

  63  >>> # For this example, we'll use the video loading utility
  64: >>> from transformers.video_utils import load_video
  65  >>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"

train_real_world/transformers_4573/docs/source/en/model_doc/edgetam.md:
   46  ```python
   47: >>> from transformers import pipeline
   48  

   63  ```python
   64: >>> from transformers import Sam2Processor, EdgeTamModel
   65  from accelerate import Accelerator

  160  ```python
  161: >>> from transformers import Sam2Processor, EdgeTamModel
  162  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/efficientloftr.md:
  36  ```py
  37: from transformers import pipeline
  38  

  52  ```py
  53: from transformers import AutoImageProcessor, AutoModelForKeypointMatching
  54  import torch

  84      ```py
  85:     from transformers import AutoImageProcessor, AutoModelForKeypointMatching
  86      import torch

train_real_world/transformers_4573/docs/source/en/model_doc/electra.md:
  42  import torch
  43: from transformers import pipeline
  44  

  58  import torch
  59: from transformers import AutoTokenizer, AutoModelForSequenceClassification
  60  

train_real_world/transformers_4573/docs/source/en/model_doc/emu3.md:
  56  ```python
  57: from transformers import Emu3Processor, Emu3ForConditionalGeneration
  58  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/encodec.md:
  40  >>> from datasets import load_dataset, Audio
  41: >>> from transformers import EncodecModel, AutoProcessor
  42  >>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

train_real_world/transformers_4573/docs/source/en/model_doc/encoder-decoder.md:
   39  ```python
   40: from transformers import pipeline
   41  

   56  import torch
   57: from transformers import AutoModelForCausalLM, AutoTokenizer
   58  

   86  ```python
   87: from transformers import EncoderDecoderModel, BertTokenizer
   88  

   98  ```python
   99: >>> from transformers import BertTokenizer, EncoderDecoderModel
  100  

  123  ```python
  124: >>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel
  125  

  135  ```python
  136: from transformers import AutoTokenizer, EncoderDecoderModel
  137  

train_real_world/transformers_4573/docs/source/en/model_doc/eomt.md:
   62  
   63: from transformers import EomtForUniversalSegmentation, AutoImageProcessor
   64  

  110  
  111: from transformers import EomtForUniversalSegmentation, AutoImageProcessor
  112  

  153  
  154: from transformers import EomtForUniversalSegmentation, AutoImageProcessor
  155  

train_real_world/transformers_4573/docs/source/en/model_doc/ernie.md:
  43  ```py
  44: from transformers import pipeline
  45  

  58  import torch
  59: from transformers import AutoModelForMaskedLM, AutoTokenizer
  60  

train_real_world/transformers_4573/docs/source/en/model_doc/ernie4_5_moe.md:
   48  import torch
   49: from transformers import AutoModelForCausalLM, AutoTokenizer
   50  

   88  import torch
   89: from transformers import AutoModelForCausalLM, AutoTokenizer
   90  

  129  import torch
  130: from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer
  131  

train_real_world/transformers_4573/docs/source/en/model_doc/ernie4_5_vl_moe.md:
   55  ```py
   56: from transformers import pipeline
   57  

   82  ```py
   83: from transformers import AutoModelForImageTextToText, AutoProcessor
   84  

  133  ```python
  134: from transformers import AutoModelForImageTextToText, AutoProcessor
  135  

train_real_world/transformers_4573/docs/source/en/model_doc/ernie4_5.md:
  46  import torch
  47: from transformers import AutoModelForCausalLM, AutoTokenizer
  48  

train_real_world/transformers_4573/docs/source/en/model_doc/exaone4.md:
  65  ```python
  66: from transformers import AutoModelForCausalLM, AutoTokenizer
  67  

train_real_world/transformers_4573/docs/source/en/model_doc/falcon_h1.md:
  48  ```python
  49: from transformers import AutoModelForCausalLM, AutoTokenizer
  50  

train_real_world/transformers_4573/docs/source/en/model_doc/falcon_mamba.md:
  39  import torch
  40: from transformers import pipeline
  41  

  60  import torch
  61: from transformers import AutoTokenizer, AutoModelForCausalLM
  62  

  91  import torch
  92: from transformers import AutoTokenizer, FalconMambaForCausalLM, BitsAndBytesConfig
  93  

train_real_world/transformers_4573/docs/source/en/model_doc/falcon.md:
  41  import torch
  42: from transformers import pipeline
  43  

  62  import torch
  63: from transformers import AutoTokenizer, AutoModelForCausalLM
  64  

  95  import torch
  96: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  97  

train_real_world/transformers_4573/docs/source/en/model_doc/fast_vlm.md:
   75  import torch
   76: from transformers import AutoProcessor, FastVlmForConditionalGeneration
   77  

  110  import torch
  111: from transformers import AutoProcessor, FastVlmForConditionalGeneration
  112  

train_real_world/transformers_4573/docs/source/en/model_doc/fastspeech2_conformer.md:
  60  
  61: from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerModel, FastSpeech2ConformerHifiGan
  62  import soundfile as sf

  80  ```python
  81: from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerWithHifiGan
  82  import soundfile as sf

  97  ```python
  98: from transformers import pipeline, FastSpeech2ConformerHifiGan
  99  import soundfile as sf

train_real_world/transformers_4573/docs/source/en/model_doc/flan-t5.md:
  30  ```python
  31: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  32  

train_real_world/transformers_4573/docs/source/en/model_doc/flan-ul2.md:
  42  ```python
  43: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig
  44  

train_real_world/transformers_4573/docs/source/en/model_doc/flex_olmo.md:
  42  import torch
  43: from transformers import pipeline
  44  

  60  import torch
  61: from transformers import AutoModelForCausalLM, AutoTokenizer
  62  

  96  import torch
  97: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  98  

train_real_world/transformers_4573/docs/source/en/model_doc/florence2.md:
   45  from PIL import Image
   46: from transformers import pipeline
   47  

   67  from PIL import Image
   68: from transformers import AutoProcessor, Florence2ForConditionalGeneration
   69  

  102  from PIL import Image
  103: from transformers import AutoProcessor, Florence2ForConditionalGeneration, BitsAndBytesConfig
  104  

train_real_world/transformers_4573/docs/source/en/model_doc/fuyu.md:
  65  ```py
  66: from transformers import FuyuConfig, FuyuForCausalLM
  67  model_config = FuyuConfig()

  75  from PIL import Image
  76: from transformers import AutoTokenizer
  77: from transformers.models.fuyu.processing_fuyu import FuyuProcessor
  78: from transformers.models.fuyu.image_processing_fuyu_fast import FuyuImageProcessorFast
  79  

train_real_world/transformers_4573/docs/source/en/model_doc/gemma.md:
   45  import torch
   46: from transformers import pipeline
   47  

   62  import torch
   63: from transformers import AutoTokenizer, AutoModelForCausalLM
   64  

   96  import torch
   97: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   98  

  124  ```py
  125: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  126  

  140     import torch
  141:    from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
  142  

train_real_world/transformers_4573/docs/source/en/model_doc/gemma2.md:
   44  import torch
   45: from transformers import pipeline
   46  

   61  import torch
   62: from transformers import AutoTokenizer, AutoModelForCausalLM
   63  

   95  import torch
   96: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   97  

  116  ```python
  117: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  118  visualizer = AttentionMaskVisualizer("google/gemma-2b")

train_real_world/transformers_4573/docs/source/en/model_doc/gemma3.md:
   43  import torch
   44: from transformers import pipeline
   45  

   62  import torch
   63: from transformers import AutoProcessor, Gemma3ForConditionalGeneration
   64  

  118  import torch
  119: from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor
  120  

  161  ```py
  162: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  163  

  217      import torch
  218:     from transformers import AutoModelForCausalLM, AutoTokenizer
  219  

train_real_world/transformers_4573/docs/source/en/model_doc/gemma3n.md:
  53  import torch
  54: from transformers import pipeline
  55  

  72  import torch
  73: from transformers import AutoProcessor, Gemma3nForConditionalGeneration
  74  

train_real_world/transformers_4573/docs/source/en/model_doc/glm.md:
  62  ```python
  63: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  64  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/glm4v.md:
   56  import torch
   57: from transformers import pipeline
   58  pipe = pipeline(

   83  import torch
   84: from transformers import Glm4vForConditionalGeneration, AutoProcessor
   85  

  134  ```python
  135: from transformers import AutoProcessor, Glm4vForConditionalGeneration
  136  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/got_ocr2.md:
   50  >>> import torch
   51: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
   52  from accelerate import Accelerator

   76  >>> import torch
   77: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
   78  from accelerate import Accelerator

  106  >>> import torch
  107: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  108  from accelerate import Accelerator

  135  >>> import torch
  136: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  137  from accelerate import Accelerator

  165  >>> import torch
  166: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  167  from accelerate import Accelerator

  193  >>> import torch
  194: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  195  

  221  >>> import torch
  222: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  223  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/gpt_bigcode.md:
  66  >>> import torch
  67: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  68  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/gpt_neo.md:
  40  import torch
  41: from transformers import pipeline
  42  

  51  import torch
  52: from transformers import AutoModelForCausalLM, AutoTokenizer
  53  

  78  import torch
  79: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  80  

train_real_world/transformers_4573/docs/source/en/model_doc/gpt_neox_japanese.md:
   45  import torch
   46: from transformers import pipeline
   47  pipeline = pipeline(task="text-generation", 

   56  import torch
   57: from transformers import AutoModelForCausalLM, AutoTokenizer
   58  

   80  import torch
   81: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   82  

  103  ```py
  104: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  105  

train_real_world/transformers_4573/docs/source/en/model_doc/gpt_neox.md:
   51  ```python
   52: >>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast
   53  

   88  ```python
   89: >>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast
   90  

  114  ```python
  115: from transformers import GPTNeoXForCausalLM
  116  model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b", dtype=torch.float16, attn_implementation="sdpa")

train_real_world/transformers_4573/docs/source/en/model_doc/gpt-sw3.md:
  42  ```python
  43: >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  44  

train_real_world/transformers_4573/docs/source/en/model_doc/gpt2.md:
  43  import torch
  44: from transformers import pipeline
  45  

  54  import torch
  55: from transformers import AutoModelForCausalLM, AutoTokenizer
  56  

  87  import torch
  88: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
  89  

train_real_world/transformers_4573/docs/source/en/model_doc/gptj.md:
  40  ```python
  41: >>> from transformers import GPTJForCausalLM
  42  from accelerate import Accelerator

  71  ```python
  72: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  73  

  96  ```python
  97: >>> from transformers import GPTJForCausalLM, AutoTokenizer
  98  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/granite_speech.md:
   52  ```python
   53: from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor
   54  from datasets import load_dataset, Audio

   83  ```python
   84: from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor
   85  from datasets import load_dataset, Audio

  124  ```python
  125: from transformers import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor
  126  from datasets import load_dataset, Audio

train_real_world/transformers_4573/docs/source/en/model_doc/granite.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForCausalLM, AutoTokenizer
  58  

  87  import torch
  88: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  89  

train_real_world/transformers_4573/docs/source/en/model_doc/granitemoe.md:
  42  import torch
  43: from transformers import AutoModelForCausalLM, AutoTokenizer
  44  

train_real_world/transformers_4573/docs/source/en/model_doc/granitemoehybrid.md:
  24  ```python
  25: from transformers import AutoModelForCausalLM, AutoTokenizer
  26  

  66    ```python
  67:   from transformers import DataCollatorWithFlattening
  68  

train_real_world/transformers_4573/docs/source/en/model_doc/granitemoeshared.md:
  27  import torch
  28: from transformers import AutoModelForCausalLM, AutoTokenizer
  29  

train_real_world/transformers_4573/docs/source/en/model_doc/granitevision.md:
  36  ```python
  37: from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
  38  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/grounding-dino.md:
  52  >>> from PIL import Image
  53: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  54  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/helium.md:
  114  ```python
  115: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  116  

train_real_world/transformers_4573/docs/source/en/model_doc/herbert.md:
  50  ```python
  51: >>> from transformers import HerbertTokenizer, RobertaModel
  52  

  60  >>> import torch
  61: >>> from transformers import AutoModel, AutoTokenizer
  62  

train_real_world/transformers_4573/docs/source/en/model_doc/hgnet_v2.md:
  40  import torch
  41: from transformers import pipeline
  42  

  57  import requests
  58: from transformers import HGNetV2ForImageClassification, AutoImageProcessor
  59  from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/hubert.md:
  43  import torch
  44: from transformers import pipeline
  45  

  60  import torch
  61: from transformers import AutoProcessor, AutoModelForCTC
  62  from datasets import load_dataset

  90  import torch
  91: from transformers import AutoProcessor, AutoModelForCTC, BitsAndBytesConfig
  92  from datasets import load_dataset

train_real_world/transformers_4573/docs/source/en/model_doc/idefics2.md:
   59  from PIL import Image
   60: from transformers import Idefics2Processor, Idefics2ForConditionalGeneration
   61  from accelerate import Accelerator

  102  from PIL import Image
  103: from transformers import Idefics2Processor, Idefics2ForConditionalGeneration
  104  from accelerate import Accelerator

  179  ```diff
  180: + from transformers import BitsAndBytesConfig
  181  

train_real_world/transformers_4573/docs/source/en/model_doc/ijepa.md:
  44  import torch
  45: from transformers import pipeline
  46  feature_extractor = pipeline(

  65  from torch.nn.functional import cosine_similarity
  66: from transformers import AutoModel, AutoProcessor  
  67  

  97  import torch
  98: from transformers import BitsAndBytesConfig, AutoModel, AutoProcessor
  99  from datasets import load_dataset

train_real_world/transformers_4573/docs/source/en/model_doc/internvl.md:
   51  ```python
   52: >>> from transformers import pipeline
   53  

   80  ```python
   81: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
   82  >>> import torch

  111  ```python
  112: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  113  >>> import torch

  141  ```python
  142: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  143  >>> import torch

  185  ```python
  186: >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  187  >>> import torch

  229  ```python
  230: >>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
  231  

  269  ```python
  270: >>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
  271  >>> import torch

train_real_world/transformers_4573/docs/source/en/model_doc/jamba.md:
   45  import torch
   46: from transformers import pipeline
   47  

   61  import torch
   62: from transformers import AutoModelForCausalLM, AutoTokenizer
   63  

   94  import torch
   95: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   96  

  134      import torch
  135:     from transformers import AutoModelForCausalLM
  136      model = AutoModelForCausalLM.from_pretrained("ai21labs/AI21-Jamba-1.5-Large",

train_real_world/transformers_4573/docs/source/en/model_doc/janus.md:
   51  
   52: from transformers import JanusForConditionalGeneration, JanusProcessor
   53  

   94  
   95: from transformers import JanusForConditionalGeneration, JanusProcessor
   96  

  155  import torch
  156: from transformers import JanusForConditionalGeneration, JanusProcessor
  157  

train_real_world/transformers_4573/docs/source/en/model_doc/kosmos-2.md:
  47  >>> import requests
  48: >>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration
  49  

train_real_world/transformers_4573/docs/source/en/model_doc/kosmos2_5.md:
   47  from PIL import Image, ImageDraw
   48: from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration
   49  from accelerate import Accelerator

   87  from PIL import Image, ImageDraw
   88: from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration
   89  from accelerate import Accelerator

  171  from PIL import Image, ImageDraw
  172: from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration
  173  

train_real_world/transformers_4573/docs/source/en/model_doc/kyutai_speech_to_text.md:
  37  from datasets import load_dataset, Audio
  38: from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration
  39  from accelerate import Accelerator

  71  from datasets import load_dataset, Audio
  72: from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration
  73  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/lasr.md:
  36  ```py
  37: from transformers import pipeline
  38  

  47  ```py
  48: from transformers import AutoModelForCTC, AutoProcessor
  49  from datasets import load_dataset, Audio

train_real_world/transformers_4573/docs/source/en/model_doc/layoutlm.md:
  40  from datasets import load_dataset
  41: from transformers import AutoTokenizer, LayoutLMForQuestionAnswering
  42  

train_real_world/transformers_4573/docs/source/en/model_doc/layoutlmv2.md:
  165  ```python
  166: from transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor
  167  

  196  ```python
  197: from transformers import LayoutLMv2Processor
  198  from PIL import Image

  218  ```python
  219: from transformers import LayoutLMv2Processor
  220  from PIL import Image

  242  ```python
  243: from transformers import LayoutLMv2Processor
  244  from PIL import Image

  264  ```python
  265: from transformers import LayoutLMv2Processor
  266  from PIL import Image

  284  ```python
  285: from transformers import LayoutLMv2Processor
  286  from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/layoutxlm.md:
  47  ```python
  48: from transformers import LayoutLMv2Model
  49  

  57  ```python
  58: from transformers import LayoutXLMTokenizer
  59  

train_real_world/transformers_4573/docs/source/en/model_doc/led.md:
   41  import torch
   42: from transformers import pipeline
   43  

   60  import torch
   61: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
   62  

  101  import torch
  102: from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  103  

train_real_world/transformers_4573/docs/source/en/model_doc/lfm2_moe.md:
  32  ```python
  33: from transformers import AutoModelForCausalLM, AutoTokenizer
  34  

train_real_world/transformers_4573/docs/source/en/model_doc/lfm2_vl.md:
  41  ```python
  42: from transformers import AutoProcessor, AutoModelForImageTextToText
  43  \

train_real_world/transformers_4573/docs/source/en/model_doc/lfm2.md:
  40  ```python
  41: from transformers import AutoModelForCausalLM, AutoTokenizer
  42  

train_real_world/transformers_4573/docs/source/en/model_doc/lightglue.md:
  38  ```py
  39: from transformers import pipeline
  40  

  54  ```py
  55: from transformers import AutoImageProcessor, AutoModel
  56  import torch

  86      ```py
  87:     from transformers import AutoImageProcessor, AutoModel
  88      import torch

train_real_world/transformers_4573/docs/source/en/model_doc/lilt.md:
  47  ```python
  48: from transformers import LiltModel
  49  

train_real_world/transformers_4573/docs/source/en/model_doc/llama.md:
   42  import torch
   43: from transformers import pipeline
   44  

   58  import torch
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   92  import torch
   93: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
   94  

  112  ```py
  113: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  114  

train_real_world/transformers_4573/docs/source/en/model_doc/llama2.md:
   42  import torch
   43: from transformers import pipeline
   44  

   58  import torch
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   92  import torch
   93: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
   94  

  112  ```py
  113: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  114  

train_real_world/transformers_4573/docs/source/en/model_doc/llama3.md:
  74      ```python
  75:     from transformers import AutoModelForCausalLM, AutoTokenizer
  76  

train_real_world/transformers_4573/docs/source/en/model_doc/llama4.md:
   60  ```py
   61: from transformers import pipeline
   62  import torch

   84  ```py
   85: from transformers import AutoTokenizer, Llama4ForConditionalGeneration
   86  import torch

  111  ```py
  112: from transformers import AutoProcessor, Llama4ForConditionalGeneration
  113  import torch

  155  ```py
  156: from transformers import AutoProcessor, Llama4ForConditionalGeneration
  157  import torch

  207  ```py
  208: from transformers import Llama4ForConditionalGeneration, AutoTokenizer
  209  from accelerate import Accelerator

  270  ```py
  271: from transformers import Llama4ForConditionalGeneration
  272  import torch

  286  ```py
  287: from transformers import Llama4ForConditionalGeneration
  288  import torch

  302  ```py
  303: from transformers import Llama4ForConditionalGeneration
  304  import torch

  328  ```python
  329: from transformers import AutoTokenizer, Llama4ForConditionalGeneration, FbgemmFp8Config
  330  import torch

  358  ```python
  359: from transformers import AutoTokenizer, Llama4ForConditionalGeneration
  360  import torch

  393  ```py
  394: from transformers import Llama4ForConditionalGeneration
  395  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/llava_next_video.md:
   73  ```python
   74: from transformers import LlavaNextVideoProcessor
   75  

  122  import torch
  123: from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor
  124  

  205  ```python
  206: from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor
  207  

  232  ```python
  233: from transformers import LlavaNextVideoForConditionalGeneration
  234  

train_real_world/transformers_4573/docs/source/en/model_doc/llava_next.md:
   44  import torch  
   45: from transformers import pipeline  
   46  

   75  from PIL import Image
   76: from transformers import AutoProcessor, LlavaNextForConditionalGeneration
   77  from accelerate import Accelerator

  113  from PIL import Image
  114: from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig
  115  

  165  ```python
  166: from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
  167  from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/llava_onevision.md:
   70  ```python
   71: from transformers import AutoProcessor
   72  

  114  ```python
  115: from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
  116  from accelerate import Accelerator

  155  import torch
  156: from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
  157  

  218  import torch
  219: from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
  220  

  267  ```python
  268: from transformers import LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig
  269  

  284  ```python
  285: from transformers import LlavaOnevisionForConditionalGeneration
  286  

train_real_world/transformers_4573/docs/source/en/model_doc/llava.md:
   68  ```python
   69: from transformers import AutoProcessor
   70  

  134  import torch
  135: from transformers import AutoProcessor, LlavaForConditionalGeneration
  136  

  169  import torch
  170: from transformers import AutoProcessor, LlavaForConditionalGeneration
  171  

  217  ```python
  218: from transformers import LlavaImageProcessor
  219  

train_real_world/transformers_4573/docs/source/en/model_doc/longcat_flash.md:
  47  # launch_longcat.py
  48: from transformers import LongcatFlashForCausalLM, AutoTokenizer
  49  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/longformer.md:
  37  import torch
  38: from transformers import pipeline
  39  

  55  import torch
  56: from transformers import AutoModelForMaskedLM, AutoTokenizer
  57  

train_real_world/transformers_4573/docs/source/en/model_doc/longt5.md:
  69  >>> from datasets import load_dataset
  70: >>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration
  71  

train_real_world/transformers_4573/docs/source/en/model_doc/luke.md:
  87  ```python
  88: >>> from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification
  89  

train_real_world/transformers_4573/docs/source/en/model_doc/lxmert.md:
  25  
  26: The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://huggingface.co/papers/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders
  27  (one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a

  35  the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality
  36: Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we
  37  build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language

train_real_world/transformers_4573/docs/source/en/model_doc/m2m_100.md:
   59  ```python
   60: from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer
   61  

   80  ```python
   81: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
   82  

  150  >>> import torch
  151: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  152  

  184  ```python
  185: from transformers import M2M100ForConditionalGeneration
  186  model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M", dtype=torch.float16, attn_implementation="sdpa")

train_real_world/transformers_4573/docs/source/en/model_doc/madlad-400.md:
  45  ```python
  46: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  47  

train_real_world/transformers_4573/docs/source/en/model_doc/mamba.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch  
  57: from transformers import AutoModelForCausalLM, AutoTokenizer  
  58  

  82  import torch
  83: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  84  from torchao.quantization import Int4WeightOnlyConfig

train_real_world/transformers_4573/docs/source/en/model_doc/mamba2.md:
  41  import torch
  42: from transformers import pipeline
  43  

  57  import torch
  58: from transformers import AutoModelForCausalLM, AutoTokenizer
  59  

  83  import torch
  84: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  85  

train_real_world/transformers_4573/docs/source/en/model_doc/marian.md:
   46  import torch
   47: from transformers import pipeline
   48  

   60  import torch
   61: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
   62  

   77  ```python
   78: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
   79  

   96  
   97: from transformers import MarianMTModel, MarianTokenizer
   98  

  121  
  122: from transformers import MarianMTModel, MarianTokenizer
  123  

train_real_world/transformers_4573/docs/source/en/model_doc/markuplm.md:
   70  ```python
   71: from transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor
   72  

   96  ```python
   97: >>> from transformers import MarkupLMProcessor
   98  

  124  ```python
  125: >>> from transformers import MarkupLMProcessor
  126  

  145  ```python
  146: >>> from transformers import MarkupLMProcessor
  147  

  164  ```python
  165: >>> from transformers import MarkupLMProcessor
  166  

  192  ```python
  193: >>> from transformers import MarkupLMProcessor
  194  

train_real_world/transformers_4573/docs/source/en/model_doc/matcha.md:
  52  ```python
  53: from transformers import AutoProcessor, Pix2StructForConditionalGeneration
  54  import requests

  71  ```python
  72: from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup
  73  

train_real_world/transformers_4573/docs/source/en/model_doc/mbart.md:
   43  import torch
   44: from transformers import pipeline
   45  

   61  import torch
   62: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
   63  

   85      import torch
   86:     from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
   87  

  102      import torch
  103:     from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  104  

train_real_world/transformers_4573/docs/source/en/model_doc/metaclip_2.md:
  46  import torch
  47: from transformers import pipeline
  48  

  65  from PIL import Image
  66: from transformers import AutoProcessor, AutoModel
  67  

train_real_world/transformers_4573/docs/source/en/model_doc/mgp-str.md:
  52  ```py
  53: >>> from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition
  54  >>> import requests

train_real_world/transformers_4573/docs/source/en/model_doc/mimi.md:
  43  >>> from datasets import load_dataset, Audio
  44: >>> from transformers import MimiModel, AutoFeatureExtractor
  45  >>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

train_real_world/transformers_4573/docs/source/en/model_doc/minimax.md:
   58  ```python
   59: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   94  >>> import torch
   95: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   96  

  124  >>> import torch
  125: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  126  

train_real_world/transformers_4573/docs/source/en/model_doc/ministral.md:
  40  >>> import torch
  41: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  42  

train_real_world/transformers_4573/docs/source/en/model_doc/ministral3.md:
  43  import torch
  44: from transformers import Mistral3ForConditionalGeneration, MistralCommonBackend
  45  

train_real_world/transformers_4573/docs/source/en/model_doc/mistral.md:
   43  >>> import torch
   44: >>> from transformers import pipeline
   45  

   60  >>> import torch
   61: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   62  

   94  >>> import torch
   95: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   96  

  124  ```py
  125: >>> from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  126  

train_real_world/transformers_4573/docs/source/en/model_doc/mistral3.md:
   40  import torch
   41: from transformers import pipeline
   42  

   69  import torch
   70: from transformers import AutoProcessor, AutoModelForImageTextToText
   71  from accelerate import Accelerator 

  113  import torch
  114: from transformers import AutoProcessor, AutoModelForImageTextToText
  115  from accelerate import Accelerator

  154  import torch
  155: from transformers import AutoProcessor, AutoModelForImageTextToText
  156  from accelerate import Accelerator

  198  import torch
  199: from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
  200  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/mixtral.md:
   65  ```python
   66: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   67  

   82  ```python
   83: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   84  

  118  >>> import torch
  119: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  120  

  155  >>> import torch
  156: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  157  

train_real_world/transformers_4573/docs/source/en/model_doc/mlcd.md:
  52  from PIL import Image
  53: from transformers import AutoProcessor, MLCDVisionModel
  54  

train_real_world/transformers_4573/docs/source/en/model_doc/mllama.md:
  60  import torch
  61: from transformers import MllamaForConditionalGeneration, AutoProcessor
  62  

  88  from PIL import Image
  89: from transformers import MllamaForConditionalGeneration, AutoProcessor
  90  

train_real_world/transformers_4573/docs/source/en/model_doc/mluke.md:
  51  ```python
  52: from transformers import LukeModel
  53  

  59  ```python
  60: from transformers import MLukeTokenizer
  61  

train_real_world/transformers_4573/docs/source/en/model_doc/mm-grounding-dino.md:
  41  import torch
  42: from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor
  43  from accelerate import Accelerator
  44: from transformers.image_utils import load_image
  45  

train_real_world/transformers_4573/docs/source/en/model_doc/mms.md:
   64  ```py
   65: from transformers import Wav2Vec2ForCTC, AutoProcessor
   66  

   89  ```py
   90: from transformers import pipeline
   91  

  119  ```py
  120: from transformers import Wav2Vec2ForCTC, AutoProcessor
  121  import torch

  193  import torch
  194: from transformers import VitsTokenizer, VitsModel, set_seed
  195  

  231  ```python
  232: from transformers import VitsTokenizer
  233  

  253  import torch
  254: from transformers import VitsTokenizer, VitsModel, set_seed
  255  import os

  295  import torch
  296: from transformers import VitsTokenizer, VitsModel, set_seed
  297  

  345  ```py
  346: from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor
  347  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/mobilebert.md:
  38  import torch
  39: from transformers import pipeline
  40  

  54  import torch
  55: from transformers import AutoModelForMaskedLM, AutoTokenizer
  56  

train_real_world/transformers_4573/docs/source/en/model_doc/mobilenet_v1.md:
  39  import torch
  40: from transformers import pipeline
  41  

  57  from PIL import Image
  58: from transformers import AutoModelForImageClassification, AutoImageProcessor
  59  

  93      ```python
  94:     from transformers import MobileNetV1Config
  95  

train_real_world/transformers_4573/docs/source/en/model_doc/mobilenet_v2.md:
  39  import torch
  40: from transformers import pipeline
  41  

  57  from PIL import Image
  58: from transformers import AutoModelForImageClassification, AutoImageProcessor
  59  

  91      ```python
  92:     from transformers import MobileNetV2Config
  93  

train_real_world/transformers_4573/docs/source/en/model_doc/mobilevit.md:
  43  import torch
  44: from transformers import pipeline
  45  

  64  from PIL import Image
  65: from transformers import AutoImageProcessor, MobileViTForImageClassification
  66  

train_real_world/transformers_4573/docs/source/en/model_doc/modernbert-decoder.md:
   45  import torch
   46: from transformers import pipeline
   47  

   70  import torch
   71: from transformers import AutoModelForCausalLM, AutoTokenizer
   72  

   96  # For sequence classification
   97: from transformers import AutoModelForSequenceClassification
   98  

  123  import torch
  124: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  125  

train_real_world/transformers_4573/docs/source/en/model_doc/modernbert.md:
  41  import torch
  42: from transformers import pipeline
  43  

  57  import torch
  58: from transformers import AutoModelForMaskedLM, AutoTokenizer
  59  

train_real_world/transformers_4573/docs/source/en/model_doc/moonshine.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  from datasets import load_dataset
  60: from transformers import AutoProcessor, MoonshineForConditionalGeneration
  61  

train_real_world/transformers_4573/docs/source/en/model_doc/moshi.md:
  117  >>> import torch, math
  118: >>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer
  119  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/mt5.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  import torch
  60: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  61  

  95  import torch
  96: from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  97  

train_real_world/transformers_4573/docs/source/en/model_doc/musicgen_melody.md:
   94  ```python
   95: >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
   96  

  112  ```python
  113: >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
  114  

  150  ```python
  151: >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
  152  

  168  ```python
  169: >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
  170  >>> from datasets import load_dataset

  195  ```python
  196: >>> from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor
  197  

  208  ```python
  209: >>> from transformers import MusicgenMelodyForConditionalGeneration
  210  

  235  ```python
  236: >>> from transformers import AutoConfig, MusicgenMelodyForCausalLM, MusicgenMelodyForConditionalGeneration
  237  

train_real_world/transformers_4573/docs/source/en/model_doc/musicgen.md:
   87  ```python
   88: >>> from transformers import MusicgenForConditionalGeneration
   89  

  120  ```python
  121: >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
  122  

  151  ```python
  152: >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
  153  >>> from datasets import load_dataset

  177  ```python
  178: >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
  179  >>> from datasets import load_dataset

  211  ```python
  212: >>> from transformers import MusicgenForConditionalGeneration
  213  

  243  ```python
  244: >>> from transformers import AutoConfig, MusicgenForCausalLM, MusicgenForConditionalGeneration
  245  

train_real_world/transformers_4573/docs/source/en/model_doc/mvp.md:
  48  ```python
  49: >>> from transformers import MvpTokenizer, MvpForConditionalGeneration
  50  

  70  ```python
  71: >>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration
  72  

  92  ```python
  93: >>> from transformers import MvpForConditionalGeneration
  94  

train_real_world/transformers_4573/docs/source/en/model_doc/nanochat.md:
  39  import torch
  40: from transformers import pipeline
  41  

  61  import torch
  62: from transformers import AutoModelForCausalLM, AutoTokenizer
  63  

train_real_world/transformers_4573/docs/source/en/model_doc/nemotron.md:
  60  import torch
  61: from transformers import AutoTokenizer, AutoModelForCausalLM
  62  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/nllb-moe.md:
  70  ```python
  71: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  72  

  93  ```python
  94: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  95  

train_real_world/transformers_4573/docs/source/en/model_doc/nllb.md:
   44  import torch
   45: from transformers import pipeline
   46  

   54  ```python
   55: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
   56  

   83  ```python
   84: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig
   85  

  100  ```python
  101: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  102  

  115     ```python
  116:    >>> from transformers import NllbTokenizer
  117  

  125     ```python
  126:    >>> from transformers import NllbTokenizer
  127  

  135      ```python
  136:     >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  137  

train_real_world/transformers_4573/docs/source/en/model_doc/nougat.md:
  64  
  65: >>> from transformers import NougatProcessor, VisionEncoderDecoderModel
  66  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/olmo.md:
  44  import torch
  45: from transformers import pipeline
  46  

  62  import torch
  63: from transformers import AutoModelForCausalLM, AutoTokenizer
  64  

  96  import torch
  97: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  98  

train_real_world/transformers_4573/docs/source/en/model_doc/olmo2.md:
   41  import torch
   42: from transformers import pipeline
   43  

   59  import torch
   60: from transformers import AutoModelForCausalLM, AutoTokenizer
   61  

   95  import torch
   96: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
   97  

  127      ```py
  128:     from transformers import AutoModelForCausalLM
  129  

train_real_world/transformers_4573/docs/source/en/model_doc/olmo3.md:
   41  import torch
   42: from transformers import pipeline
   43  

   59  import torch
   60: from transformers import AutoModelForCausalLM, AutoTokenizer
   61  

   95  import torch
   96: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
   97  

  125      ```py
  126:     from transformers import AutoModelForCausalLM
  127  

train_real_world/transformers_4573/docs/source/en/model_doc/olmoe.md:
  43  import torch
  44: from transformers import pipeline
  45  

  61  import torch
  62: from transformers import AutoModelForCausalLM, AutoTokenizer
  63  from accelerate import Accelerator

  82  import torch
  83: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  84  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/omdet-turbo.md:
  55  
  56: >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection
  57  

  96  >>> from PIL import Image
  97: >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection
  98  

train_real_world/transformers_4573/docs/source/en/model_doc/openai-gpt.md:
  43  import torch
  44: from transformers import pipeline
  45  

  54  ```python
  55: from transformers import AutoModelForCausalLM, AutoTokenizer
  56  

train_real_world/transformers_4573/docs/source/en/model_doc/opt.md:
  43  import torch
  44: from transformers import pipeline
  45  

  54  import torch
  55: from transformers import AutoModelForCausalLM, AutoTokenizer
  56  

  83  import torch
  84: from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
  85  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/ovis2.md:
  40  from typing import Dict
  41: from transformers.image_utils import load_images, load_video
  42: from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor
  43  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/owlv2.md:
  50  
  51: >>> from transformers import Owlv2Processor, Owlv2ForObjectDetection
  52  

train_real_world/transformers_4573/docs/source/en/model_doc/owlvit.md:
  49  
  50: >>> from transformers import OwlViTProcessor, OwlViTForObjectDetection
  51  

train_real_world/transformers_4573/docs/source/en/model_doc/paddleocr_vl.md:
   76  ```py
   77: from transformers import pipeline
   78  

   97  ```py
   98: from transformers import AutoProcessor, AutoModelForImageTextToText
   99  

  134  ```py
  135: from transformers import pipeline
  136  

  156  ```py
  157: from transformers import AutoProcessor, AutoModelForImageTextToText
  158  

  200  ```python
  201: from transformers import AutoModelForImageTextToText
  202  model = AutoModelForImageTextToText.from_pretrained("PaddlePaddle/PaddleOCR-VL", dtype="bfloat16", attn_implementation="flash_attention_2")

train_real_world/transformers_4573/docs/source/en/model_doc/paligemma.md:
   43  import torch
   44: from transformers import pipeline
   45  

   64  from PIL import Image
   65: from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
   66  

   97  from PIL import Image
   98: from transformers import TorchAoConfig, AutoProcessor, PaliGemmaForConditionalGeneration
   99  

  122  ```py
  123: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  124  

  149      from PIL import Image
  150:     from transformers import TorchAoConfig, AutoProcessor, PaliGemmaForConditionalGeneration
  151  

train_real_world/transformers_4573/docs/source/en/model_doc/parakeet.md:
   50  ```py
   51: from transformers import pipeline
   52  

   61  ```py
   62: from transformers import AutoModelForCTC, AutoProcessor
   63  from datasets import load_dataset, Audio

   88  ```python
   89: from transformers import AutoModelForCTC, AutoProcessor
   90  from datasets import load_dataset, Audio

  168  ```python
  169: from transformers import AutoModelForCTC, AutoProcessor
  170  from datasets import load_dataset, Audio

train_real_world/transformers_4573/docs/source/en/model_doc/patchtsmixer.md:
  43  
  44: from transformers import PatchTSMixerConfig, PatchTSMixerForPrediction
  45: from transformers import Trainer, TrainingArguments,
  46  

train_real_world/transformers_4573/docs/source/en/model_doc/pegasus_x.md:
  42  import torch
  43: from transformers import pipeline
  44  

  61  import torch
  62: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  63  

  98  import torch
  99: from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  100  

train_real_world/transformers_4573/docs/source/en/model_doc/pegasus.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  import torch
  60: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  61  

  96  import torch
  97: from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  98  

train_real_world/transformers_4573/docs/source/en/model_doc/persimmon.md:
  72  ```py
  73: from transformers import PersimmonForCausalLM, PersimmonTokenizer
  74  

train_real_world/transformers_4573/docs/source/en/model_doc/phi.md:
   41  import torch
   42: from transformers import pipeline
   43  

   54  import torch
   55: from transformers import AutoTokenizer, AutoModelForCausalLM
   56  

   84  import torch
   85: from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
   86  

  105      import torch
  106:     from transformers import AutoTokenizer, AutoModelForCausalLM
  107  

train_real_world/transformers_4573/docs/source/en/model_doc/phi3.md:
  56  ```python
  57: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/phi4_multimodal.md:
   35  ```python
   36: from transformers import pipeline
   37  generator = pipeline("text-generation", model="microsoft/Phi-4-multimodal-instruct", dtype="auto", device=0)

   49  import torch
   50: from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
   51  from accelerate import Accelerator

  100  import torch
  101: from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
  102  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/phimoe.md:
  65  import torch
  66: from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline 
  67  

train_real_world/transformers_4573/docs/source/en/model_doc/phobert.md:
  41  >>> import torch
  42: >>> from transformers import AutoModel, AutoTokenizer
  43  

train_real_world/transformers_4573/docs/source/en/model_doc/pixio.md:
  35  import requests
  36: from transformers import AutoImageProcessor, AutoModel
  37  from PIL import Image

  60    ```py
  61:   from transformers import AutoImageProcessor, AutoModel
  62    from PIL import Image

  91    import torch
  92:   from transformers import AutoImageProcessor, AutoModel
  93    from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/pixtral.md:
  44  import torch
  45: from transformers import AutoProcessor, LlavaForConditionalGeneration
  46  

  81  from PIL import Image
  82: from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig
  83  

train_real_world/transformers_4573/docs/source/en/model_doc/plbart.md:
  62  ```python
  63: >>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer
  64  

  77  ```python
  78: >>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer
  79  

train_real_world/transformers_4573/docs/source/en/model_doc/poolformer.md:
  29  
  30: âŸª 985 characters skipped âŸ«y with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.*
  31  

train_real_world/transformers_4573/docs/source/en/model_doc/pop2piano.md:
   74  >>> from datasets import load_dataset
   75: >>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor
   76  

   94  >>> import librosa
   95: >>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor
   96  

  112  >>> import librosa
  113: >>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor
  114  

  140  >>> import librosa
  141: >>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer
  142  

train_real_world/transformers_4573/docs/source/en/model_doc/prompt_depth_anything.md:
  42  >>> from PIL import Image
  43: >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  44  

train_real_world/transformers_4573/docs/source/en/model_doc/pvt_v2.md:
  55  
  56: from transformers import AutoModelForImageClassification, AutoImageProcessor
  57  from PIL import Image

  72  
  73: from transformers import AutoConfig, AutoModelForObjectDetection, AutoImageProcessor
  74  from PIL import Image

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2_5_omni.md:
   50  import soundfile as sf
   51: from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
   52  

  106  ```python
  107: from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniProcessor
  108  

  163  import soundfile as sf
  164: from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
  165  

  334  ```python
  335: from transformers import Qwen2_5OmniForConditionalGeneration
  336  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2_5_vl.md:
   40  import torch
   41: from transformers import pipeline
   42  pipe = pipeline(

   69  import torch
   70: from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
   71  

  122  import torch
  123: from transformers import TorchAoConfig, Qwen2_5_VLForConditionalGeneration, AutoProcessor
  124  

  170      import torch
  171:     from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
  172      

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2_audio.md:
   48  import librosa
   49: from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
   50  

   83  import librosa
   84: from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
   85  

  125  import librosa
  126: from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
  127  

  175  import librosa
  176: from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
  177  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2_moe.md:
   42  import torch
   43: from transformers import pipeline
   44  

   64  import torch
   65: from transformers import AutoModelForCausalLM, AutoTokenizer
   66  

  118  import torch
  119: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  120  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2_vl.md:
   49  import torch
   50: from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
   51  

  269  ```python
  270: from transformers import Qwen2VLForConditionalGeneration
  271  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen2.md:
   42  import torch
   43: from transformers import pipeline
   44  

   64  import torch
   65: from transformers import AutoModelForCausalLM, AutoTokenizer
   66  

  121  import torch
  122: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  123  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen3_next.md:
  37  ```python
  38: from transformers import AutoModelForCausalLM, AutoTokenizer
  39  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen3_omni_moe.md:
   50  import soundfile as sf
   51: from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
   52  

  106  ```python
  107: from transformers import Qwen3OmniMoeThinkerForConditionalGeneration, Qwen3OmniMoeProcessor
  108  

  163  import soundfile as sf
  164: from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
  165  

  334  ```python
  335: from transformers import Qwen3OmniMoeForConditionalGeneration
  336  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen3_vl_moe.md:
  35  import torch
  36: from transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor
  37  

train_real_world/transformers_4573/docs/source/en/model_doc/qwen3_vl.md:
  35  import torch
  36: from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
  37  

train_real_world/transformers_4573/docs/source/en/model_doc/rag.md:
  42  import torch
  43: from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
  44  

  68  import torch
  69: from transformers import BitsAndBytesConfig, RagTokenizer, RagRetriever, RagSequenceForGeneration
  70  

train_real_world/transformers_4573/docs/source/en/model_doc/roberta.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForMaskedLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/roc_bert.md:
  41  import torch
  42: from transformers import pipeline
  43  

  57  import torch
  58: from transformers import AutoModelForMaskedLM, AutoTokenizer
  59  

train_real_world/transformers_4573/docs/source/en/model_doc/roformer.md:
  41  import torch
  42: from transformers import pipeline
  43  

  60  import torch
  61: from transformers import AutoModelForMaskedLM, AutoTokenizer
  62  

train_real_world/transformers_4573/docs/source/en/model_doc/rt_detr_v2.md:
  51  >>> from PIL import Image
  52: >>> from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor
  53  

train_real_world/transformers_4573/docs/source/en/model_doc/rt_detr.md:
  49  >>> from PIL import Image
  50: >>> from transformers import RTDetrForObjectDetection, RTDetrImageProcessor
  51  

train_real_world/transformers_4573/docs/source/en/model_doc/rwkv.md:
  38  import torch
  39: from transformers import AutoTokenizer, RwkvConfig, RwkvModel
  40  

  61  ```python
  62: from transformers import StoppingCriteria
  63  

train_real_world/transformers_4573/docs/source/en/model_doc/sam_hq.md:
  58  import requests
  59: from transformers import SamHQModel, SamHQProcessor
  60  from accelerate import Accelerator

  85  import requests
  86: from transformers import SamHQModel, SamHQProcessor
  87  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/sam.md:
  52  import requests
  53: from transformers import SamModel, SamProcessor
  54  from accelerate import Accelerator

  79  import requests
  80: from transformers import SamModel, SamProcessor
  81  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/sam2_video.md:
  56  ```python
  57: >>> from transformers import Sam2VideoModel, Sam2VideoProcessor
  58  from accelerate import Accelerator

  68  >>> # For this example, we'll use the video loading utility
  69: >>> from transformers.video_utils import load_video
  70  >>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"

train_real_world/transformers_4573/docs/source/en/model_doc/sam2.md:
   54  ```python
   55: >>> from transformers import pipeline
   56  

   71  ```python
   72: >>> from transformers import Sam2Processor, Sam2Model
   73  from accelerate import Accelerator

  160  ```python
  161: >>> from transformers import Sam2Processor, Sam2Model
  162  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/sam3_tracker_video.md:
  50  ```python
  51: >>> from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor
  52  from accelerate import Accelerator

  62  >>> # For this example, we'll use the video loading utility
  63: >>> from transformers.video_utils import load_video
  64  >>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"

train_real_world/transformers_4573/docs/source/en/model_doc/sam3_tracker.md:
   50  ```python
   51: >>> from transformers import pipeline
   52  

   67  ```python
   68: >>> from transformers import Sam3TrackerProcessor, Sam3TrackerModel
   69  from accelerate import Accelerator

  156  ```python
  157: >>> from transformers import Sam3TrackerProcessor, Sam3TrackerModel
  158  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/sam3_video.md:
  50  ```python
  51: >>> from transformers import Sam3VideoModel, Sam3VideoProcessor
  52  >>> from accelerate import Accelerator

  59  >>> # Load video frames
  60: >>> from transformers.video_utils import load_video
  61  >>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"

train_real_world/transformers_4573/docs/source/en/model_doc/sam3.md:
   44  ```python
   45: >>> from transformers import Sam3Processor, Sam3Model
   46  >>> import torch

  263  ```python
  264: >>> from transformers import Sam3Processor, Sam3Model
  265  >>> import torch

  308  ```python
  309: >>> from transformers import Sam3Processor, Sam3Model
  310  >>> import torch

train_real_world/transformers_4573/docs/source/en/model_doc/seamless_m4t_v2.md:
   46  ```python
   47: >>> from transformers import AutoProcessor, SeamlessM4Tv2Model
   48  

  103  ```python
  104: >>> from transformers import SeamlessM4Tv2ForSpeechToSpeech
  105  >>> model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained("facebook/seamless-m4t-v2-large")

  110  ```python
  111: >>> from transformers import SeamlessM4Tv2ForTextToText
  112  >>> model = SeamlessM4Tv2ForTextToText.from_pretrained("facebook/seamless-m4t-v2-large")

train_real_world/transformers_4573/docs/source/en/model_doc/seamless_m4t.md:
   46  ```python
   47: >>> from transformers import AutoProcessor, SeamlessM4TModel
   48  

  103  ```python
  104: >>> from transformers import SeamlessM4TForSpeechToSpeech
  105  >>> model = SeamlessM4TForSpeechToSpeech.from_pretrained("facebook/hf-seamless-m4t-medium")

  110  ```python
  111: >>> from transformers import SeamlessM4TForTextToText
  112  >>> model = SeamlessM4TForTextToText.from_pretrained("facebook/hf-seamless-m4t-medium")

train_real_world/transformers_4573/docs/source/en/model_doc/segformer.md:
  44  import torch
  45: from transformers import pipeline
  46  

  56  from PIL import Image
  57: from transformers import AutoProcessor, AutoModelForSemanticSegmentation
  58  

  82  ```python
  83: from transformers import SegformerImageProcessor
  84  processor = SegformerImageProcessor(do_reduce_labels=True)

train_real_world/transformers_4573/docs/source/en/model_doc/seggpt.md:
  43  from datasets import load_dataset
  44: from transformers import SegGptImageProcessor, SegGptForImageSegmentation
  45  

train_real_world/transformers_4573/docs/source/en/model_doc/shieldgemma2.md:
  43  import requests
  44: from transformers import AutoProcessor, ShieldGemma2ForImageClassification
  45  

  63  import requests
  64: from transformers import AutoProcessor, ShieldGemma2ForImageClassification
  65  

train_real_world/transformers_4573/docs/source/en/model_doc/siglip.md:
   43  import torch
   44: from transformers import pipeline
   45  

   59  from PIL import Image
   60: from transformers import AutoProcessor, AutoModel
   61  

   89  from PIL import Image
   90: from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig
   91  

  119  
  120:     from transformers import SiglipModel
  121  

train_real_world/transformers_4573/docs/source/en/model_doc/siglip2.md:
   46  import torch
   47: from transformers import pipeline
   48  

   62  from PIL import Image
   63: from transformers import AutoProcessor, AutoModel
   64  

   92  from PIL import Image
   93: from transformers import AutoProcessor, AutoModel
   94  

  124  from PIL import Image
  125: from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig
  126  

  162  
  163:     from transformers import SiglipModel
  164  

train_real_world/transformers_4573/docs/source/en/model_doc/smollm3.md:
   39  import torch
   40: from transformers import pipeline
   41  

   61  import torch
   62: from transformers import AutoModelForCausalLM, AutoTokenizer
   63  

  118  import torch
  119: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  120  

train_real_world/transformers_4573/docs/source/en/model_doc/smolvlm.md:
   59  import torch
   60: from transformers import AutoProcessor, AutoModelForImageTextToText
   61  

  121  import torch
  122: from transformers import AutoProcessor, AutoModelForImageTextToText
  123  

train_real_world/transformers_4573/docs/source/en/model_doc/speech_to_text.md:
  54  >>> import torch
  55: >>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
  56  >>> from datasets import load_dataset

  81  >>> import torch
  82: >>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
  83  >>> from datasets import load_dataset

train_real_world/transformers_4573/docs/source/en/model_doc/speech-encoder-decoder.md:
  41  ```python
  42: >>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel
  43  

  58  ```python
  59: >>> from transformers import SpeechEncoderDecoderModel
  60  

  72  ```python
  73: >>> from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel
  74  >>> from datasets import load_dataset

  98  ```python
  99: >>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel
  100  >>> from datasets import load_dataset

train_real_world/transformers_4573/docs/source/en/model_doc/stablelm.md:
  46  ```python
  47: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  48  from accelerate import Accelerator, set_seed

  78  >>> import torch
  79: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  80  from accelerate import Accelerator, set_seed

train_real_world/transformers_4573/docs/source/en/model_doc/starcoder2.md:
  45  ```python
  46: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  47  

train_real_world/transformers_4573/docs/source/en/model_doc/superglue.md:
  38  ```py
  39: from transformers import pipeline
  40  

  54  ```py
  55: from transformers import AutoImageProcessor, AutoModel
  56  import torch

  86      ```python
  87:     from transformers import AutoImageProcessor, AutoModel
  88      import torch

train_real_world/transformers_4573/docs/source/en/model_doc/superpoint.md:
  40  ```py
  41: from transformers import AutoImageProcessor, SuperPointForKeypointDetection
  42  import torch

  68      ```py
  69:     from transformers import AutoImageProcessor, SuperPointForKeypointDetection
  70      import torch

train_real_world/transformers_4573/docs/source/en/model_doc/swin.md:
  39  import torch
  40: from transformers import pipeline
  41  

  58  from PIL import Image
  59: from transformers import AutoModelForImageClassification, AutoImageProcessor
  60  

train_real_world/transformers_4573/docs/source/en/model_doc/swinv2.md:
  37  import torch
  38: from transformers import pipeline
  39  

  56  from PIL import Image
  57: from transformers import AutoModelForImageClassification, AutoImageProcessor
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/switch_transformers.md:
  41  import torch
  42: from transformers import pipeline
  43  

  57  import torch
  58: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  59  

  87  import torch
  88: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig
  89  

train_real_world/transformers_4573/docs/source/en/model_doc/t5.md:
  41  import torch
  42: from transformers import pipeline
  43  

  57  import torch
  58: from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  59  

  91  import torch
  92: from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer
  93  

train_real_world/transformers_4573/docs/source/en/model_doc/t5gemma.md:
  43  import torch
  44: from transformers import pipeline
  45  

  66  import torch
  67: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  68  

train_real_world/transformers_4573/docs/source/en/model_doc/t5gemma2.md:
  40  import torch
  41: from transformers import pipeline
  42  

  63  from PIL import Image
  64: from transformers import AutoProcessor, AutoModelForSeq2SeqLM
  65  

train_real_world/transformers_4573/docs/source/en/model_doc/t5v1.1.md:
  35  ```python
  36: >>> from transformers import T5ForConditionalGeneration
  37  

train_real_world/transformers_4573/docs/source/en/model_doc/tapas.md:
   82  ```py
   83: >>> from transformers import TapasConfig, TapasForQuestionAnswering
   84  

   99  ```py
  100: >>> from transformers import TapasConfig, TapasForQuestionAnswering
  101  

  141  ```py
  142: >>> from transformers import TapasTokenizer
  143  >>> import pandas as pd

  222  ```py
  223: >>> from transformers import TapasConfig, TapasForQuestionAnswering, AdamW
  224  

  277  ```py
  278: >>> from transformers import TapasTokenizer, TapasForQuestionAnswering
  279  >>> import pandas as pd

train_real_world/transformers_4573/docs/source/en/model_doc/timesfm.md:
  39  import torch
  40: from transformers import TimesFmModelForPrediction
  41  

train_real_world/transformers_4573/docs/source/en/model_doc/timm_wrapper.md:
  30  >>> from urllib.request import urlopen
  31: >>> from transformers import AutoModelForImageClassification, AutoImageProcessor
  32  

train_real_world/transformers_4573/docs/source/en/model_doc/trocr.md:
  43  ```python
  44: from transformers import TrOCRProcessor, VisionEncoderDecoderModel
  45  import requests

  72  # pip install bitsandbytes accelerate
  73: from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BitsandBytesConfig
  74  import requests

train_real_world/transformers_4573/docs/source/en/model_doc/tvp.md:
  56  from huggingface_hub import hf_hub_download
  57: from transformers import AutoProcessor, TvpForVideoGrounding
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/umt5.md:
  56  ```python
  57: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/univnet.md:
  45  
  46: from transformers import UnivNetFeatureExtractor, UnivNetModel
  47  

train_real_world/transformers_4573/docs/source/en/model_doc/upernet.md:
  45  ```py
  46: from transformers import SwinConfig, UperNetConfig, UperNetForSemanticSegmentation
  47  

  56  ```py
  57: from transformers import ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation
  58  

train_real_world/transformers_4573/docs/source/en/model_doc/vaultgemma.md:
  47  ```python
  48: from transformers import pipeline
  49  

  67  # pip install accelerate
  68: from transformers import AutoTokenizer, AutoModelForCausalLM
  69  

train_real_world/transformers_4573/docs/source/en/model_doc/video_llama_3.md:
   48  import torch
   49: from transformers import VideoLlama3ForConditionalGeneration, AutoTokenizer, AutoProcessor
   50  

  179  ```python
  180: from transformers import VideoLlama3ForConditionalGeneration
  181  

train_real_world/transformers_4573/docs/source/en/model_doc/video_llava.md:
   76  import numpy as np
   77: from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor
   78  

  164  ```python
  165: from transformers import VideoLlavaForConditionalGeneration, BitsAndBytesConfig
  166  

  191  ```python
  192: from transformers import VideoLlavaForConditionalGeneration
  193  

train_real_world/transformers_4573/docs/source/en/model_doc/videomae.md:
  54  ```py
  55: from transformers import VideoMAEForVideoClassification
  56  model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/vipllava.md:
  55  ```python
  56: from transformers import AutoProcessor
  57  

train_real_world/transformers_4573/docs/source/en/model_doc/vision-encoder-decoder.md:
   47  ```python
   48: >>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel
   49  

   64  ```python
   65: >>> from transformers import VisionEncoderDecoderModel
   66  

   81  
   82: >>> from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel
   83  

  107  ```python
  108: >>> from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel
  109  >>> from datasets import load_dataset

train_real_world/transformers_4573/docs/source/en/model_doc/visual_bert.md:
  43  import numpy as np
  44: from transformers import AutoTokenizer, VisualBertForQuestionAnswering
  45  import requests

train_real_world/transformers_4573/docs/source/en/model_doc/vit_mae.md:
  46  from PIL import Image
  47: from transformers import ViTImageProcessor, ViTMAEForPreTraining
  48  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/vit_msn.md:
  70  ```py
  71: from transformers import ViTMSNForImageClassification
  72  model = ViTMSNForImageClassification.from_pretrained("facebook/vit-msn-base", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/vit.md:
  40  import torch
  41: from transformers import pipeline
  42  

  58  from PIL import Image
  59: from transformers import AutoModelForImageClassification, AutoImageProcessor
  60  

train_real_world/transformers_4573/docs/source/en/model_doc/vitpose.md:
   38  from PIL import Image
   39: from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation
   40  from accelerate import Accelerator

  120  from PIL import Image
  121: from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, TorchAoConfig
  122  

  165      ```py
  166:     from transformers import AutoProcessor, VitPoseForPoseEstimation
  167      from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/vits.md:
   34  import torch
   35: from transformers import pipeline, set_seed
   36  from scipy.io.wavfile import write

   63  from IPython.display import Audio
   64: from transformers import AutoTokenizer, VitsModel, set_seed
   65  

   91     # pip install -U uroman
   92:    from transformers import VitsTokenizer
   93  

  109     import torch
  110:    from transformers import VitsTokenizer, VitsModel, set_seed
  111     import os

train_real_world/transformers_4573/docs/source/en/model_doc/vivit.md:
  44  ```py
  45: from transformers import VivitModel
  46  model = VivitModel.from_pretrained("google/vivit-b-16x2-kinetics400", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/en/model_doc/vjepa2.md:
  76  from torchcodec.decoders import VideoDecoder
  77: from transformers import AutoVideoProcessor, AutoModelForVideoClassification
  78  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/voxtral.md:
   49  import torch
   50: from transformers import VoxtralForConditionalGeneration, AutoProcessor
   51  from accelerate import Accelerator

   87  import torch
   88: from transformers import VoxtralForConditionalGeneration, AutoProcessor
   89  from accelerate import Accelerator

  129  import torch
  130: from transformers import VoxtralForConditionalGeneration, AutoProcessor
  131  from accelerate import Accelerator

  185  import torch
  186: from transformers import VoxtralForConditionalGeneration, AutoProcessor
  187  from accelerate import Accelerator

  222  import torch
  223: from transformers import VoxtralForConditionalGeneration, AutoProcessor
  224  from accelerate import Accelerator

  259  import torch
  260: from transformers import VoxtralForConditionalGeneration, AutoProcessor
  261  from accelerate import Accelerator

  322  import torch
  323: from transformers import VoxtralForConditionalGeneration, AutoProcessor
  324  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/wav2vec2.md:
   69  ```python
   70: >>> from transformers import Wav2Vec2Model
   71  

  152  >>> from multiprocessing import get_context
  153: >>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
  154  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/whisper.md:
  41  import torch
  42: from transformers import pipeline
  43  

  59  from datasets import load_dataset
  60: from transformers import AutoProcessor, WhisperForConditionalGeneration
  61  

train_real_world/transformers_4573/docs/source/en/model_doc/xcodec.md:
  55  from datasets import load_dataset, Audio
  56: from transformers import XcodecModel, AutoFeatureExtractor
  57  dummy_dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

train_real_world/transformers_4573/docs/source/en/model_doc/xlm-roberta-xl.md:
  40  import torch
  41: from transformers import pipeline
  42  

  56  import torch
  57: from transformers import AutoModelForMaskedLM, AutoTokenizer
  58  

  97  import torch
  98: from transformers import AutoModelForMaskedLM, AutoTokenizer, TorchAoConfig
  99  

train_real_world/transformers_4573/docs/source/en/model_doc/xlm-roberta.md:
   40  import torch
   41: from transformers import pipeline
   42  

   56  ```python
   57: from transformers import AutoModelForMaskedLM, AutoTokenizer
   58  import torch

   99  import torch
  100: from transformers import AutoModelForMaskedLM, AutoTokenizer, BitsAndBytesConfig
  101  

train_real_world/transformers_4573/docs/source/en/model_doc/xlm.md:
  39  import torch
  40: from transformers import pipeline
  41  

  55  import torch
  56: from transformers import AutoModelForMaskedLM, AutoTokenizer
  57  

train_real_world/transformers_4573/docs/source/en/model_doc/xmod.md:
  51  ```python
  52: from transformers import XmodModel
  53  

train_real_world/transformers_4573/docs/source/en/model_doc/yolos.md:
  45  import torch
  46: from transformers import pipeline
  47  

  63  import requests
  64: from transformers import AutoImageProcessor, AutoModelForObjectDetection
  65  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/model_doc/zamba.md:
  56  ```python
  57: from transformers import AutoTokenizer, AutoModelForCausalLM
  58  import torch

train_real_world/transformers_4573/docs/source/en/model_doc/zamba2.md:
  49  import torch
  50: from transformers import AutoTokenizer, AutoModelForCausalLM
  51  

train_real_world/transformers_4573/docs/source/en/model_doc/zoedepth.md:
  40  import torch
  41: from transformers import pipeline
  42  from PIL import Image

  62  from PIL import Image
  63: from transformers import AutoModelForDepthEstimation, AutoImageProcessor
  64  

train_real_world/transformers_4573/docs/source/en/quantization/aqlm.md:
  31  ```python
  32: from transformers import AutoTokenizer, AutoModelForCausalLM
  33  

train_real_world/transformers_4573/docs/source/en/quantization/auto_round.md:
   72  ```python
   73: from transformers import AutoModelForCausalLM, AutoTokenizer
   74  from auto_round import AutoRound

  105  ```python
  106: from transformers import AutoModelForCausalLM, AutoTokenizer
  107  from auto_round import AutoRound

  136  ```python
  137: from transformers import AutoModelForCausalLM, AutoTokenizer
  138  from auto_round import AutoRound

  181  ```python
  182: from transformers import AutoModelForCausalLM, AutoTokenizer
  183  

  200  ```python
  201: from transformers import AutoModelForCausalLM, AutoTokenizer
  202  

  219  ```python
  220: from transformers import AutoModelForCausalLM, AutoTokenizer
  221  

  241  ```python
  242: from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig
  243  

  261  ```python
  262: from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig
  263  

train_real_world/transformers_4573/docs/source/en/quantization/awq.md:
   57  ```py
   58: from transformers import AutoModelForCausalLM, AutoTokenizer
   59  from accelerate import Accelerator

   73  ```py
   74: from transformers import AutoModelForCausalLM, AutoTokenizer
   75  

   98  import torch
   99: from transformers import AwqConfig, AutoModelForCausalLM
  100  

  159  import torch
  160: from transformers import AwqConfig, AutoModelForCausalLM
  161  

  216  import torch
  217: from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig
  218  

  240  import torch
  241: from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig
  242  

train_real_world/transformers_4573/docs/source/en/quantization/bitnet.md:
  36  ```py
  37: from transformers import AutoModelForCausalLM
  38  path = "/path/to/model"

train_real_world/transformers_4573/docs/source/en/quantization/bitsandbytes.md:
   78  ```py
   79: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   80  

   93  import torch
   94: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   95  

  109  ```py
  110: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  111  

  129  ```py
  130: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  131  

  144  import torch
  145: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  146  

  160  ```py
  161: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  162  

  189  ```py
  190: from transformers import AutoModelForCausalLM, AutoTokenizer
  191  

  203  ```py
  204: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  205  

  238  ```py
  239: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  240  

  260  ```py
  261: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  262  

  290  import torch
  291: from transformers import BitsAndBytesConfig
  292  

  300  ```py
  301: from transformers import BitsAndBytesConfig
  302  

  317  ```py
  318: from transformers import BitsAndBytesConfig
  319  

  332  ```python
  333: from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
  334  

train_real_world/transformers_4573/docs/source/en/quantization/compressed_tensors.md:
   55  ```python
   56: from transformers import AutoModelForCausalLM
   57  

  128  ```python
  129: from transformers import AutoModelForCausalLM
  130  

train_real_world/transformers_4573/docs/source/en/quantization/concept_guide.md:
  147  import torch
  148: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  149  

train_real_world/transformers_4573/docs/source/en/quantization/eetq.md:
  47  ```py
  48: from transformers import AutoModelForCausalLM, EetqConfig
  49  

train_real_world/transformers_4573/docs/source/en/quantization/fbgemm_fp8.md:
  34  ```py
  35: from transformers import FbgemmFp8Config, AutoModelForCausalLM
  36  

train_real_world/transformers_4573/docs/source/en/quantization/finegrained_fp8.md:
  41  ```py
  42: from transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer
  43  

train_real_world/transformers_4573/docs/source/en/quantization/fp_quant.md:
  25  ```python
  26: from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig
  27  import torch

  53  import torch
  54: from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig
  55  

train_real_world/transformers_4573/docs/source/en/quantization/gptq.md:
   37  ```py
   38: from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig
   39  

   91  ```py
   92: from transformers import AutoModelForCausalLM
   93  

  104  
  105: from transformers import AutoModelForCausalLM, GPTQConfig
  106  

train_real_world/transformers_4573/docs/source/en/quantization/higgs.md:
  44  ```python
  45: from transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig
  46  

  62  import torch
  63: from transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig
  64  

train_real_world/transformers_4573/docs/source/en/quantization/hqq.md:
  37  import torch
  38: from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig
  39  

train_real_world/transformers_4573/docs/source/en/quantization/mxfp4.md:
  37  ```py
  38: from transformers import GptOssConfig
  39  

train_real_world/transformers_4573/docs/source/en/quantization/quanto.md:
  34  ```py
  35: from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig
  36  

  51  import torch
  52: from transformers import AutoModelForSpeechSeq2Seq, QuantoConfig
  53  

train_real_world/transformers_4573/docs/source/en/quantization/quark.md:
  60  ```python
  61: from transformers import AutoModelForCausalLM, AutoTokenizer
  62  

train_real_world/transformers_4573/docs/source/en/quantization/spqr.md:
  30  ```python
  31: from transformers import AutoTokenizer, AutoModelForCausalLM
  32  import torch

train_real_world/transformers_4573/docs/source/en/quantization/torchao.md:
   95  import torch
   96: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
   97  from torchao.quantization import Float8DynamicActivationFloat8WeightConfig, Float8WeightOnlyConfig

  125  import torch
  126: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  127  from torchao.quantization import GemliteUIntXWeightOnlyConfig

  157  import torch
  158: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  159  from torchao.quantization import Int4WeightOnlyConfig

  191  import torch
  192: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  193  from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

  222  import torch
  223: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  224  from torchao.quantization import GemliteUIntXWeightOnlyConfig, Int4WeightOnlyConfig

  260  import torch
  261: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  262  from torchao.quantization import Int4WeightOnlyConfig

  294  import torch
  295: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  296  from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

  325  import torch
  326: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  327  from torchao.quantization import Int4WeightOnlyConfig

  361  import torch
  362: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  363  from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

  393  import torch
  394: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  395  from torchao.quantization import Int4WeightOnlyConfig

  428  import torch
  429: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  430  

  457  import torch
  458: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  459  

  502  import torch
  503: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
  504  

  618  import torch
  619: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  620  

  678  import torch
  679: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  680  from torchao.quantization import Int8WeightOnlyConfig

  714  import torch
  715: from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
  716  from torchao.quantization import Int4WeightOnlyConfig

train_real_world/transformers_4573/docs/source/en/quantization/vptq.md:
  43  ```py
  44: from transformers import AutoTokenizer, AutoModelForCausalLM
  45  

train_real_world/transformers_4573/docs/source/en/reference/environment_variables.md:
  33  
  34: from transformers import pipeline
  35  

  54  
  55: from transformers import pipeline
  56  

train_real_world/transformers_4573/docs/source/en/tasks/any_to_any.md:
  33  ```python
  34: from transformers import AutoProcessor, AutoModelForMultimodalLM, infer_device
  35  import torch

  88  ```python
  89: from transformers import pipeline
  90  pipe = pipeline("any-to-any", model="mistralai/Voxtral-Mini-3B-2507")

train_real_world/transformers_4573/docs/source/en/tasks/asr.md:
  109  ```py
  110: >>> from transformers import AutoProcessor
  111  

  239  ```py
  240: >>> from transformers import AutoModelForCTC, TrainingArguments, Trainer
  241  

  319  ```py
  320: >>> from transformers import pipeline
  321  

  337  ```py
  338: >>> from transformers import AutoProcessor
  339  

  346  ```py
  347: >>> from transformers import AutoModelForCTC
  348  

train_real_world/transformers_4573/docs/source/en/tasks/audio_classification.md:
  125  ```py
  126: >>> from transformers import AutoFeatureExtractor
  127  

  198  ```py
  199: >>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer
  200  

  271  ```py
  272: >>> from transformers import pipeline
  273  

  289  ```py
  290: >>> from transformers import AutoFeatureExtractor
  291  

  298  ```py
  299: >>> from transformers import AutoModelForAudioClassification
  300  

train_real_world/transformers_4573/docs/source/en/tasks/document_question_answering.md:
  161  ```py
  162: >>> from transformers import AutoProcessor
  163  

  377  ```py
  378: >>> from transformers import AutoModelForDocumentQuestionAnswering
  379  

  387  ```py
  388: >>> from transformers import TrainingArguments
  389  

  409  ```py
  410: >>> from transformers import DefaultDataCollator
  411  

  417  ```py
  418: >>> from transformers import Trainer
  419  

  459  ```py
  460: >>> from transformers import pipeline
  461  

  480  >>> import torch
  481: >>> from transformers import AutoProcessor
  482: >>> from transformers import AutoModelForDocumentQuestionAnswering
  483  

train_real_world/transformers_4573/docs/source/en/tasks/idefics.md:
   75  
   76: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
   77  

   93  >>> import torch
   94: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig
   95  

  389  >>> import torch
  390: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
  391  

train_real_world/transformers_4573/docs/source/en/tasks/image_captioning.md:
  116  ```python
  117: from transformers import AutoProcessor
  118  

  144  ```python
  145: from transformers import AutoModelForCausalLM
  146  

  178  ```python
  179: from transformers import TrainingArguments, Trainer
  180  

train_real_world/transformers_4573/docs/source/en/tasks/image_classification.md:
  104  ```py
  105: >>> from transformers import AutoImageProcessor
  106  

  144  ```py
  145: >>> from transformers import DefaultDataCollator
  146  

  186  ```py
  187: >>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer
  188  

  263  ```py
  264: >>> from transformers import pipeline
  265  

  279  ```py
  280: >>> from transformers import AutoImageProcessor
  281  >>> import torch

  289  ```py
  290: >>> from transformers import AutoModelForImageClassification
  291  

train_real_world/transformers_4573/docs/source/en/tasks/image_feature_extraction.md:
   44  import torch
   45: from transformers import pipeline
   46  from accelerate import Accelerator

  102  ```python
  103: from transformers import AutoImageProcessor, AutoModel
  104  

train_real_world/transformers_4573/docs/source/en/tasks/image_text_to_text.md:
   42  ```python
   43: from transformers import AutoProcessor, AutoModelForImageTextToText
   44  from accelerate import Accelerator

  133  ```python
  134: from transformers import pipeline
  135  pipe = pipeline("image-text-to-text", model="llava-hf/llava-interleave-qwen-0.5b-hf")

  217  import time
  218: from transformers import TextIteratorStreamer
  219  from threading import Thread

  303  ```python
  304: from transformers import AutoModelForImageTextToText, QuantoConfig
  305  

train_real_world/transformers_4573/docs/source/en/tasks/image_to_image.md:
  38  ```python
  39: from transformers import pipeline
  40  from accelerate import Accelerator

  80  ```python
  81: from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor 
  82  

train_real_world/transformers_4573/docs/source/en/tasks/keypoint_detection.md:
  30  ```python
  31: from transformers import AutoImageProcessor, SuperPointForKeypointDetection
  32  processor = AutoImageProcessor.from_pretrained("magic-leap-community/superpoint")

train_real_world/transformers_4573/docs/source/en/tasks/keypoint_matching.md:
   26  ```python
   27: from transformers import AutoImageProcessor, AutoModelForKeypointMatching
   28  import torch

   45  ```python
   46: from transformers.image_utils import load_image
   47  image1 = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg")

  105  ```python
  106: from transformers import pipeline 
  107  

train_real_world/transformers_4573/docs/source/en/tasks/knowledge_distillation_for_image_classification.md:
   42  ```python
   43: from transformers import AutoImageProcessor
   44  teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

   55  ```python
   56: from transformers import TrainingArguments, Trainer
   57  from accelerate import Accelerator

  105  ```python
  106: from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification
  107  

  154  ```python
  155: from transformers import DefaultDataCollator
  156  

train_real_world/transformers_4573/docs/source/en/tasks/language_modeling.md:
  103  ```py
  104: >>> from transformers import AutoTokenizer
  105  

  194  ```py
  195: >>> from transformers import DataCollatorForLanguageModeling
  196  

  211  ```py
  212: >>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
  213  

  279  ```py
  280: >>> from transformers import pipeline
  281  

  289  ```py
  290: >>> from transformers import AutoTokenizer
  291  

  299  ```py
  300: >>> from transformers import AutoModelForCausalLM
  301  

train_real_world/transformers_4573/docs/source/en/tasks/mask_generation.md:
   54  ```python
   55: >>> from transformers import pipeline
   56  

  119  ```python
  120: from transformers import SamModel, SamProcessor
  121  from accelerate import Accelerator

  347  ```python
  348: from transformers import Sam2Processor
  349  

  406  ```python
  407: from transformers import Sam2Model
  408  

train_real_world/transformers_4573/docs/source/en/tasks/masked_language_modeling.md:
   99  ```py
  100: >>> from transformers import AutoTokenizer
  101  

  187  ```py
  188: >>> from transformers import DataCollatorForLanguageModeling
  189  

  204  ```py
  205: >>> from transformers import AutoModelForMaskedLM
  206  

  273  ```py
  274: >>> from transformers import pipeline
  275  

  294  ```py
  295: >>> from transformers import AutoTokenizer
  296  

  304  ```py
  305: >>> from transformers import AutoModelForMaskedLM
  306  

train_real_world/transformers_4573/docs/source/en/tasks/monocular_depth_estimation.md:
   53  ```py
   54: >>> from transformers import pipeline
   55  from accelerate import Accelerator

  104  ```py
  105: >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  106  

train_real_world/transformers_4573/docs/source/en/tasks/multiple_choice.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  117  ```py
  118: >>> from transformers import DataCollatorForMultipleChoice
  119  >>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)

  156  ```py
  157: >>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer
  158  

  222  ```py
  223: >>> from transformers import AutoTokenizer
  224  

  232  ```py
  233: >>> from transformers import AutoModelForMultipleChoice
  234  

train_real_world/transformers_4573/docs/source/en/tasks/object_detection.md:
   205  ```py
   206: >>> from transformers import AutoImageProcessor
   207  

   392  ```py
   393: >>> from transformers.image_transforms import center_to_corners_format
   394  

   519  ```py
   520: >>> from transformers import AutoModelForObjectDetection
   521  

   541  ```py
   542: >>> from transformers import TrainingArguments
   543  

   568  ```py
   569: >>> from transformers import Trainer
   570  

  1487  >>> from PIL import Image, ImageDraw
  1488: >>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
  1489  

train_real_world/transformers_4573/docs/source/en/tasks/prompting.md:
   25  ```py
   26: from transformers import pipeline
   27  import torch

   85  ```python
   86: from transformers import pipeline
   87  import torch

  110  ```python
  111: from transformers import pipeline
  112  import torch

  143  ```py
  144: from transformers import pipeline
  145  import torch

  191  ```py
  192: from transformers import pipeline
  193  import torch

  210  ```py
  211: from transformers import pipeline
  212  import torch

  229  ```py
  230: from transformers import pipeline
  231  import torch

  248  ```py
  249: from transformers import pipeline
  250  import torch

train_real_world/transformers_4573/docs/source/en/tasks/question_answering.md:
   93  ```py
   94: >>> from transformers import AutoTokenizer
   95  

  170  ```py
  171: >>> from transformers import DefaultDataCollator
  172  

  186  ```py
  187: >>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
  188  

  254  ```py
  255: >>> from transformers import pipeline
  256  

  269  ```py
  270: >>> from transformers import AutoTokenizer
  271  

  279  >>> import torch
  280: >>> from transformers import AutoModelForQuestionAnswering
  281  

train_real_world/transformers_4573/docs/source/en/tasks/semantic_segmentation.md:
   50  ```python
   51: from transformers import pipeline
   52  from PIL import Image

  320  ```py
  321: >>> from transformers import AutoImageProcessor
  322  

  414  ```py
  415: >>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer
  416  

train_real_world/transformers_4573/docs/source/en/tasks/sequence_classification.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  100  ```py
  101: >>> from transformers import DataCollatorWithPadding
  102  

  147  ```py
  148: >>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
  149  

  219  ```py
  220: >>> from transformers import pipeline
  221  

  231  ```py
  232: >>> from transformers import AutoTokenizer
  233  

  240  ```py
  241: >>> from transformers import AutoModelForSequenceClassification
  242  

train_real_world/transformers_4573/docs/source/en/tasks/summarization.md:
   87  ```py
   88: >>> from transformers import AutoTokenizer
   89  

  122  ```py
  123: >>> from transformers import DataCollatorForSeq2Seq
  124  

  170  ```py
  171: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  172  

  235  ```py
  236: >>> from transformers import pipeline
  237  

  247  ```py
  248: >>> from transformers import AutoTokenizer
  249  

  256  ```py
  257: >>> from transformers import AutoModelForSeq2SeqLM
  258  

train_real_world/transformers_4573/docs/source/en/tasks/text-to-speech.md:
   28  ```python
   29: >>> from transformers import pipeline
   30  

   47  >>> from datasets import Audio, load_dataset
   48: >>> from transformers import pipeline
   49  

   69  ```python
   70: >>> from transformers import pipeline
   71  

  158  ```py
  159: >>> from transformers import SpeechT5Processor
  160  

  488  ```py
  489: >>> from transformers import SpeechT5ForTextToSpeech
  490  

  503  ```python
  504: >>> from transformers import Seq2SeqTrainingArguments
  505  

  530  ```py
  531: >>> from transformers import Seq2SeqTrainer
  532  

  571  ```py
  572: >>> from transformers import pipeline
  573  

train_real_world/transformers_4573/docs/source/en/tasks/token_classification.md:
  105  ```py
  106: >>> from transformers import AutoTokenizer
  107  

  160  ```py
  161: >>> from transformers import DataCollatorForTokenClassification
  162  

  253  ```py
  254: >>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
  255  

  319  ```py
  320: >>> from transformers import pipeline
  321  

  360  ```py
  361: >>> from transformers import AutoTokenizer
  362  

  369  ```py
  370: >>> from transformers import AutoModelForTokenClassification
  371  

train_real_world/transformers_4573/docs/source/en/tasks/training_vision_backbone.md:
   38  ```py
   39: from transformers import DetrConfig, DetrForObjectDetection, AutoImageProcessor
   40  

  168  ```py
  169: from transformers import Trainer, TrainingArguments
  170  

  210  ```py
  211: from transformers import pipeline
  212  

train_real_world/transformers_4573/docs/source/en/tasks/translation.md:
   83  ```py
   84: >>> from transformers import AutoTokenizer
   85  

  117  ```py
  118: >>> from transformers import DataCollatorForSeq2Seq
  119  

  178  ```py
  179: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  180  

  243  ```py
  244: >>> from transformers import pipeline
  245  

  258  ```py
  259: >>> from transformers import AutoTokenizer
  260  

  267  ```py
  268: >>> from transformers import AutoModelForSeq2SeqLM
  269  

train_real_world/transformers_4573/docs/source/en/tasks/video_classification.md:
  169  ```py
  170: >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification
  171  

  366  ```py
  367: >>> from transformers import TrainingArguments, Trainer
  368  

  465  ```py
  466: >>> from transformers import pipeline
  467  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/tasks/video_text_to_text.md:
  47  ```python
  48: from transformers import AutoProcessor, LlavaForConditionalGeneration
  49  import torch

train_real_world/transformers_4573/docs/source/en/tasks/visual_document_retrieval.md:
  54  import torch
  55: from transformers import ColPaliForRetrieval, ColPaliProcessor
  56  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/tasks/visual_question_answering.md:
  180  ```py
  181: >>> from transformers import ViltProcessor
  182  

  238  ```py
  239: >>> from transformers import DefaultDataCollator
  240  

  249  ```py
  250: >>> from transformers import ViltForQuestionAnswering
  251  

  259  ```py
  260: >>> from transformers import TrainingArguments
  261  

  279  ```py
  280: >>> from transformers import Trainer
  281  

  308  ```py
  309: >>> from transformers import pipeline
  310  

  366  ```py
  367: >>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
  368  from accelerate import Accelerator

train_real_world/transformers_4573/docs/source/en/tasks/zero_shot_image_classification.md:
  50  ```python
  51: >>> from transformers import pipeline
  52  

  94  ```py
  95: >>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification
  96  

train_real_world/transformers_4573/docs/source/en/tasks/zero_shot_object_detection.md:
   48  ```python
   49: >>> from transformers import pipeline
   50  

   59  ```py
   60: >>> from transformers.image_utils import load_image
   61  

  134  ```py
  135: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  136  

  250  ```py
  251: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  252  

train_real_world/transformers_4573/docs/source/es/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/es/add_new_pipeline.md:
   32  ```python
   33: from transformers import Pipeline
   34  

  126  ```python
  127: from transformers.pipelines import PIPELINE_REGISTRY
  128  

  156  
  157: from transformers import Pipeline
  158  

  194  from pair_classification import PairClassificationPipeline
  195: from transformers.pipelines import PIPELINE_REGISTRY
  196: from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
  197  

  209  ```py
  210: from transformers import pipeline
  211  

  226  ```py
  227: from transformers import pipeline
  228  

train_real_world/transformers_4573/docs/source/es/autoclass_tutorial.md:
  40  ```py
  41: >>> from transformers import AutoTokenizer
  42  

  62  ```py
  63: >>> from transformers import AutoFeatureExtractor
  64  

  76  ```py
  77: >>> from transformers import AutoProcessor
  78  

  86  ```py
  87: >>> from transformers import AutoModelForSequenceClassification
  88  

  94  ```py
  95: >>> from transformers import AutoModelForTokenClassification
  96  

train_real_world/transformers_4573/docs/source/es/chat_templating.md:
   26  ```python
   27: >>> from transformers import AutoTokenizer
   28  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

   42  ```python
   43: >>> from transformers import AutoTokenizer
   44  >>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

   64  ```python
   65: from transformers import AutoModelForCausalLM, AutoTokenizer
   66  

  116  ```python
  117: from transformers import pipeline
  118  

  184  ```python
  185: from transformers import AutoTokenizer
  186  from datasets import Dataset

  219  ```python
  220: >>> from transformers import AutoTokenizer
  221  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

train_real_world/transformers_4573/docs/source/es/create_a_model.md:
   33  ```py
   34: >>> from transformers import DistilBertConfig
   35  

  117  ```py
  118: >>> from transformers import DistilBertModel
  119  

  146  ```py
  147: >>> from transformers import DistilBertForSequenceClassification
  148  

  155  ```py
  156: >>> from transformers import DistilBertForQuestionAnswering
  157  

  178  ```py
  179: >>> from transformers import DistilBertTokenizer
  180  

  187  ```py
  188: >>> from transformers import DistilBertTokenizer
  189  

  196  ```py
  197: >>> from transformers import DistilBertTokenizerFast
  198  

  215  ```py
  216: >>> from transformers import ViTFeatureExtractor
  217  

  247  ```py
  248: >>> from transformers import ViTFeatureExtractor
  249  

  274  ```py
  275: >>> from transformers import Wav2Vec2FeatureExtractor
  276  

  297  ```py
  298: >>> from transformers import Wav2Vec2FeatureExtractor
  299  

  305  ```py
  306: >>> from transformers import Wav2Vec2CTCTokenizer
  307  

  314  ```py
  315: >>> from transformers import Wav2Vec2Processor
  316  

train_real_world/transformers_4573/docs/source/es/custom_models.md:
   40  ```python
   41: from transformers import PreTrainedConfig
   42  from typing import List

  118  ```py
  119: from transformers import PreTrainedModel
  120  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  319  ```py
  320: from transformers import AutoModelForImageClassification
  321  

  348  ```py
  349: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  350  

train_real_world/transformers_4573/docs/source/es/fast_tokenizers.md:
  47  ```python
  48: >>> from transformers import PreTrainedTokenizerFast
  49  

  67  ```python
  68: >>> from transformers import PreTrainedTokenizerFast
  69  

train_real_world/transformers_4573/docs/source/es/glossary.md:
   33  ```python
   34: >>> from transformers import BertTokenizer
   35  

  188  ```python
  189: >>> from transformers import BertTokenizer
  190  

  415  ```python
  416: >>> from transformers import BertTokenizer
  417  

train_real_world/transformers_4573/docs/source/es/index.md:
  107  1. **[mLUKE](model_doc/mluke)** (de Studio Ousia) publicado con el paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) por Ryokan Ri, Ikuya Yamada, y Yoshimasa Tsuruoka.
  108: 1. **[LXMERT](model_doc/lxmert)** (de UNC Chapel Hill) publicado con el paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) por Hao Tan y Mohit Bansal.
  109  1. **[M2M100](model_doc/m2m_100)** (de Facebook) publicado con el paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) por Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.

train_real_world/transformers_4573/docs/source/es/installation.md:
   76  ```bash
   77: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   78  ```

   97  ```bash
   98: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
   99  ```

  186      ```py
  187:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  188  

  226  ```py
  227: >>> from transformers import AutoConfig
  228  

train_real_world/transformers_4573/docs/source/es/model_memory_anatomy.md:
   89  ```py
   90: >>> from transformers import AutoModelForSequenceClassification
   91  

  149  ```py
  150: >>> from transformers import TrainingArguments, Trainer, logging
  151  

train_real_world/transformers_4573/docs/source/es/model_sharing.md:
  145  ```py
  146: >>> from transformers import PushToHubCallback
  147  

  171  ```py
  172: >>> from transformers import AutoModel
  173  

train_real_world/transformers_4573/docs/source/es/multilingual.md:
   44  >>> import torch
   45: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   46  

  119  ```py
  120: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  121  

  155  ```py
  156: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  157  

train_real_world/transformers_4573/docs/source/es/perplexity.md:
  56  ```python
  57: from transformers import GPT2LMHeadModel, GPT2TokenizerFast
  58  

train_real_world/transformers_4573/docs/source/es/pipeline_tutorial.md:
   37  ```py
   38: >>> from transformers import pipeline
   39  

  188  # KeyDataset is a util that will just output the item we're interested in.
  189: from transformers.pipelines.pt_utils import KeyDataset
  190  from datasets import load_dataset

  215  ```py
  216: >>> from transformers import pipeline
  217  

  231  ```py
  232: >>> from transformers import pipeline
  233  

  250  ```py
  251: >>> from transformers import pipeline
  252  

  282  import torch
  283: from transformers import pipeline
  284  

  293  import torch
  294: from transformers import pipeline
  295  

  312  ```py
  313: from transformers import pipeline
  314  import gradio as gr

train_real_world/transformers_4573/docs/source/es/pipeline_webserver.md:
  23  from starlette.routing import Route
  24: from transformers import pipeline
  25  import asyncio

train_real_world/transformers_4573/docs/source/es/preprocessing.md:
   45  ```py
   46: >>> from transformers import AutoTokenizer
   47  

  261  ```py
  262: >>> from transformers import AutoFeatureExtractor
  263  

  344  ```py
  345: >>> from transformers import AutoFeatureExtractor
  346  

  466  ```py
  467: >>> from transformers import AutoProcessor
  468  

train_real_world/transformers_4573/docs/source/es/quicktour.md:
   76  ```py
   77: >>> from transformers import pipeline
   78  

  100  >>> import torch
  101: >>> from transformers import pipeline
  102  

  143  ```py
  144: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  145  

  176  ```py
  177: >>> from transformers import AutoTokenizer
  178  

  219  ```py
  220: >>> from transformers import AutoModelForSequenceClassification
  221  

  286  ```py
  287: >>> from transformers import AutoModel
  288  

train_real_world/transformers_4573/docs/source/es/task_summary.md:
   38  ```py
   39: >>> from transformers import pipeline
   40  

   57  ```py
   58: >>> from transformers import pipeline
   59  

   83  ```py
   84: >>> from transformers import pipeline
   85  

  107  ```py
  108: >>> from transformers import pipeline
  109  

  130  ```py
  131: >>> from transformers import pipeline
  132  

  153  ```py
  154: >>> from transformers import pipeline
  155  

  173  ```py
  174: >>> from transformers import pipeline
  175  

  192  ```py
  193: >>> from transformers import pipeline
  194  

  227  ```py
  228: >>> from transformers import pipeline
  229  

  250  ```py
  251: >>> from transformers import pipeline
  252  

  266  ```py
  267: >>> from transformers import pipeline
  268  

  283      ```py
  284:     >>> from transformers import pipeline
  285  

  323  ```py
  324: >>> from transformers import pipeline
  325  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/es/tokenizer_summary.md:
  73  ```py
  74: >>> from transformers import BertTokenizer
  75  

  85  ```py
  86: >>> from transformers import XLNetTokenizer
  87  

train_real_world/transformers_4573/docs/source/es/trainer.md:
   53  ```py
   54: from transformers import TrainingArguments
   55  

   74  ```py
   75: from transformers import Trainer
   76  

  128  from torch import nn
  129: from transformers import Trainer
  130  

  148  ```py
  149: from transformers import TrainerCallback
  150  

  164  ```py
  165: from transformers import Trainer
  166  

  243  ```py
  244: from transformers import TrainingArguments, Trainer
  245  

train_real_world/transformers_4573/docs/source/es/training.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   78  ```py
   79: >>> from transformers import AutoModelForSequenceClassification
   80  

   97  ```py
   98: >>> from transformers import TrainingArguments
   99  

  125  ```py
  126: >>> from transformers import TrainingArguments
  127  

  163  ```py
  164: >>> from transformers import DefaultDataCollator
  165  

  200  >>> import tensorflow as tf
  201: >>> from transformers import TFAutoModelForSequenceClassification
  202  

  275  ```py
  276: >>> from transformers import AutoModelForSequenceClassification
  277  

  293  ```py
  294: >>> from transformers import get_scheduler
  295  

train_real_world/transformers_4573/docs/source/es/tasks/asr.md:
  106  ```py
  107: >>> from transformers import AutoProcessor
  108  

  235  ```py
  236: >>> from transformers import AutoModelForCTC, TrainingArguments, Trainer
  237  

  314  ```py
  315: >>> from transformers import pipeline
  316  

  332  ```py
  333: >>> from transformers import AutoProcessor
  334  

  341  ```py
  342: >>> from transformers import AutoModelForCTC
  343  

train_real_world/transformers_4573/docs/source/es/tasks/audio_classification.md:
  126  ```py
  127: >>> from transformers import AutoFeatureExtractor
  128  

  198  ```py
  199: >>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer
  200  

  271  ```py
  272: >>> from transformers import pipeline
  273  

  289  ```py
  290: >>> from transformers import AutoFeatureExtractor
  291  

  298  ```py
  299: >>> from transformers import AutoModelForAudioClassification
  300  

train_real_world/transformers_4573/docs/source/es/tasks/image_captioning.md:
  115  ```python
  116: from transformers import AutoProcessor
  117  

  143  ```python
  144: from transformers import AutoModelForCausalLM
  145  

  177  ```python
  178: from transformers import TrainingArguments, Trainer
  179  

train_real_world/transformers_4573/docs/source/es/tasks/image_classification.md:
   78  ```py
   79: >>> from transformers import AutoImageProcessor
   80  

  110  ```py
  111: >>> from transformers import DefaultDataCollator
  112  

  119  ```py
  120: >>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer
  121  

train_real_world/transformers_4573/docs/source/es/tasks/language_modeling.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

   89  ```py
   90: >>> from transformers import AutoTokenizer
   91  

  165  ```py
  166: >>> from transformers import DataCollatorForLanguageModeling
  167  

  174  ```py
  175: >>> from transformers import DataCollatorForLanguageModeling
  176  

  189  ```py
  190: >>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
  191  

  234  ```py
  235: >>> from transformers import AutoModelForMaskedLM
  236  

train_real_world/transformers_4573/docs/source/es/tasks/multiple_choice.md:
   58  ```py
   59: >>> from transformers import AutoTokenizer
   60  

   98  ```py
   99: >>> from transformers import DataCollatorForMultipleChoice
  100  >>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)

  107  ```py
  108: >>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer
  109  

train_real_world/transformers_4573/docs/source/es/tasks/question_answering.md:
   64  ```py
   65: >>> from transformers import AutoTokenizer
   66  

  141  ```py
  142: >>> from transformers import DefaultDataCollator
  143  

  151  ```py
  152: >>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
  153  

train_real_world/transformers_4573/docs/source/es/tasks/summarization.md:
   65  ```py
   66: >>> from transformers import AutoTokenizer
   67  

   99  ```py
  100: >>> from transformers import DataCollatorForSeq2Seq
  101  

  109  ```py
  110: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  111  

train_real_world/transformers_4573/docs/source/fr/autoclass_tutorial.md:
   41  ```py
   42: >>> from transformers import AutoTokenizer
   43  

   61  ```py
   62: >>> from transformers import AutoImageProcessor
   63  

   88  ```py
   89: >>> from transformers import AutoImageProcessor, AutoBackbone
   90  >>> import torch

  117  ```py
  118: >>> from transformers import AutoFeatureExtractor
  119  

  131  ```py
  132: >>> from transformers import AutoProcessor
  133  

  141  ```py
  142: >>> from transformers import AutoModelForSequenceClassification
  143  

  149  ```py
  150: >>> from transformers import AutoModelForTokenClassification
  151  

train_real_world/transformers_4573/docs/source/fr/index.md:
  141  1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.
  142: 1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.
  143  1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.

train_real_world/transformers_4573/docs/source/fr/installation.md:
   92  ```bash
   93: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   94  ```

  114  ```bash
  115: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  116  ```

  185  ```py
  186: from transformers import T5Model
  187  

  203      ```py
  204:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  205  

  242  ```py
  243: >>> from transformers import AutoConfig
  244  

train_real_world/transformers_4573/docs/source/fr/quicktour.md:
   60  ```py
   61: >>> from transformers import pipeline
   62  

   86  >>> import torch
   87: >>> from transformers import pipeline
   88  

  127  ```py
  128: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  129  

  158  ```py
  159: >>> from transformers import AutoTokenizer
  160  

  203  ```py
  204: >>> from transformers import AutoModelForSequenceClassification
  205  

  258  ```py
  259: >>> from transformers import AutoModel
  260  

  271  ```py
  272: >>> from transformers import AutoConfig
  273  

  279  ```py
  280: >>> from transformers import AutoModel
  281  

  295     ```py
  296:    >>> from transformers import AutoModelForSequenceClassification
  297  

  303     ```py
  304:    >>> from transformers import TrainingArguments
  305  

  317     ```py
  318:    >>> from transformers import AutoTokenizer
  319  

  346     ```py
  347:    >>> from transformers import DataCollatorWithPadding
  348  

  354  ```py
  355: >>> from transformers import Trainer
  356  

  389     ```py
  390:    >>> from transformers import TFAutoModelForSequenceClassification
  391  

  397     ```py
  398:    >>> from transformers import AutoTokenizer
  399  

train_real_world/transformers_4573/docs/source/fr/task_summary.md:
   38  ```py
   39: >>> from transformers import pipeline
   40  

   57  ```py
   58: >>> from transformers import pipeline
   59  

   83  ```py
   84: >>> from transformers import pipeline
   85  

  107  ```py
  108: >>> from transformers import pipeline
  109  

  130  ```py
  131: >>> from transformers import pipeline
  132  

  153  ```py
  154: >>> from transformers import pipeline
  155  

  173  ```py
  174: >>> from transformers import pipeline
  175  

  192  ```py
  193: >>> from transformers import pipeline
  194  

  228  ```py
  229: >>> from transformers import pipeline
  230  

  251  ```py
  252: >>> from transformers import pipeline
  253  

  267  ```py
  268: >>> from transformers import pipeline
  269  

  284      ```py
  285:     >>> from transformers import pipeline
  286  

  324  ```py
  325: >>> from transformers import pipeline
  326  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/fr/tutoriel_pipeline.md:
   25  ```py
   26: >>> from transformers import pipeline
   27  

  176  # KeyDataset is a util that will just output the item we're interested in.
  177: from transformers.pipelines.pt_utils import KeyDataset
  178  from datasets import load_dataset

  203  ```py
  204: >>> from transformers import pipeline
  205  

  220  ```py
  221: >>> from transformers import pipeline
  222  

  240  ```py
  241: >>> from transformers import pipeline
  242  

  272  import torch
  273: from transformers import pipeline
  274  

  283  import torch
  284: from transformers import pipeline, BitsAndBytesConfig
  285  

  301  ```py
  302: from transformers import pipeline
  303  import gradio as gr

train_real_world/transformers_4573/docs/source/hi/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/hi/pipeline_tutorial.md:
   41  ```py
   42: >>> from transformers import pipeline
   43  

  205  # KeyDataset is a util that will just output the item we're interested in.
  206: from transformers.pipelines.pt_utils import KeyDataset
  207  from datasets import load_dataset

  234  ```py
  235: >>> from transformers import pipeline
  236  

  250  ```py
  251: >>> from transformers import pipeline
  252  

  269  ```py
  270: >>> from transformers import pipeline
  271  

  301  import torch
  302: from transformers import pipeline
  303  

  312  import torch
  313: from transformers import pipeline, BitsAndBytesConfig
  314  

train_real_world/transformers_4573/docs/source/it/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/it/add_new_model.md:
  445  ```python
  446: from transformers import BrandNewBertModel, BrandNewBertConfig
  447  

  680  ```python
  681: from transformers import BrandNewBertTokenizer
  682  

train_real_world/transformers_4573/docs/source/it/add_new_pipeline.md:
   33  ```python
   34: from transformers import Pipeline
   35  

  118  ```python
  119: from transformers.pipelines import PIPELINE_REGISTRY
  120  

  147  
  148: from transformers import Pipeline
  149  

  184  from pair_classification import PairClassificationPipeline
  185: from transformers.pipelines import PIPELINE_REGISTRY
  186: from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
  187  

  199  ```py
  200: from transformers import pipeline
  201  

  216  ```py
  217: from transformers import pipeline
  218  

train_real_world/transformers_4573/docs/source/it/autoclass_tutorial.md:
  40  ```py
  41: >>> from transformers import AutoTokenizer
  42  

  61  ```py
  62: >>> from transformers import AutoFeatureExtractor
  63  

  75  ```py
  76: >>> from transformers import AutoProcessor
  77  

  85  ```py
  86: >>> from transformers import AutoModelForSequenceClassification
  87  

  93  ```py
  94: >>> from transformers import AutoModelForTokenClassification
  95  

train_real_world/transformers_4573/docs/source/it/big_models.md:
   42  ```py
   43: from transformers import AutoModel
   44  

  111  ```py
  112: >>> from transformers.trainer_utils import load_sharded_checkpoint
  113  

train_real_world/transformers_4573/docs/source/it/create_a_model.md:
   33  ```py
   34: >>> from transformers import DistilBertConfig
   35  

  116  ```py
  117: >>> from transformers import DistilBertModel
  118  

  143  ```py
  144: >>> from transformers import DistilBertForSequenceClassification
  145  

  151  ```py
  152: >>> from transformers import DistilBertForQuestionAnswering
  153  

  174  ```py
  175: >>> from transformers import DistilBertTokenizer
  176  

  182  ```py
  183: >>> from transformers import DistilBertTokenizer
  184  

  190  ```py
  191: >>> from transformers import DistilBertTokenizerFast
  192  

  208  ```py
  209: >>> from transformers import ViTFeatureExtractor
  210  

  240  ```py
  241: >>> from transformers import ViTFeatureExtractor
  242  

  266  ```py
  267: >>> from transformers import Wav2Vec2FeatureExtractor
  268  

  288  ```py
  289: >>> from transformers import Wav2Vec2FeatureExtractor
  290  

  296  ```py
  297: >>> from transformers import Wav2Vec2CTCTokenizer
  298  

  304  ```py
  305: >>> from transformers import Wav2Vec2Processor
  306  

train_real_world/transformers_4573/docs/source/it/custom_models.md:
   39  ```python
   40: from transformers import PreTrainedConfig
   41  from typing import List

  119  ```py
  120: from transformers import PreTrainedModel
  121  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  320  ```py
  321: from transformers import AutoModelForImageClassification
  322  

  348  ```py
  349: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  350  

train_real_world/transformers_4573/docs/source/it/index.md:
  115  1. **[mLUKE](model_doc/mluke)** (da Studio Ousia) rilasciato con il paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) da Ryokan Ri, Ikuya Yamada, e Yoshimasa Tsuruoka.
  116: 1. **[LXMERT](model_doc/lxmert)** (da UNC Chapel Hill) rilasciato con il paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) da Hao Tan e Mohit Bansal.
  117  1. **[M2M100](model_doc/m2m_100)** (da Facebook) rilasciato con il paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) da Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.

train_real_world/transformers_4573/docs/source/it/installation.md:
   73  ```bash
   74: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   75  ```

   95  ```bash
   96: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
   97  ```

  184      ```py
  185:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  186  

  223  ```py
  224: >>> from transformers import AutoConfig
  225  

train_real_world/transformers_4573/docs/source/it/migration.md:
   42  ```py
   43: from transformers import AutoTokenizer
   44  

   48  ```py
   49: from transformers import AutoTokenizer
   50  

   95  ```bash
   96: from transformers.modeling_bert import BertLayer
   97  ```

   99  ```bash
  100: from transformers.models.bert.modeling_bert import BertLayer
  101  ```

train_real_world/transformers_4573/docs/source/it/model_sharing.md:
  131  ```py
  132: >>> from transformers import AutoModel
  133  

train_real_world/transformers_4573/docs/source/it/multilingual.md:
   44  >>> import torch
   45: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   46  

  118  ```py
  119: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  120  

  154  ```py
  155: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  156  

train_real_world/transformers_4573/docs/source/it/perf_infer_gpu_one.md:
  53  ```py
  54: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  55  

  67  ```py
  68: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  69  

train_real_world/transformers_4573/docs/source/it/pipeline_tutorial.md:
   37  ```py
   38: >>> from transformers import pipeline
   39  

   76  ```py
   77: >>> from transformers import AutoTokenizer, AutoModelForCausalLM
   78  

   85  ```py
   86: >>> from transformers import pipeline
   87  

  117  ```py
  118: >>> from transformers import pipeline
  119  

  142  ```py
  143: >>> from transformers import pipeline
  144  

train_real_world/transformers_4573/docs/source/it/preprocessing.md:
   45  ```py
   46: >>> from transformers import AutoTokenizer
   47  

  260  ```py
  261: >>> from transformers import AutoFeatureExtractor
  262  

  344  ```py
  345: >>> from transformers import AutoFeatureExtractor
  346  

  466  ```py
  467: >>> from transformers import AutoProcessor
  468  

train_real_world/transformers_4573/docs/source/it/quicktour.md:
   76  ```py
   77: >>> from transformers import pipeline
   78  

  110  >>> import torch
  111: >>> from transformers import pipeline
  112  

  153  ```py
  154: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  155  

  184  ```py
  185: >>> from transformers import AutoTokenizer
  186  

  226  ```py
  227: >>> from transformers import AutoModelForSequenceClassification
  228  

  291  ```py
  292: >>> from transformers import AutoModel
  293  

train_real_world/transformers_4573/docs/source/it/training.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   78  ```py
   79: >>> from transformers import AutoModelForSequenceClassification
   80  

   96  ```py
   97: >>> from transformers import TrainingArguments
   98  

  124  ```py
  125: >>> from transformers import TrainingArguments, Trainer
  126  

  207  ```py
  208: >>> from transformers import AutoModelForSequenceClassification
  209  

  225  ```py
  226: >>> from transformers import get_scheduler
  227  

train_real_world/transformers_4573/docs/source/ja/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/ja/add_new_model.md:
  393  ```python
  394: from transformers import BrandNewBertModel, BrandNewBertConfig
  395  

  662  ```python
  663: from transformers import BrandNewBertTokenizer
  664  

train_real_world/transformers_4573/docs/source/ja/autoclass_tutorial.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   69  ```py
   70: >>> from transformers import AutoImageProcessor
   71  

   81  ```py
   82: >>> from transformers import AutoFeatureExtractor
   83  

   97  ```py
   98: >>> from transformers import AutoProcessor
   99  

  108  ```py
  109: >>> from transformers import AutoModelForSequenceClassification
  110  

  116  ```py
  117: >>> from transformers import AutoModelForTokenClassification
  118  

train_real_world/transformers_4573/docs/source/ja/big_models.md:
   42  ```py
   43: from transformers import AutoModel
   44  

  116  ```py
  117: >>> from transformers.trainer_utils import load_sharded_checkpoint
  118  

train_real_world/transformers_4573/docs/source/ja/chat_templating.md:
  43  ```python
  44: >>> from transformers import AutoTokenizer
  45  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

  62  ```python
  63: >> from transformers import AutoTokenizer
  64  >> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

  84  
  85: >>> from transformers import AutoTokenizer
  86  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

train_real_world/transformers_4573/docs/source/ja/create_a_model.md:
   34  ```py
   35: >>> from transformers import DistilBertConfig
   36  

  121  ```py
  122: >>> from transformers import DistilBertModel
  123  

  153  ```py
  154: >>> from transformers import DistilBertForSequenceClassification
  155  

  163  ```py
  164: >>> from transformers import DistilBertForQuestionAnswering
  165  

  191  ```py
  192: >>> from transformers import DistilBertTokenizer
  193  

  202  ```py
  203: >>> from transformers import DistilBertTokenizer
  204  

  210  ```py
  211: >>> from transformers import DistilBertTokenizerFast
  212  

  229  ```py
  230: >>> from transformers import ViTImageProcessor
  231  

  262  ```py
  263: >>> from transformers import ViTImageProcessor
  264  

  293  ```py
  294: >>> from transformers import Wav2Vec2FeatureExtractor
  295  

  317  ```py
  318: >>> from transformers import Wav2Vec2FeatureExtractor
  319  

  342  ```py
  343: >>> from transformers import Wav2Vec2FeatureExtractor
  344  

  350  ```py
  351: >>> from transformers import Wav2Vec2CTCTokenizer
  352  

  359  ```py
  360: >>> from transformers import Wav2Vec2Processor
  361  

train_real_world/transformers_4573/docs/source/ja/custom_models.md:
   31  ```python
   32: from transformers import PreTrainedConfig
   33  from typing import List

  106  ```py
  107: from transformers import PreTrainedModel
  108  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  300  ```py
  301: from transformers import AutoModelForImageClassification
  302  

  327  ```py
  328: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  329  

train_real_world/transformers_4573/docs/source/ja/fast_tokenizers.md:
  47  ```python
  48: >>> from transformers import PreTrainedTokenizerFast
  49  

  66  ```python
  67: >>> from transformers import PreTrainedTokenizerFast
  68  

train_real_world/transformers_4573/docs/source/ja/generation_strategies.md:
   41  ```python
   42: >>> from transformers import AutoModelForCausalLM
   43  

   79  ```python
   80: >>> from transformers import AutoModelForCausalLM, GenerationConfig
   81  

   94  ```python
   95: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig
   96  

  132  ```python
  133: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
  134  

  156  ```python
  157: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  158  

  177  ```python
  178: >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
  179  >>> set_seed(0)  # For reproducibility

  206  ```python
  207: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  208  

  227  ```python
  228: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
  229  >>> set_seed(0)  # For reproducibility

  253  ```python
  254: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  255  

  273  ```python
  274: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
  275  >>> set_seed(42)  # For reproducibility

train_real_world/transformers_4573/docs/source/ja/glossary.md:
   33  ```python
   34: >>> from transformers import BertTokenizer
   35  

  191  ```python
  192: >>> from transformers import BertTokenizer
  193  

  400  ```python
  401: >>> from transformers import BertTokenizer
  402  

train_real_world/transformers_4573/docs/source/ja/index.md:
  136  1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (Studio Ousia ã‹ã‚‰) Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057)
  137: 1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (UNC Chapel Hill ã‹ã‚‰) Hao Tan and Mohit Bansal ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490)
  138  1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (Facebook ã‹ã‚‰) Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161)

train_real_world/transformers_4573/docs/source/ja/installation.md:
   78  ```bash
   79: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   80  ```

  100  ```bash
  101: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  102  ```

  189      ```py
  190:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  191  

  228  ```py
  229: >>> from transformers import AutoConfig
  230  

train_real_world/transformers_4573/docs/source/ja/llm_tutorial.md:
   82  ```py
   83: >>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   84  

  100  ```py
  101: >>> from transformers import AutoTokenizer
  102  

  125  ```py
  126: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  127  

  158  >>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
  159: >>> from transformers import set_seed
  160  >>> set_seed(0)

train_real_world/transformers_4573/docs/source/ja/model_memory_anatomy.md:
   94  ```py
   95: >>> from transformers import AutoModelForSequenceClassification
   96  

  157  ```py
  158: >>> from transformers import TrainingArguments, Trainer, logging
  159  

train_real_world/transformers_4573/docs/source/ja/model_sharing.md:
  151  ```py
  152: >>> from transformers import AutoModel
  153  

train_real_world/transformers_4573/docs/source/ja/multilingual.md:
   44  >>> import torch
   45: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   46  

  118  ```py
  119: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  120  

  154  ```py
  155: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  156  

train_real_world/transformers_4573/docs/source/ja/peft.md:
   62  ```py
   63: from transformers import AutoModelForCausalLM, AutoTokenizer
   64  

   78  ```py
   79: from transformers import AutoModelForCausalLM, AutoTokenizer
   80  

   93  ```py
   94: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   95  

  105  ```py
  106: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  107  from peft import PeftConfig

  147  ```py
  148: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  149  from peft import PeftConfig

train_real_world/transformers_4573/docs/source/ja/perf_infer_gpu_one.md:
   43  import torch
   44: from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM
   45  

  100  import torch
  101: from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM
  102  

  118  import torch
  119: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM
  120  

  136  import torch
  137: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM
  138  from peft import LoraConfig

  192  ```py
  193: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  194  

  260  ```py
  261: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  262  

  273  ```py
  274: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  275  

train_real_world/transformers_4573/docs/source/ja/perf_torch_compile.md:
  28  ```diff
  29: from transformers import AutoModelForImageClassification
  30  

  48  import numpy as np
  49: from transformers import AutoImageProcessor, AutoModelForImageClassification
  50  

  66  ```python 
  67: from transformers import AutoImageProcessor, AutoModelForObjectDetection
  68  

  82  ```python 
  83: from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation
  84  

train_real_world/transformers_4573/docs/source/ja/perf_train_gpu_one.md:
  234  from torch import nn
  235: from transformers.trainer_pt_utils import get_parameter_names
  236  

train_real_world/transformers_4573/docs/source/ja/perplexity.md:
  55  ```python
  56: from transformers import GPT2LMHeadModel, GPT2TokenizerFast
  57  

train_real_world/transformers_4573/docs/source/ja/pipeline_tutorial.md:
   39  ```py
   40: >>> from transformers import pipeline
   41  

  182  # KeyDataset is a util that will just output the item we're interested in.
  183: from transformers.pipelines.pt_utils import KeyDataset
  184  from datasets import load_dataset

  209  ```py
  210: >>> from transformers import pipeline
  211  

  225  ```py
  226: >>> from transformers import pipeline
  227  

  245  ```py
  246: >>> from transformers import pipeline
  247  

  277  import torch
  278: from transformers import pipeline
  279  

  288  import torch
  289: from transformers import pipeline
  290  

train_real_world/transformers_4573/docs/source/ja/pipeline_webserver.md:
  24  from starlette.routing import Route
  25: from transformers import pipeline
  26  import asyncio

train_real_world/transformers_4573/docs/source/ja/pr_checks.md:
  168  ```py
  169: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  170  ```

  175  ```py
  176: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta
  177  ```

  184  ```py
  185: # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT
  186  ```

  200  ```py
  201: # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing
  202  ```

train_real_world/transformers_4573/docs/source/ja/preprocessing.md:
   59  ```python
   60: >>> from transformers import AutoTokenizer
   61  

  252  ```python
  253: >>> from transformers import AutoFeatureExtractor
  254  

  350  ```py
  351: >>> from transformers import AutoImageProcessor
  352  

  484  ```py
  485: >>> from transformers import AutoProcessor
  486  

train_real_world/transformers_4573/docs/source/ja/quicktour.md:
   70  ```python
   71: >>> from transformers import pipeline
   72  

   97  >>> import torch
   98: >>> from transformers import pipeline
   99  

  138  ```python
  139: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  140  

  177  ```python
  178: >>> from transformers import AutoTokenizer
  179  

  225  ```py
  226: >>> from transformers import AutoModelForSequenceClassification
  227  

  285  ```py
  286: >>> from transformers import AutoModel
  287  

  299  ```python
  300: >>> from transformers import AutoConfig
  301  

  307  ```python
  308: >>> from transformers import AutoModel
  309  

  326      ```py
  327:     >>> from transformers import AutoModelForSequenceClassification
  328  

  334     ```py
  335:    >>> from transformers import TrainingArguments
  336  

  348     ```py
  349:    >>> from transformers import AutoTokenizer
  350  

  377     ```py
  378:    >>> from transformers import DataCollatorWithPadding
  379  

  385  ```python
  386: >>> from transformers import Trainer
  387  

  421     ```py
  422:    >>> from transformers import TFAutoModelForSequenceClassification
  423  

  429     ```py
  430:    >>> from transformers import AutoTokenizer
  431  

train_real_world/transformers_4573/docs/source/ja/serialization.md:
   91  ```python
   92: >>> from transformers import AutoTokenizer
   93  >>> from optimum.onnxruntime import ORTModelForQuestionAnswering

  113  >>> from optimum.onnxruntime import ORTModelForSequenceClassification
  114: >>> from transformers import AutoTokenizer
  115  

  118  
  119: >>> # Load a model from transformers and export it to ONNX
  120  >>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)

train_real_world/transformers_4573/docs/source/ja/task_summary.md:
   39  ```py
   40: >>> from transformers import pipeline
   41  

   59  ```py
   60: >>> from transformers import pipeline
   61  

   87  ```py
   88: >>> from transformers import pipeline
   89  

  112  ```py
  113: >>> from transformers import pipeline
  114  

  137  ```py
  138: >>> from transformers import pipeline
  139  

  161  ```py
  162: >>> from transformers import pipeline
  163  

  183  ```py
  184: >>> from transformers import pipeline
  185  

  203  ```py
  204: >>> from transformers import pipeline
  205  

  239  ```py
  240: >>> from transformers import pipeline
  241  

  263  ```py
  264: >>> from transformers import pipeline
  265  

  280  ```py
  281: >>> from transformers import pipeline
  282  

  297      ```py
  298:     >>> from transformers import pipeline
  299  

  338  ```py
  339: >>> from transformers import pipeline
  340  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/ja/testing.md:
   189      >>> import torch
   190:     >>> from transformers import WhisperModel, WhisperFeatureExtractor
   191      >>> from datasets import load_dataset

   473  ```python
   474: from transformers.testing_utils import get_gpu_count
   475  

   693  ```python
   694: from transformers.testing_utils import TestCasePlus
   695  

   704  ```python
   705: from transformers.testing_utils import TestCasePlus
   706  

   724  ```python
   725: from transformers.testing_utils import TestCasePlus
   726  

   779  import os
   780: from transformers.testing_utils import ExtendSysPath
   781  

   883  ```python no-style
   884: from transformers.testing_utils import slow
   885  @slow

  1010  ```python
  1011: from transformers.testing_utils import CaptureStdout
  1012  

  1020  ```python
  1021: from transformers.testing_utils import CaptureStdout
  1022  

  1032  ```python
  1033: from transformers.testing_utils import CaptureStderr
  1034  

  1042  ```python
  1043: from transformers.testing_utils import CaptureStd
  1044  

  1058  ```python
  1059: from transformers import logging
  1060: from transformers.testing_utils import CaptureLogger
  1061  

  1075  ```python
  1076: from transformers.testing_utils import mockenv
  1077  

  1088  ```python
  1089: from transformers.testing_utils import TestCasePlus
  1090  

train_real_world/transformers_4573/docs/source/ja/tokenizer_summary.md:
  76  ```py
  77: >>> from transformers import BertTokenizer
  78  

  88  ```py
  89: >>> from transformers import XLNetTokenizer
  90  

train_real_world/transformers_4573/docs/source/ja/training.md:
   55  ```py
   56: >>> from transformers import AutoTokenizer
   57  

   89  ```py
   90: >>> from transformers import AutoModelForSequenceClassification
   91  

  109  ```python
  110: >>> from transformers import TrainingArguments
  111  

  138  ```python
  139: >>> from transformers import TrainingArguments, Trainer
  140  

  220  ```py
  221: >>> from transformers import AutoModelForSequenceClassification
  222  

  239  ```py
  240: >>> from transformers import get_scheduler
  241  

train_real_world/transformers_4573/docs/source/ja/troubleshooting.md:
   70  ```py
   71: >>> from transformers import TFPreTrainedModel
   72  

   79  ```py
   80: >>> from transformers import TFPreTrainedModel
   81  

  131  ```py
  132: >>> from transformers import AutoModelForSequenceClassification
  133  >>> import torch

  188  ```py
  189: >>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
  190  

train_real_world/transformers_4573/docs/source/ja/internal/generation_utils.md:
  29  ```python
  30: from transformers import GPT2Tokenizer, GPT2LMHeadModel
  31  

train_real_world/transformers_4573/docs/source/ja/main_classes/deepspeed.md:
  1608  ```python
  1609: from transformers.trainer_utils import get_last_checkpoint
  1610  from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

  1709  ```python
  1710: from transformers import T5ForConditionalGeneration, T5Config
  1711  import deepspeed

  1726  ```python
  1727: from transformers import AutoModel, Trainer, TrainingArguments
  1728  

  1811  ```bash
  1812: $ python -c 'from transformers import AutoModel; \
  1813  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

  1837  ```bash
  1838: $ python -c 'from transformers import AutoModel; \
  1839  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

  1980  ```python
  1981: from transformers.integrations import HfDeepSpeedConfig
  1982: from transformers import AutoModel
  1983  import deepspeed

  1995  ```python
  1996: from transformers.integrations import HfDeepSpeedConfig
  1997: from transformers import AutoModel, AutoConfig
  1998  import deepspeed

  2064  
  2065: from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
  2066: from transformers.integrations import HfDeepSpeedConfig
  2067  import deepspeed

train_real_world/transformers_4573/docs/source/ja/main_classes/logging.md:
  52  ```python
  53: from transformers.utils import logging
  54  

train_real_world/transformers_4573/docs/source/ja/main_classes/model.md:
  45  ```py
  46: from transformers import AutoModelForSeq2SeqLM
  47  

train_real_world/transformers_4573/docs/source/ja/main_classes/output.md:
  25  ```python
  26: from transformers import BertTokenizer, BertForSequenceClassification
  27  import torch

train_real_world/transformers_4573/docs/source/ja/main_classes/pipelines.md:
   66  import datasets
   67: from transformers import pipeline
   68: from transformers.pipelines.pt_utils import KeyDataset
   69  from tqdm.auto import tqdm

   85  ```python
   86: from transformers import pipeline
   87  

  117  ```python
  118: from transformers import pipeline
  119: from transformers.pipelines.pt_utils import KeyDataset
  120  import datasets

  142  ```python
  143: from transformers import pipeline
  144  from torch.utils.data import Dataset

train_real_world/transformers_4573/docs/source/ja/main_classes/quantization.md:
   74  ```python
   75: from transformers import AutoModelForCausalLM
   76  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)

   83  ```python
   84: from transformers import AutoModelForCausalLM
   85  model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)

  123  ```python
  124: from transformers import AutoModelForCausalLM
  125  model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq")

  130  ```python
  131: from transformers import AutoModelForCausalLM
  132  model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")

  178  ```python
  179: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  180  

  188  >>> import torch
  189: >>> from transformers import AutoModelForCausalLM
  190  

  227  # pip install transformers accelerate bitsandbytes
  228: from transformers import AutoModelForCausalLM, AutoTokenizer
  229  

  247  # pip install transformers accelerate bitsandbytes
  248: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  249  

  281  import torch
  282: from transformers import BitsAndBytesConfig
  283  

  291  ```python
  292: from transformers import BitsAndBytesConfig
  293  

  306  ```python
  307: from transformers import BitsAndBytesConfig
  308  

  323  ```python
  324: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  325  

  342  ```python
  343: from transformers import AutoModelForCausalLM, AutoTokenizer
  344  

  361  ```python
  362: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  363  

  396  ```python
  397: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  398  

  417  ```python
  418: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  419  

train_real_world/transformers_4573/docs/source/ja/main_classes/trainer.md:
  57  from torch import nn
  58: from transformers import Trainer
  59  

train_real_world/transformers_4573/docs/source/ja/model_doc/align.md:
  39  from PIL import Image
  40: from transformers import AlignProcessor, AlignModel
  41  

train_real_world/transformers_4573/docs/source/ja/model_doc/altclip.md:
  43  
  44: >>> from transformers import AltCLIPModel, AltCLIPProcessor
  45  

train_real_world/transformers_4573/docs/source/ja/model_doc/auto.md:
  35  ```python
  36: from transformers import AutoConfig, AutoModel
  37  

train_real_world/transformers_4573/docs/source/ja/model_doc/bark.md:
  37  ```python
  38: from transformers import BarkModel
  39  import torch

  63  ```python
  64: >>> from transformers import AutoProcessor, BarkModel
  65  

train_real_world/transformers_4573/docs/source/ja/model_doc/bart.md:
  88  ```python
  89: from transformers import BartForConditionalGeneration, BartTokenizer
  90  

train_real_world/transformers_4573/docs/source/ja/model_doc/bartpho.md:
  37  >>> import torch
  38: >>> from transformers import AutoModel, AutoTokenizer
  39  

  51  >>> # With TensorFlow 2.0+:
  52: >>> from transformers import TFAutoModel
  53  

  66  ```python
  67: >>> from transformers import MBartForConditionalGeneration
  68  

train_real_world/transformers_4573/docs/source/ja/model_doc/bert-japanese.md:
  37  >>> import torch
  38: >>> from transformers import AutoModel, AutoTokenizer
  39  

train_real_world/transformers_4573/docs/source/ja/model_doc/bertweet.md:
  34  >>> import torch
  35: >>> from transformers import AutoModel, AutoTokenizer
  36  

  53  >>> # With TensorFlow 2.0+:
  54: >>> # from transformers import TFAutoModel
  55  >>> # bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")

train_real_world/transformers_4573/docs/source/ja/model_doc/blenderbot.md:
  58  ```python
  59: >>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
  60  

train_real_world/transformers_4573/docs/source/ja/model_doc/bridgetower.md:
   53  ```python
   54: >>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
   55  >>> import requests

   76  ```python
   77: >>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
   78  >>> import requests

   99  ```python
  100: >>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
  101  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/ja/model_doc/byt5.md:
   57  ```python
   58: >>> from transformers import T5ForConditionalGeneration
   59  >>> import torch

   79  ```python
   80: >>> from transformers import T5ForConditionalGeneration, AutoTokenizer
   81  

  104  ```python
  105: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  106  >>> import torch

train_real_world/transformers_4573/docs/source/ja/model_doc/canine.md:
  67  ```python
  68: >>> from transformers import CanineModel
  69  >>> import torch

  85  ```python
  86: >>> from transformers import CanineTokenizer, CanineModel
  87  

train_real_world/transformers_4573/docs/source/ja/model_doc/chinese_clip.md:
  36  >>> import requests
  37: >>> from transformers import ChineseCLIPProcessor, ChineseCLIPModel
  38  

train_real_world/transformers_4573/docs/source/ja/model_doc/clip.md:
  65  
  66: >>> from transformers import CLIPProcessor, CLIPModel
  67  

train_real_world/transformers_4573/docs/source/ja/model_doc/clvp.md:
  50  >>> import datasets
  51: >>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration
  52  

train_real_world/transformers_4573/docs/source/ja/model_doc/code_llama.md:
  60  ```python
  61: >>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer
  62  

  92  ```python
  93: >>> from transformers import pipeline
  94  >>> import torch

train_real_world/transformers_4573/docs/source/ja/model_doc/codegen.md:
  46  ```python
  47: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  48  

train_real_world/transformers_4573/docs/source/ja/model_doc/deplot.md:
  36  ```python
  37: from transformers import AutoProcessor, Pix2StructForConditionalGeneration
  38  import requests

  54  ```python
  55: from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup
  56  

train_real_world/transformers_4573/docs/source/ja/model_doc/detr.md:
  125  ```py
  126: >>> from transformers import DetrForObjectDetection
  127  

  133  ```py
  134: >>> from transformers import DetrConfig, DetrForObjectDetection
  135  

train_real_world/transformers_4573/docs/source/ja/tasks/asr.md:
  109  ```py
  110: >>> from transformers import AutoProcessor
  111  

  239  ```py
  240: >>> from transformers import AutoModelForCTC, TrainingArguments, Trainer
  241  

  320  ```py
  321: >>> from transformers import pipeline
  322  

  339  ```py
  340: >>> from transformers import AutoProcessor
  341  

  348  ```py
  349: >>> from transformers import AutoModelForCTC
  350  

train_real_world/transformers_4573/docs/source/ja/tasks/audio_classification.md:
  125  ```py
  126: >>> from transformers import AutoFeatureExtractor
  127  

  197  ```py
  198: >>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer
  199  

  270  ```py
  271: >>> from transformers import pipeline
  272  

  289  ```py
  290: >>> from transformers import AutoFeatureExtractor
  291  

  298  ```py
  299: >>> from transformers import AutoModelForAudioClassification
  300  

train_real_world/transformers_4573/docs/source/ja/tasks/document_question_answering.md:
  162  ```py
  163: >>> from transformers import AutoProcessor
  164  

  377  ```py
  378: >>> from transformers import AutoModelForDocumentQuestionAnswering
  379  

  387  ```py
  388: >>> from transformers import TrainingArguments
  389  

  409  ```py
  410: >>> from transformers import DefaultDataCollator
  411  

  417  ```py
  418: >>> from transformers import Trainer
  419  

  458  ```py
  459: >>> from transformers import pipeline
  460  

  478  >>> import torch
  479: >>> from transformers import AutoProcessor
  480: >>> from transformers import AutoModelForDocumentQuestionAnswering
  481  

train_real_world/transformers_4573/docs/source/ja/tasks/idefics.md:
   76  
   77: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
   78  

   95  >>> import torch
   96: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig
   97  

  392  >>> import torch
  393: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
  394  

train_real_world/transformers_4573/docs/source/ja/tasks/image_captioning.md:
  115  ```python
  116: from transformers import AutoProcessor
  117  

  144  ```python
  145: from transformers import AutoModelForCausalLM
  146  

  150  ```python
  151: from transformers import AutoModelForCausalLM
  152  

  183  ```python
  184: from transformers import TrainingArguments, Trainer
  185  

train_real_world/transformers_4573/docs/source/ja/tasks/image_classification.md:
  106  ```py
  107: >>> from transformers import AutoImageProcessor
  108  

  149  ```py
  150: >>> from transformers import DefaultDataCollator
  151  

  191  ```py
  192: >>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer
  193  

  268  ```py
  269: >>> from transformers import pipeline
  270  

  285  ```py
  286: >>> from transformers import AutoImageProcessor
  287  >>> import torch

  295  ```py
  296: >>> from transformers import AutoModelForImageClassification
  297  

train_real_world/transformers_4573/docs/source/ja/tasks/image_to_image.md:
  37  ```python
  38: from transformers import pipeline
  39  

  75  ```python
  76: from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor 
  77  

train_real_world/transformers_4573/docs/source/ja/tasks/knowledge_distillation_for_image_classification.md:
   42  ```python
   43: from transformers import AutoImageProcessor
   44  teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

   56  ```python
   57: from transformers import TrainingArguments, Trainer
   58  import torch

  106  ```python
  107: from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification
  108  

  156  ```python
  157: from transformers import DefaultDataCollator
  158  

train_real_world/transformers_4573/docs/source/ja/tasks/language_modeling.md:
  105  ```py
  106: >>> from transformers import AutoTokenizer
  107  

  191  ```py
  192: >>> from transformers import DataCollatorForLanguageModeling
  193  

  211  ```py
  212: >>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
  213  

  280  ```py
  281: >>> from transformers import pipeline
  282  

  292  ```py
  293: >>> from transformers import AutoTokenizer
  294  

  302  ```py
  303: >>> from transformers import AutoModelForCausalLM
  304  

train_real_world/transformers_4573/docs/source/ja/tasks/masked_language_modeling.md:
   95  ```py
   96: >>> from transformers import AutoTokenizer
   97  

  179  ```py
  180: >>> from transformers import DataCollatorForLanguageModeling
  181  

  196  ```py
  197: >>> from transformers import AutoModelForMaskedLM
  198  

  267  ```py
  268: >>> from transformers import pipeline
  269  

  289  ```py
  290: >>> from transformers import AutoTokenizer
  291  

  299  ```py
  300: >>> from transformers import AutoModelForMaskedLM
  301  

train_real_world/transformers_4573/docs/source/ja/tasks/monocular_depth_estimation.md:
   51  ```py
   52: >>> from transformers import pipeline
   53  

  101  ```py
  102: >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  103  

train_real_world/transformers_4573/docs/source/ja/tasks/multiple_choice.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  117  ```py
  118: >>> from transformers import DataCollatorForMultipleChoice
  119  >>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)

  156  ```py
  157: >>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer
  158  

  226  ```py
  227: >>> from transformers import AutoTokenizer
  228  

  236  ```py
  237: >>> from transformers import AutoModelForMultipleChoice
  238  

train_real_world/transformers_4573/docs/source/ja/tasks/object_detection.md:
  179  ```py
  180: >>> from transformers import AutoImageProcessor
  181  

  329  ```py
  330: >>> from transformers import AutoModelForObjectDetection
  331  

  346  ```py
  347: >>> from transformers import TrainingArguments
  348  

  366  ```py
  367: >>> from transformers import Trainer
  368  

  549  ```py
  550: >>> from transformers import pipeline
  551  >>> import requests

train_real_world/transformers_4573/docs/source/ja/tasks/prompting.md:
   73  ```python
   74: >>> from transformers import pipeline
   75  >>> import torch

  119  ```python
  120: >>> from transformers import pipeline, AutoTokenizer
  121  >>> import torch

train_real_world/transformers_4573/docs/source/ja/tasks/question_answering.md:
    4  ```py
    5: >>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
    6  

   74  ```py
   75: >>> from transformers import pipeline
   76  

   90  ```py
   91: >>> from transformers import AutoTokenizer
   92  

  101  >>> import torch
  102: >>> from transformers import AutoModelForQuestionAnswering
  103  

train_real_world/transformers_4573/docs/source/ja/tasks/semantic_segmentation.md:
  101  ```py
  102: >>> from transformers import AutoImageProcessor
  103  

  199  ```py
  200: >>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer
  201  

  264  ```py
  265: >>> from transformers import pipeline
  266  

train_real_world/transformers_4573/docs/source/ja/tasks/summarization.md:
   87  ```py
   88: >>> from transformers import AutoTokenizer
   89  

  123  ```py
  124: >>> from transformers import DataCollatorForSeq2Seq
  125  

  173  ```py
  174: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  175  

  239  ```py
  240: >>> from transformers import pipeline
  241  

  251  ```py
  252: >>> from transformers import AutoTokenizer
  253  

  260  ```py
  261: >>> from transformers import AutoModelForSeq2SeqLM
  262  

train_real_world/transformers_4573/docs/source/ja/tasks/text-to-speech.md:
   29  ```py
   30: >>> from transformers import pipeline
   31  

  121  ```py
  122: >>> from transformers import SpeechT5Processor
  123  

  453  ```py
  454: >>> from transformers import SpeechT5ForTextToSpeech
  455  

  468  ```python
  469: >>> from transformers import Seq2SeqTrainingArguments
  470  

  495  ```py
  496: >>> from transformers import Seq2SeqTrainer
  497  

  534  ```py
  535: >>> from transformers import pipeline
  536  

train_real_world/transformers_4573/docs/source/ja/tasks/token_classification.md:
  103  ```py
  104: >>> from transformers import AutoTokenizer
  105  

  159  ```py
  160: >>> from transformers import DataCollatorForTokenClassification
  161  

  251  ```py
  252: >>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
  253  

  319  ```py
  320: >>> from transformers import pipeline
  321  

  360  ```py
  361: >>> from transformers import AutoTokenizer
  362  

  369  ```py
  370: >>> from transformers import AutoModelForTokenClassification
  371  

train_real_world/transformers_4573/docs/source/ja/tasks/translation.md:
   83  ```py
   84: >>> from transformers import AutoTokenizer
   85  

  117  ```py
  118: >>> from transformers import DataCollatorForSeq2Seq
  119  

  176  ```py
  177: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  178  

  244  ```py
  245: >>> from transformers import pipeline
  246  

  260  ```py
  261: >>> from transformers import AutoTokenizer
  262  

  270  ```py
  271: >>> from transformers import AutoModelForSeq2SeqLM
  272  

train_real_world/transformers_4573/docs/source/ja/tasks/video_classification.md:
  146  ```py
  147: >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification
  148  

  348  ```py
  349: >>> from transformers import TrainingArguments, Trainer
  350  

  449  ```py
  450: >>> from transformers import pipeline
  451  

train_real_world/transformers_4573/docs/source/ja/tasks/visual_question_answering.md:
  178  ```py
  179: >>> from transformers import ViltProcessor
  180  

  238  ```py
  239: >>> from transformers import DefaultDataCollator
  240  

  249  ```py
  250: >>> from transformers import ViltForQuestionAnswering
  251  

  259  ```py
  260: >>> from transformers import TrainingArguments
  261  

  279  ```py
  280: >>> from transformers import Trainer
  281  

  308  ```py
  309: >>> from transformers import pipeline
  310  

  366  ```py
  367: >>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
  368  >>> import torch

train_real_world/transformers_4573/docs/source/ja/tasks/zero_shot_image_classification.md:
  50  ```python
  51: >>> from transformers import pipeline
  52  

  94  ```py
  95: >>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification
  96  

train_real_world/transformers_4573/docs/source/ja/tasks/zero_shot_object_detection.md:
   52  ```python
   53: >>> from transformers import pipeline
   54  

  138  ```py
  139: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  140  

train_real_world/transformers_4573/docs/source/ko/accelerate.md:
  67  + from accelerate import Accelerator
  68:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  69  

train_real_world/transformers_4573/docs/source/ko/add_new_model.md:
  337  ```python
  338: from transformers import BrandNewBertModel, BrandNewBertConfig
  339  

  558  ```python
  559: from transformers import BrandNewBertTokenizer
  560  

train_real_world/transformers_4573/docs/source/ko/add_new_pipeline.md:
   32  ```python
   33: from transformers import Pipeline
   34  

  118  ```python
  119: from transformers.pipelines import PIPELINE_REGISTRY
  120  

  147  
  148: from transformers import Pipeline
  149  

  185  from pair_classification import PairClassificationPipeline
  186: from transformers.pipelines import PIPELINE_REGISTRY
  187: from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
  188  

  200  ```py
  201: from transformers import pipeline
  202  

  215  ```py
  216: from transformers import pipeline
  217  

train_real_world/transformers_4573/docs/source/ko/cache_explanation.md:
  101  import torch
  102: from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device
  103  

  147  import torch
  148: from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device
  149  

train_real_world/transformers_4573/docs/source/ko/chat_extras.md:
   66  import torch
   67: from transformers import AutoModelForCausalLM, AutoTokenizer
   68  

  153  ```py
  154: from transformers.utils import get_json_schema
  155  

  267  ```py
  268: from transformers import AutoTokenizer, AutoModelForCausalLM
  269  

train_real_world/transformers_4573/docs/source/ko/chat_templating.md:
   27  ```python
   28: >>> from transformers import AutoTokenizer
   29  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

   43  ```python
   44: >>> from transformers import AutoTokenizer
   45  >>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

   65  ```python
   66: from transformers import AutoModelForCausalLM, AutoTokenizer
   67  

  116  ```python
  117: from transformers import pipeline
  118  

  186  ```python
  187: from transformers import AutoTokenizer
  188  from datasets import Dataset

  280  import torch
  281: from transformers import AutoModelForCausalLM, AutoTokenizer
  282  

  388  ```python
  389: from transformers.utils import get_json_schema
  390  

  507  
  508: >>> from transformers import AutoTokenizer
  509  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

train_real_world/transformers_4573/docs/source/ko/conversations.md:
   65  import torch
   66: from transformers import pipeline
   67  

  180  ```python
  181: from transformers import AutoModelForCausalLM, AutoTokenizer
  182  import torch

  246  ```python
  247: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  248  

  255  ```python
  256: from transformers import pipeline, BitsAndBytesConfig
  257  

train_real_world/transformers_4573/docs/source/ko/custom_models.md:
   38  ```python
   39: from transformers import PreTrainedConfig
   40  from typing import List

  114  ```py
  115: from transformers import PreTrainedModel
  116  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  312  ```py
  313: from transformers import AutoModelForImageClassification
  314  

  337  ```py
  338: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  339  

train_real_world/transformers_4573/docs/source/ko/debugging.md:
   80  ```python
   81: from transformers.debug_utils import DebugUnderflowOverflow
   82  

  249  ```python
  250: from transformers.debug_utils import DebugUnderflowOverflow
  251  

train_real_world/transformers_4573/docs/source/ko/deepspeed.md:
    60  ```bash
    61: $ python -c 'from transformers import AutoModel; \
    62  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

   264  ```py
   265: from transformers import T5ForConditionalGeneration, T5Config
   266  import deepspeed

   275  ```py
   276: from transformers import AutoModel, Trainer, TrainingArguments
   277  

   833  ```py
   834: from transformers.trainer_utils import get_last_checkpoint
   835  from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

   931  ```py
   932: from transformers.integrations import HfDeepSpeedConfig
   933: from transformers import AutoModel
   934  import deepspeed

   948  ```py
   949: from transformers.integrations import HfDeepSpeedConfig
   950: from transformers import AutoModel, AutoConfig
   951  import deepspeed

  1007  
  1008: from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
  1009: from transformers.integrations import HfDeepSpeedConfig
  1010  import deepspeed

train_real_world/transformers_4573/docs/source/ko/fast_tokenizers.md:
  45  ```python
  46: >>> from transformers import PreTrainedTokenizerFast
  47  

  65  ```python
  66: >>> from transformers import PreTrainedTokenizerFast
  67  

train_real_world/transformers_4573/docs/source/ko/generation_strategies.md:
   43  ```python
   44: >>> from transformers import AutoModelForCausalLM
   45  

   81  ```python
   82: >>> from transformers import AutoModelForCausalLM, GenerationConfig
   83  

   93  ```python
   94: >>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig
   95  

  130  ```python
  131: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
  132  

  153  ```python
  154: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  155  

  174  ```python
  175: >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
  176  >>> set_seed(0)  # ìž¬í˜„ì„±ì„ ìœ„í•´

  197  ```python
  198: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  199  

  218  ```python
  219: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
  220  >>> set_seed(0)  # ìž¬í˜„ì„±ì„ ìœ„í•´

  243  ```python
  244: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  245  

  262  ```python
  263: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
  264  >>> set_seed(42)  # ìž¬í˜„ì„±ì„ ìœ„í•´

train_real_world/transformers_4573/docs/source/ko/gguf.md:
  79  ```python
  80: from transformers import AutoTokenizer, AutoModelForCausalLM
  81  

train_real_world/transformers_4573/docs/source/ko/glossary.md:
   33  ```python
   34: >>> from transformers import BertTokenizer
   35  

  192  ```python
  193: >>> from transformers import BertTokenizer
  194  

  404  ```python
  405: >>> from transformers import BertTokenizer
  406  

train_real_world/transformers_4573/docs/source/ko/how_to_hack_models.md:
   25  > ```py
   26: > from transformers import AutoModel
   27: > from transformers.utils.import_utils import clear_import_cache
   28  >

   45  import torch.nn as nn
   46: from transformers.models.sam.modeling_sam import SamVisionAttention
   47  

  110  ```py
  111: from transformers import SamModel
  112  

train_real_world/transformers_4573/docs/source/ko/image_processors.md:
   26  ```py
   27: from transformers import AutoImageProcessor
   28  

   63  ```py
   64: from transformers import AutoImageProcessor
   65  

   76  ```py
   77: from transformers import ViTImageProcessor
   78  

   84  ```py
   85: from transformers import ViTImageProcessorFast
   86  

   98  ```py
   99: from transformers import AutoImageProcessor
  100  

  107  from torchvision.io import read_image
  108: from transformers import DetrImageProcessorFast
  109  

train_real_world/transformers_4573/docs/source/ko/index.md:
  128  1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.
  129: 1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.
  130  1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.

train_real_world/transformers_4573/docs/source/ko/installation.md:
   78  ```bash
   79: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   80  ```

  100  ```bash
  101: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  102  ```

  190      ```py
  191:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  192  

  229  ```py
  230: >>> from transformers import AutoConfig
  231  

train_real_world/transformers_4573/docs/source/ko/llm_optims.md:
   53  ```py
   54: from transformers import AutoTokenizer, AutoModelForCausalLM
   55  import torch

   85  ```py
   86: from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache
   87  import torch

  122  ```py
  123: from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging
  124: from transformers.testing_utils import CaptureLogger
  125  import torch

  194  ```py
  195: from transformers import AutoTokenizer, AutoModelForCausalLM
  196  import torch

  238  ```py
  239: from transformers import AutoModelForCausalLM, AutoTokenizer
  240  import torch

  259  ```py
  260: from transformers import AutoModelForCausalLM, AutoTokenizer
  261  import torch

  287  ```py
  288: from transformers import AutoModelForCausalLM, AutoTokenizer
  289  import torch

  308  ```py
  309: from transformers import AutoModelForCausalLM, AutoTokenizer
  310  import torch

  336  ```py
  337: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  338  

  358  import torch
  359: from transformers import AutoModelForCausalLM
  360  

  388  ```py
  389: from transformers import AutoTokenizer, AutoModelForCausalLM
  390  import torch

  399  ```py
  400: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  401  import torch

train_real_world/transformers_4573/docs/source/ko/llm_tutorial_optimization.md:
  66  ```python
  67: from transformers import AutoModelForCausalLM
  68  

  80  ```python
  81: from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
  82  import torch

train_real_world/transformers_4573/docs/source/ko/llm_tutorial.md:
   79  ```python
   80: >>> from transformers import AutoModelForCausalLM
   81  

   96  ```python
   97: >>> from transformers import AutoTokenizer
   98  >>> import torch

  122  ```py
  123: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  124  

  156  >>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
  157: >>> from transformers import set_seed
  158  >>> set_seed(0)

train_real_world/transformers_4573/docs/source/ko/model_memory_anatomy.md:
   91  ```py
   92: >>> from transformers import AutoModelForSequenceClassification
   93  

  152  ```py
  153: >>> from transformers import TrainingArguments, Trainer, logging
  154  

train_real_world/transformers_4573/docs/source/ko/model_sharing.md:
  129  ```py
  130: >>> from transformers import AutoModel
  131  

train_real_world/transformers_4573/docs/source/ko/models.md:
   26  ```py
   27: from transformers import AutoModelForCausalLM
   28  

   53  ```py
   54: from transformers import AutoModelForCausalLM, MistralForCausalLM
   55  

   64  ```py
   65: from transformers import TFAutoModelForCausalLM, TFMistralForCausalLM
   66  

   75  ```py
   76: from transformers import FlaxAutoModelForCausalLM, FlaxMistralForCausalLM
   77  

  103  ```py
  104: from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering
  105  

  114  ```py
  115: from transformers import AutoModelForCausalLM
  116  

  130  ```py
  131: from transformers import LlamaModel, LlamaForCausalLM
  132  

  162  ```py
  163: from transformers import AutoModel
  164  import tempfile

  183  ```py
  184: from transformers.trainer_utils import load_sharded_checkpoint
  185  

  245  ```py
  246: from transformers import AutoModelForCausalLM
  247  

  270  import torch
  271: from transformers import AutoModelForCausalLM
  272  

  279  ```py
  280: from transformers import AutoModelForCausalLM
  281  

  291  import torch
  292: from transformers import AutoConfig, AutoModel
  293  

  306  ```py
  307: from transformers import AutoModelForImageClassification
  308  

train_real_world/transformers_4573/docs/source/ko/optimizers.md:
   24  import torch
   25: from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer
   26  

   53  import torch
   54: from transformers import TrainingArguments
   55  

   85  ```py
   86: from transformers import TrainingArguments
   87  

  107  import torch
  108: from transformers import TrainingArguments
  109  

train_real_world/transformers_4573/docs/source/ko/peft.md:
   59  ```py
   60: from transformers import AutoModelForCausalLM, AutoTokenizer
   61  

   74  ```py
   75: from transformers import AutoModelForCausalLM, AutoTokenizer
   76  

   88  ```py
   89: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   90  

   99  ```py
  100: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  101  from peft import PeftConfig

  139  ```py
  140: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  141  from peft import PeftConfig

train_real_world/transformers_4573/docs/source/ko/perf_infer_gpu_multi.md:
   58  import torch
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   89  ```py
   90: from transformers import AutoModelForCausalLM
   91  

  242      ```python
  243:     from transformers.integrations.tensor_parallel import ParallelInterface
  244  

train_real_world/transformers_4573/docs/source/ko/perf_infer_gpu_one.md:
   45  ```py
   46: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   47  

  108  ```py
  109: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  110  

  122  ```py
  123: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  124  

train_real_world/transformers_4573/docs/source/ko/perf_train_gpu_one.md:
   46  ```py
   47: from transformers import TrainingArguments
   48  

   67  ```py
   68: from transformers import TrainingArguments
   69  

   90  ```py
   91: from transformers import TrainingArguments
   92  

  113  ```py
  114: from transformers import TrainingArguments
  115  

  132  ```py
  133: from transformers import TrainingArguments
  134  

  158  ```py
  159: from transformers import TrainingArguments
  160  

  179  ```py
  180: from transformers import TrainingArguments
  181  

  203  ```py
  204: from transformers import TrainingArguments
  205  

  227  ```py
  228: from transformers import TrainingArguments
  229  

  248  ```py
  249: from transformers import TrainingArguments
  250  

  290  ```py
  291: from transformers import AutoModelForCausalLM
  292  

train_real_world/transformers_4573/docs/source/ko/perplexity.md:
  71  ```python
  72: from transformers import GPT2LMHeadModel, GPT2TokenizerFast
  73  

train_real_world/transformers_4573/docs/source/ko/pipeline_gradio.md:
  31  ```py
  32: from transformers import pipeline
  33  import gradio as gr

train_real_world/transformers_4573/docs/source/ko/pipeline_tutorial.md:
   37  ```py
   38: >>> from transformers import pipeline
   39  

  172  # KeyDataset is a util that will just output the item we're interested in.
  173: from transformers.pipelines.pt_utils import KeyDataset
  174  

  199  ```py
  200: >>> from transformers import pipeline
  201  

  215  ```py
  216: >>> from transformers import pipeline
  217  

  234  ```py
  235: >>> from transformers import pipeline
  236  

train_real_world/transformers_4573/docs/source/ko/pipeline_webserver.md:
  23  from starlette.routing import Route
  24: from transformers import pipeline
  25  import asyncio

train_real_world/transformers_4573/docs/source/ko/pr_checks.md:
  159  ```py
  160: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  161  ```

  165  ```py
  166: # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights
  167  ```

  171  ```py
  172: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta
  173  ```

  179  ```py
  180: # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT
  181  ```

  193  ```py
  194: # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing
  195  ```

train_real_world/transformers_4573/docs/source/ko/quicktour.md:
   68  ```py
   69: >>> from transformers import pipeline
   70  

   94  >>> import torch
   95: >>> from transformers import pipeline
   96  

  134  ```py
  135: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  136  

  165  ```py
  166: >>> from transformers import AutoTokenizer
  167  

  210  ```py
  211: >>> from transformers import AutoModelForSequenceClassification
  212  

  265  ```py
  266: >>> from transformers import AutoModel
  267  

  278  ```py
  279: >>> from transformers import AutoConfig
  280  

  286  ```py
  287: >>> from transformers import AutoModel
  288  

  302     ```py
  303:    >>> from transformers import AutoModelForSequenceClassification
  304  

  310     ```py
  311:    >>> from transformers import TrainingArguments
  312  

  324     ```py
  325:    >>> from transformers import AutoTokenizer
  326  

  353     ```py
  354:    >>> from transformers import DataCollatorWithPadding
  355  

  361  ```py
  362: >>> from transformers import Trainer
  363  

  396     ```py
  397:    >>> from transformers import TFAutoModelForSequenceClassification
  398  

  404     ```py
  405:    >>> from transformers import AutoTokenizer
  406  

train_real_world/transformers_4573/docs/source/ko/serialization.md:
   87  ```python
   88: >>> from transformers import AutoTokenizer
   89  >>> from optimum.onnxruntime import ORTModelForQuestionAnswering

  108  >>> from optimum.onnxruntime import ORTModelForSequenceClassification
  109: >>> from transformers import AutoTokenizer
  110  

  113  
  114: >>> # Load a model from transformers and export it to ONNX
  115  >>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)

train_real_world/transformers_4573/docs/source/ko/testing.md:
   203      >>> import torch
   204:     >>> from transformers import WhisperModel, WhisperFeatureExtractor
   205      >>> from datasets import load_dataset

   501  ```python
   502: from transformers.testing_utils import get_gpu_count
   503  

   716  ```python
   717: from transformers.testing_utils import TestCasePlus
   718  

   728  ```python
   729: from transformers.testing_utils import TestCasePlus
   730  

   751  ```python
   752: from transformers.testing_utils import TestCasePlus
   753  

   812  import os
   813: from transformers.testing_utils import ExtendSysPath
   814  

   920  ```python no-style
   921: from transformers.testing_utils import slow
   922  @slow

  1064  ```python
  1065: from transformers.testing_utils import CaptureStdout
  1066  

  1074  ```python
  1075: from transformers.testing_utils import CaptureStdout
  1076  

  1086  ```python
  1087: from transformers.testing_utils import CaptureStderr
  1088  

  1096  ```python
  1097: from transformers.testing_utils import CaptureStd
  1098  

  1112  ```python
  1113: from transformers import logging
  1114: from transformers.testing_utils import CaptureLogger
  1115  

  1129  ```python
  1130: from transformers.testing_utils import mockenv
  1131  

  1142  ```python
  1143: from transformers.testing_utils import TestCasePlus
  1144  

train_real_world/transformers_4573/docs/source/ko/tokenizer_summary.md:
   97  ```py
   98: >>> from transformers import BertTokenizer
   99  

  111  ```py
  112: >>> from transformers import XLNetTokenizer
  113  

train_real_world/transformers_4573/docs/source/ko/trainer.md:
   55  ```py
   56: from transformers import TrainingArguments
   57  

   76  ```py
   77: from transformers import Trainer
   78  

  129  from torch import nn
  130: from transformers import Trainer
  131  

  152  ```py
  153: from transformers import TrainerCallback
  154  

  168  ```py
  169: from transformers import Trainer
  170  

  248  ```py
  249: from transformers import TrainingArguments, Trainer
  250  

train_real_world/transformers_4573/docs/source/ko/training.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   82  ```py
   83: >>> from transformers import AutoModelForSequenceClassification
   84  

  103  ```py
  104: >>> from transformers import TrainingArguments
  105  

  132  ```py
  133: >>> from transformers import TrainingArguments, Trainer
  134  

  214  ```py
  215: >>> from transformers import AutoModelForSequenceClassification
  216  

  232  ```py
  233: >>> from transformers import get_scheduler
  234  

train_real_world/transformers_4573/docs/source/ko/troubleshooting.md:
   74  ```py
   75: >>> from transformers import TFPreTrainedModel
   76  >>> from tensorflow import keras

   84  ```py
   85: >>> from transformers import TFPreTrainedModel
   86  

  133  ```py
  134: >>> from transformers import AutoModelForSequenceClassification
  135  >>> import torch

  191  ```py
  192: >>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
  193  

train_real_world/transformers_4573/docs/source/ko/internal/generation_utils.md:
  27  ```python
  28: from transformers import GPT2Tokenizer, GPT2LMHeadModel
  29  

train_real_world/transformers_4573/docs/source/ko/main_classes/logging.md:
  45  ```python
  46: from transformers.utils import logging
  47  

train_real_world/transformers_4573/docs/source/ko/main_classes/output.md:
  24  ```python
  25: from transformers import BertTokenizer, BertForSequenceClassification
  26  import torch

train_real_world/transformers_4573/docs/source/ko/main_classes/pipelines.md:
   60  import datasets
   61: from transformers import pipeline
   62: from transformers.pipelines.pt_utils import KeyDataset
   63  from tqdm.auto import tqdm

   79  ```python
   80: from transformers import pipeline
   81  

  106  ```python
  107: from transformers import pipeline
  108: from transformers.pipelines.pt_utils import KeyDataset
  109  import datasets

  127  ```python
  128: from transformers import pipeline
  129  from torch.utils.data import Dataset

train_real_world/transformers_4573/docs/source/ko/model_doc/albert.md:
  48  import torch
  49: from transformers import pipeline
  50  

  64  import torch
  65: from transformers import AutoModelForMaskedLM, AutoTokenizer
  66  

train_real_world/transformers_4573/docs/source/ko/model_doc/altclip.md:
  25  
  26: >>> from transformers import AltCLIPModel, AltCLIPProcessor
  27  

train_real_world/transformers_4573/docs/source/ko/model_doc/auto.md:
  36  ```python
  37: from transformers import AutoConfig, AutoModel
  38  

train_real_world/transformers_4573/docs/source/ko/model_doc/bart.md:
  66  ```python
  67: from transformers import BartForConditionalGeneration, BartTokenizer
  68  

train_real_world/transformers_4573/docs/source/ko/model_doc/bartpho.md:
  37  >>> import torch
  38: >>> from transformers import AutoModel, AutoTokenizer
  39  

  51  >>> # With TensorFlow 2.0+:
  52: >>> from transformers import TFAutoModel
  53  

  66  ```python
  67: >>> from transformers import MBartForConditionalGeneration
  68  

train_real_world/transformers_4573/docs/source/ko/model_doc/bert-japanese.md:
  35  >>> import torch
  36: >>> from transformers import AutoModel, AutoTokenizer
  37  

train_real_world/transformers_4573/docs/source/ko/model_doc/bert.md:
  33  
  34: *ìš°ë¦¬ëŠ” BERT(Bidirectional Encoder Representations from Transformers)ë¼ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìµœê·¼ì˜ ë‹¤ë¥¸ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  ê³„ì¸µì—ì„œ ì–‘ë°©í–¥ìœ¼ë¡œ ì–‘ìª½ ë¬¸ë§¥ì„ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¹„ì§€ë„ í•™ìŠµëœ í…ìŠ¤íŠ¸ì—ì„œ ê¹Šì´ ìžˆëŠ” ì–‘ë°©í–¥ í‘œí˜„ì„ ì‚¬ì „ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì€ ì¶”ê°€ì ì¸ ì¶œë ¥ ê³„ì¸µ í•˜ë‚˜ë§Œìœ¼ë¡œ ì§ˆë¬¸ ì‘ë‹µ, ì–¸ì–´ ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ìž‘ì—…ì—ì„œ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, íŠ¹ì • ìž‘ì—…ì„ ìœ„í•´ ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.*
  35  

  58  ```
  59: from transformers import BertModel
  60  

train_real_world/transformers_4573/docs/source/ko/model_doc/bertweet.md:
  35  >>> import torch
  36: >>> from transformers import AutoModel, AutoTokenizer
  37  

  54  >>> # With TensorFlow 2.0+:
  55: >>> # from transformers import TFAutoModel
  56  >>> # bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")

train_real_world/transformers_4573/docs/source/ko/model_doc/big_bird.md:
  39  import torch
  40: from transformers import pipeline
  41  

  55  import torch
  56: from transformers import AutoModelForMaskedLM, AutoTokenizer
  57  

train_real_world/transformers_4573/docs/source/ko/model_doc/biogpt.md:
  41  ```
  42: from transformers import BioGptForCausalLM
  43  model = BioGptForCausalLM.from_pretrained("microsoft/biogpt", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/ko/model_doc/chameleon.md:
   51  ```python
   52: from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
   53  import torch

   76  ```python
   77: from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
   78  import torch

  127  ```python
  128: from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig
  129  

  144  ```python
  145: from transformers import ChameleonForConditionalGeneration
  146  

train_real_world/transformers_4573/docs/source/ko/model_doc/clip.md:
   45  
   46: >>> from transformers import CLIPProcessor, CLIPModel
   47  

   84  
   85: >>> from transformers import CLIPProcessor, CLIPModel
   86  

  121  ```python
  122: from transformers import CLIPModel
  123  

train_real_world/transformers_4573/docs/source/ko/model_doc/code_llama.md:
   39  import torch
   40: from transformers import pipeline
   41  

   62  import torch
   63: from transformers import AutoModelForCausalLM, AutoTokenizer
   64  

  109  import torch
  110: from transformers import AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig
  111  

  130  ```py
  131: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  132  

  146      ```py
  147:     from transformers import LlamaForCausalLM, CodeLlamaTokenizer
  148  

train_real_world/transformers_4573/docs/source/ko/model_doc/codegen.md:
  49  ```python
  50: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  51  

train_real_world/transformers_4573/docs/source/ko/model_doc/cohere.md:
  35  # pip install transformers
  36: from transformers import AutoTokenizer, AutoModelForCausalLM
  37  

  69  # pip install transformers
  70: from transformers import AutoTokenizer, AutoModelForCausalLM
  71  

  94  # pip install transformers bitsandbytes accelerate
  95: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
  96  

train_real_world/transformers_4573/docs/source/ko/model_doc/dbrx.md:
  42  ```python
  43: from transformers import DbrxForCausalLM, AutoTokenizer
  44  import torch

  65  ```python
  66: from transformers import DbrxForCausalLM, AutoTokenizer
  67  import torch

  89  ```python
  90: from transformers import DbrxForCausalLM, AutoTokenizer
  91  import torch

train_real_world/transformers_4573/docs/source/ko/model_doc/deepseek_v3.md:
  41  # `run_deepseek_v1.py`
  42: from transformers import AutoModelForCausalLM, AutoTokenizer
  43  import torch

train_real_world/transformers_4573/docs/source/ko/model_doc/encoder-decoder.md:
   33  ```python
   34: >>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel
   35  

   48  ```python
   49: >>> from transformers import EncoderDecoderModel, BertTokenizer
   50  

   61  ```python
   62: >>> from transformers import AutoTokenizer, EncoderDecoderModel
   63  

   88  >>> # íŒŒì´í† ì¹˜ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•˜ëŠ” í•´ê²° ë°©ë²•
   89: >>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel
   90  

  108  ```python
  109: >>> from transformers import BertTokenizer, EncoderDecoderModel
  110  

train_real_world/transformers_4573/docs/source/ko/model_doc/exaone4.md:
  63  ```python
  64: from transformers import AutoModelForCausalLM, AutoTokenizer
  65  

train_real_world/transformers_4573/docs/source/ko/model_doc/gemma3.md:
   42  import torch
   43: from transformers import pipeline
   44  

   61  import torch
   62: from transformers import AutoProcessor, Gemma3ForConditionalGeneration
   63  

  117  import torch
  118: from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor
  119  

  160  ```py
  161: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  162  

  214      import torch
  215:     from transformers import AutoModelForCausalLM, AutoTokenizer
  216  

train_real_world/transformers_4573/docs/source/ko/model_doc/gemma3n.md:
  46  import torch
  47: from transformers import pipeline
  48  

  65  import torch
  66: from transformers import AutoProcessor, Gemma3nForConditionalGeneration
  67  

train_real_world/transformers_4573/docs/source/ko/model_doc/gpt_neox_japanese.md:
  34  ```python
  35: >>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer
  36  

train_real_world/transformers_4573/docs/source/ko/model_doc/gpt2.md:
  44  import torch
  45: from transformers import pipeline
  46  

  55  import torch
  56: from transformers import AutoModelForCausalLM, AutoTokenizer
  57  

  91  import torch
  92: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
  93  

train_real_world/transformers_4573/docs/source/ko/model_doc/grounding-dino.md:
  51  >>> from PIL import Image
  52: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  53  

train_real_world/transformers_4573/docs/source/ko/model_doc/jamba.md:
   44  import torch
   45: from transformers import pipeline
   46  

   60  import torch
   61: from transformers import AutoModelForCausalLM, AutoTokenizer
   62  

   92  import torch
   93: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
   94  

  132    import torch
  133:   from transformers import AutoModelForCausalLM
  134    model = AutoModelForCausalLM.from_pretrained("ai21labs/AI21-Jamba-1.5-Large",

train_real_world/transformers_4573/docs/source/ko/model_doc/lfm2.md:
  40  ```python
  41: from transformers import AutoModelForCausalLM, AutoTokenizer
  42  

train_real_world/transformers_4573/docs/source/ko/model_doc/llama.md:
  39  ```python
  40: from transformers import LlamaForCausalLM, LlamaTokenizer
  41  

train_real_world/transformers_4573/docs/source/ko/model_doc/llama2.md:
  54  ```python
  55: from transformers import LlamaForCausalLM, LlamaTokenizer
  56  

train_real_world/transformers_4573/docs/source/ko/model_doc/llama3.md:
  67      ```python
  68:     from transformers import AutoModelForCausalLM, AutoTokenizer
  69      

train_real_world/transformers_4573/docs/source/ko/model_doc/llama4.md:
   57  ```py
   58: from transformers import pipeline
   59  import torch

   81  ```py
   82: from transformers import AutoTokenizer, Llama4ForConditionalGeneration
   83  import torch

  108  ```py
  109: from transformers import AutoProcessor, Llama4ForConditionalGeneration
  110  import torch

  152  ```py
  153: from transformers import AutoProcessor, Llama4ForConditionalGeneration
  154  import torch

  204  ```py
  205: from transformers import Llama4ForConditionalGeneration, AutoTokenizer, infer_device
  206  import torch

  267  ```py
  268: from transformers import Llama4ForConditionalGeneration
  269  import torch

  282  ```py
  283: from transformers import Llama4ForConditionalGeneration
  284  import torch

  297  ```py
  298: from transformers import Llama4ForConditionalGeneration
  299  import torch

  325  ```python
  326: from transformers import AutoTokenizer, Llama4ForConditionalGeneration, FbgemmFp8Config
  327  import torch

  355  ```python
  356: from transformers import AutoTokenizer, Llama4ForConditionalGeneration
  357  import torch

  389  ```py
  390: from transformers import Llama4ForConditionalGeneration
  391  import torch

train_real_world/transformers_4573/docs/source/ko/model_doc/mamba.md:
  44  ```python 
  45: from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer
  46  import torch

train_real_world/transformers_4573/docs/source/ko/model_doc/mamba2.md:
  46  ```python 
  47: from transformers import Mamba2Config, Mamba2ForCausalLM, AutoTokenizer
  48  import torch

train_real_world/transformers_4573/docs/source/ko/model_doc/marian.md:
   72  ```python
   73: >>> from transformers import MarianMTModel, MarianTokenizer
   74  

  137  ```python
  138: >>> from transformers import MarianMTModel, MarianTokenizer
  139  

train_real_world/transformers_4573/docs/source/ko/model_doc/mistral.md:
   53  ```python
   54: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   55  

   71  ```python
   72: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   73  

  107  >>> import torch
  108: >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  109  

  144  >>> import torch
  145: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  146  

train_real_world/transformers_4573/docs/source/ko/model_doc/paligemma.md:
  36  ```python
  37: from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
  38  

train_real_world/transformers_4573/docs/source/ko/model_doc/patchtsmixer.md:
  41  
  42: from transformers import PatchTSMixerConfig, PatchTSMixerForPrediction
  43: from transformers import Trainer, TrainingArguments,
  44  

train_real_world/transformers_4573/docs/source/ko/model_doc/qwen2_vl.md:
   47  import torch
   48: from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
   49  

  267  ```python
  268: from transformers import Qwen2VLForConditionalGeneration
  269  

train_real_world/transformers_4573/docs/source/ko/model_doc/sam_hq.md:
  58  import requests
  59: from transformers import infer_device, SamHQModel, SamHQProcessor
  60  

  84  import requests
  85: from transformers import infer_device, SamHQModel, SamHQProcessor
  86  

train_real_world/transformers_4573/docs/source/ko/model_doc/siglip.md:
   56  ```python
   57: >>> from transformers import pipeline
   58  >>> from PIL import Image

   82  >>> import requests
   83: >>> from transformers import AutoProcessor, AutoModel
   84  >>> import torch

  132  >>> from PIL import Image
  133: >>> from transformers import SiglipProcessor, SiglipModel
  134  >>> device = "cuda" # ëª¨ë¸ì„ ë¡œë“œí•  ìž¥ì¹˜

  174  ```python
  175: >>> from transformers import SiglipModel
  176  

train_real_world/transformers_4573/docs/source/ko/model_doc/smolvlm.md:
   59  import torch
   60: from transformers import AutoProcessor, AutoModelForImageTextToText
   61  

  121  import torch
  122: from transformers import AutoProcessor, AutoModelForImageTextToText
  123  

train_real_world/transformers_4573/docs/source/ko/model_doc/tvp.md:
  55  from huggingface_hub import hf_hub_download
  56: from transformers import AutoProcessor, TvpForVideoGrounding
  57  

train_real_world/transformers_4573/docs/source/ko/model_doc/vit.md:
  64  ```
  65: from transformers import ViTForImageClassification
  66  model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224", attn_implementation="sdpa", dtype=torch.float16)

train_real_world/transformers_4573/docs/source/ko/quantization/awq.md:
   58  ```py
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   67  ```py
   68: from transformers import AutoModelForCausalLM, AutoTokenizer
   69  

   76  ```py
   77: from transformers import AutoModelForCausalLM, AutoTokenizer
   78  

  101  import torch
  102: from transformers import AwqConfig, AutoModelForCausalLM
  103  

  160  import torch
  161: from transformers import AwqConfig, AutoModelForCausalLM
  162  

  208  import torch
  209: from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig
  210  

train_real_world/transformers_4573/docs/source/ko/quantization/bitsandbytes.md:
   48  ```py
   49: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   50  

   62  import torch
   63: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   64  

   77  ```py
   78: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   79  

   96  ```py
   97: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   98  

  110  import torch
  111: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  112  

  142  ```py
  143: from transformers import AutoModelForCausalLM, AutoTokenizer
  144  

  162  ```py
  163: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  164  

  196  ```py
  197: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  198  

  216  ```py
  217: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  218  

  252  import torch
  253: from transformers import BitsAndBytesConfig
  254  

  262  ```py
  263: from transformers import BitsAndBytesConfig
  264  

  279  ```py
  280: from transformers import BitsAndBytesConfig
  281  

  293  ```python
  294: from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
  295  

train_real_world/transformers_4573/docs/source/ko/quantization/eetq.md:
  34  ```py
  35: from transformers import AutoModelForCausalLM, EetqConfig
  36  path = "/path/to/model".

train_real_world/transformers_4573/docs/source/ko/quantization/gptq.md:
  36  ```py
  37: from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig
  38  

  89  ```py
  90: from transformers import AutoModelForCausalLM
  91  

train_real_world/transformers_4573/docs/source/ko/quantization/quanto.md:
  47  ```py
  48: from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig
  49  

train_real_world/transformers_4573/docs/source/ko/quantization/quark.md:
  61  ```python
  62: from transformers import AutoModelForCausalLM, AutoTokenizer
  63  

train_real_world/transformers_4573/docs/source/ko/tasks/asr.md:
  111  ```py
  112: >>> from transformers import AutoProcessor
  113  

  243  ```py
  244: >>> from transformers import AutoModelForCTC, TrainingArguments, Trainer
  245  

  323  ```py
  324: >>> from transformers import pipeline
  325  

  341  ```py
  342: >>> from transformers import AutoProcessor
  343  

  350  ```py
  351: >>> from transformers import AutoModelForCTC
  352  

train_real_world/transformers_4573/docs/source/ko/tasks/audio_classification.md:
  125  ```py
  126: >>> from transformers import AutoFeatureExtractor
  127  

  198  ```py
  199: >>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer
  200  

  272  ```py
  273: >>> from transformers import pipeline
  274  

  290  ```py
  291: >>> from transformers import AutoFeatureExtractor
  292  

  299  ```py
  300: >>> from transformers import AutoModelForAudioClassification
  301  

train_real_world/transformers_4573/docs/source/ko/tasks/document_question_answering.md:
  153  ```py
  154: >>> from transformers import AutoProcessor
  155  

  360  ```py
  361: >>> from transformers import AutoModelForDocumentQuestionAnswering
  362  

  370  ```py
  371: >>> from transformers import TrainingArguments
  372  

  392  ```py
  393: >>> from transformers import DefaultDataCollator
  394  

  400  ```py
  401: >>> from transformers import Trainer
  402  

  440  ```py
  441: >>> from transformers import pipeline
  442  

  459  >>> import torch
  460: >>> from transformers import AutoProcessor
  461: >>> from transformers import AutoModelForDocumentQuestionAnswering
  462  

train_real_world/transformers_4573/docs/source/ko/tasks/idefics.md:
   62  
   63: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
   64  

   77  >>> import torch
   78: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig
   79  

  353  >>> import torch
  354: >>> from transformers import IdeficsForVisionText2Text, AutoProcessor
  355  

train_real_world/transformers_4573/docs/source/ko/tasks/image_captioning.md:
  123  ```python
  124: from transformers import AutoProcessor
  125  

  152  ```python
  153: from transformers import AutoModelForCausalLM
  154  

  190  ```python
  191: from transformers import TrainingArguments, Trainer
  192  

train_real_world/transformers_4573/docs/source/ko/tasks/image_classification.md:
  104  ```py
  105: >>> from transformers import AutoImageProcessor
  106  

  144  ```py
  145: >>> from transformers import DefaultDataCollator
  146  

  187  ```py
  188: >>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer
  189  

  265  ```py
  266: >>> from transformers import pipeline
  267  

  281  ```py
  282: >>> from transformers import AutoImageProcessor
  283  >>> import torch

  291  ```py
  292: >>> from transformers import AutoModelForImageClassification
  293  

train_real_world/transformers_4573/docs/source/ko/tasks/image_feature_extraction.md:
   47  import torch
   48: from transformers import pipeline, infer_device
   49  

  104  ```python
  105: from transformers import AutoImageProcessor, AutoModel
  106  

train_real_world/transformers_4573/docs/source/ko/tasks/image_to_image.md:
  37  ```python
  38: from transformers import pipeline
  39  

  74  ```python
  75: from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor 
  76  

train_real_world/transformers_4573/docs/source/ko/tasks/keypoint_detection.md:
  31  ```python
  32: from transformers import AutoImageProcessor, SuperPointForKeypointDetection
  33  processor = AutoImageProcessor.from_pretrained("magic-leap-community/superpoint")

train_real_world/transformers_4573/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md:
   44  ```python
   45: from transformers import AutoImageProcessor
   46  teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

   58  ```python
   59: from transformers import TrainingArguments, Trainer
   60  import torch

  111  ```python
  112: from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification
  113  

  161  ```python
  162: from transformers import DefaultDataCollator
  163  

train_real_world/transformers_4573/docs/source/ko/tasks/language_modeling.md:
   98  ```py
   99: >>> from transformers import AutoTokenizer
  100  

  180  ```py
  181: >>> from transformers import DataCollatorForLanguageModeling
  182  

  199  ```py
  200: >>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
  201  

  265  ```py
  266: >>> from transformers import pipeline
  267  

  275  ```py
  276: >>> from transformers import AutoTokenizer
  277  

  284  ```py
  285: >>> from transformers import AutoModelForCausalLM
  286  

train_real_world/transformers_4573/docs/source/ko/tasks/mask_generation.md:
   49  ```python
   50: >>> from transformers import pipeline
   51  

  121  ```python
  122: from transformers import SamModel, SamProcessor
  123  import torch

train_real_world/transformers_4573/docs/source/ko/tasks/masked_language_modeling.md:
   96  ```py
   97: >>> from transformers import AutoTokenizer
   98  

  185  ```py
  186: >>> from transformers import DataCollatorForLanguageModeling
  187  

  201  ```py
  202: >>> from transformers import AutoModelForMaskedLM
  203  

  271  ```py
  272: >>> from transformers import pipeline
  273  

  292  ```py
  293: >>> from transformers import AutoTokenizer
  294  

  302  ```py
  303: >>> from transformers import AutoModelForMaskedLM
  304  

train_real_world/transformers_4573/docs/source/ko/tasks/monocular_depth_estimation.md:
  48  ```py
  49: >>> from transformers import pipeline
  50  

  97  ```py
  98: >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  99  

train_real_world/transformers_4573/docs/source/ko/tasks/multiple_choice.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  116  ```py
  117: >>> from transformers import DataCollatorForMultipleChoice
  118  >>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)

  155  ```py
  156: >>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer
  157  

  223  ```py
  224: >>> from transformers import AutoTokenizer
  225  

  233  ```py
  234: >>> from transformers import AutoModelForMultipleChoice
  235  

train_real_world/transformers_4573/docs/source/ko/tasks/object_detection.md:
  171  ```py
  172: >>> from transformers import AutoImageProcessor
  173  

  319  ```py
  320: >>> from transformers import AutoModelForObjectDetection
  321  

  336  ```py
  337: >>> from transformers import TrainingArguments
  338  

  356  ```py
  357: >>> from transformers import Trainer
  358  

  535  ```py
  536: >>> from transformers import pipeline
  537  >>> import requests

train_real_world/transformers_4573/docs/source/ko/tasks/prompting.md:
  57  ```python
  58: >>> from transformers import pipeline
  59  >>> import torch

  98  ```python
  99: >>> from transformers import pipeline, AutoTokenizer
  100  >>> import torch

train_real_world/transformers_4573/docs/source/ko/tasks/question_answering.md:
   93  ```py
   94: >>> from transformers import AutoTokenizer
   95  

  168  ```py
  169: >>> from transformers import DefaultDataCollator
  170  

  184  ```py
  185: >>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
  186  

  251  ```py
  252: >>> from transformers import pipeline
  253  

  266  ```py
  267: >>> from transformers import AutoTokenizer
  268  

  275  ```py
  276: >>> from transformers import AutoModelForQuestionAnswering
  277  

train_real_world/transformers_4573/docs/source/ko/tasks/semantic_segmentation.md:
  100  ```py
  101: >>> from transformers import AutoImageProcessor
  102  

  197  ```py
  198: >>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer
  199  

  262  ```py
  263: >>> from transformers import pipeline
  264  

train_real_world/transformers_4573/docs/source/ko/tasks/sequence_classification.md:
   79  ```py
   80: >>> from transformers import AutoTokenizer
   81  

  100  ```py
  101: >>> from transformers import DataCollatorWithPadding
  102  

  147  ```py
  148: >>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
  149  

  218  ```py
  219: >>> from transformers import pipeline
  220  

  230  ```py
  231: >>> from transformers import AutoTokenizer
  232  

  239  ```py
  240: >>> from transformers import AutoModelForSequenceClassification
  241  

train_real_world/transformers_4573/docs/source/ko/tasks/summarization.md:
   90  ```py
   91: >>> from transformers import AutoTokenizer
   92  

  127  ```py
  128: >>> from transformers import DataCollatorForSeq2Seq
  129  

  178  ```py
  179: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  180  

  247  ```py
  248: >>> from transformers import pipeline
  249  

  260  ```py
  261: >>> from transformers import AutoTokenizer
  262  

  270  ```py
  271: >>> from transformers import AutoModelForSeq2SeqLM
  272  

train_real_world/transformers_4573/docs/source/ko/tasks/token_classification.md:
  104  ```py
  105: >>> from transformers import AutoTokenizer
  106  

  158  ```py
  159: >>> from transformers import DataCollatorForTokenClassification
  160  

  251  ```py
  252: >>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
  253  

  318  ```py
  319: >>> from transformers import pipeline
  320  

  359  ```py
  360: >>> from transformers import AutoTokenizer
  361  

  368  ```py
  369: >>> from transformers import AutoModelForTokenClassification
  370  

train_real_world/transformers_4573/docs/source/ko/tasks/translation.md:
   83  ```py
   84: >>> from transformers import AutoTokenizer
   85  

  117  ```py
  118: >>> from transformers import DataCollatorForSeq2Seq
  119  

  178  ```py
  179: >>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
  180  

  242  ```py
  243: >>> from transformers import pipeline
  244  

  257  ```py
  258: >>> from transformers import AutoTokenizer
  259  

  266  ```py
  267: >>> from transformers import AutoModelForSeq2SeqLM
  268  

train_real_world/transformers_4573/docs/source/ko/tasks/video_classification.md:
  145  ```py
  146: >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification
  147  

  345  ```py
  346: >>> from transformers import TrainingArguments, Trainer
  347  

  442  ```py
  443: >>> from transformers import pipeline
  444  

train_real_world/transformers_4573/docs/source/ko/tasks/visual_question_answering.md:
  165  ```py
  166: >>> from transformers import ViltProcessor
  167  

  219  ```py
  220: >>> from transformers import DefaultDataCollator
  221  

  229  ```py
  230: >>> from transformers import ViltForQuestionAnswering
  231  

  239  ```py
  240: >>> from transformers import TrainingArguments
  241  

  259  ```py
  260: >>> from transformers import Trainer
  261  

  287  ```py
  288: >>> from transformers import pipeline
  289  

  340  ```py
  341: >>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
  342  >>> import torch

train_real_world/transformers_4573/docs/source/ko/tasks/zero_shot_image_classification.md:
  48  ```python
  49: >>> from transformers import pipeline
  50  

  91  ```py
  92: >>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification
  93  

train_real_world/transformers_4573/docs/source/ko/tasks/zero_shot_object_detection.md:
   51  ```python
   52: >>> from transformers import pipeline
   53  

  137  ```py
  138: >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  139  

train_real_world/transformers_4573/docs/source/pt/accelerate.md:
  75  + from accelerate import Accelerator
  76:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  77  

train_real_world/transformers_4573/docs/source/pt/create_a_model.md:
   33  ```py
   34: >>> from transformers import DistilBertConfig
   35  

  116  ```py
  117: >>> from transformers import DistilBertModel
  118  

  143  ```py
  144: >>> from transformers import DistilBertForSequenceClassification
  145  

  151  ```py
  152: >>> from transformers import DistilBertForQuestionAnswering
  153  

  174  ```py
  175: >>> from transformers import DistilBertTokenizer
  176  

  182  ```py
  183: >>> from transformers import DistilBertTokenizer
  184  

  190  ```py
  191: >>> from transformers import DistilBertTokenizerFast
  192  

  208  ```py
  209: >>> from transformers import ViTFeatureExtractor
  210  

  240  ```py
  241: >>> from transformers import ViTFeatureExtractor
  242  

  266  ```py
  267: >>> from transformers import Wav2Vec2FeatureExtractor
  268  

  288  ```py
  289: >>> from transformers import Wav2Vec2FeatureExtractor
  290  

  296  ```py
  297: >>> from transformers import Wav2Vec2CTCTokenizer
  298  

  304  ```py
  305: >>> from transformers import Wav2Vec2Processor
  306  

train_real_world/transformers_4573/docs/source/pt/custom_models.md:
   39  ```python
   40: from transformers import PreTrainedConfig
   41  from typing import List

  118  ```py
  119: from transformers import PreTrainedModel
  120  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  318  ```py
  319: from transformers import AutoModelForImageClassification
  320  

  347  ```py
  348: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  349  

train_real_world/transformers_4573/docs/source/pt/fast_tokenizers.md:
  43  ```python
  44: >>> from transformers import PreTrainedTokenizerFast
  45  

  60  ```python
  61: >>> from transformers import PreTrainedTokenizerFast
  62  

train_real_world/transformers_4573/docs/source/pt/index.md:
  120  1. **[mLUKE](model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.
  121: 1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.
  122  1. **[M2M100](model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.

train_real_world/transformers_4573/docs/source/pt/installation.md:
   80  ```bash
   81: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   82  ```

  107  ```bash
  108: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
  109  ```

  204      ```py
  205:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  206  

  244  ```py
  245: >>> from transformers import AutoConfig
  246  

train_real_world/transformers_4573/docs/source/pt/multilingual.md:
   51  >>> import torch
   52: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   53  

  131  ```py
  132: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  133  

  169  ```py
  170: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  171  

train_real_world/transformers_4573/docs/source/pt/pipeline_tutorial.md:
   43  ```py
   44: >>> from transformers import pipeline
   45  

   85  ```py
   86: >>> from transformers import AutoTokenizer, AutoModelForCausalLM
   87  

   94  ```py
   95: >>> from transformers import pipeline
   96  

  116  ```py
  117: >>> from transformers import pipeline
  118  

  143  ```py
  144: >>> from transformers import pipeline
  145  

train_real_world/transformers_4573/docs/source/pt/quicktour.md:
   75  ```py
   76: >>> from transformers import pipeline
   77  

  107  >>> import torch
  108: >>> from transformers import pipeline
  109  

  148  ```py
  149: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  150  

  179  ```py
  180: >>> from transformers import AutoTokenizer
  181  

  222  ```py
  223: >>> from transformers import AutoModelForSequenceClassification
  224  

  287  ```py
  288: >>> from transformers import AutoModel
  289  

train_real_world/transformers_4573/docs/source/pt/training.md:
   58  ```py
   59: >>> from transformers import AutoTokenizer
   60  

   93  ```py
   94: >>> from transformers import AutoModelForSequenceClassification
   95  

  118  ```py
  119: >>> from transformers import TrainingArguments
  120  

  152  ```py
  153: >>> from transformers import TrainingArguments
  154  

  192  ```py
  193: >>> from transformers import DefaultDataCollator
  194  

  232  >>> import tensorflow as tf
  233: >>> from transformers import TFAutoModelForSequenceClassification
  234  

  311  ```py
  312: >>> from transformers import AutoModelForSequenceClassification
  313  

  330  ```py
  331: >>> from transformers import get_scheduler
  332  

train_real_world/transformers_4573/docs/source/pt/tasks/sequence_classification.md:
  60  ```py
  61: >>> from transformers import AutoTokenizer
  62  

  81  ```py
  82: >>> from transformers import DataCollatorWithPadding
  83  

  91  ```py
  92: >>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
  93  

train_real_world/transformers_4573/docs/source/pt/tasks/token_classification.md:
   85  ```py
   86: >>> from transformers import AutoTokenizer
   87  

  139  ```py
  140: >>> from transformers import DataCollatorForTokenClassification
  141  

  149  ```py
  150: >>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
  151  

train_real_world/transformers_4573/docs/source/zh/accelerate.md:
  63  + from accelerate import Accelerator
  64:   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
  65  

train_real_world/transformers_4573/docs/source/zh/add_new_pipeline.md:
   28  ```python
   29: from transformers import Pipeline
   30  

  109  ```python
  110: from transformers.pipelines import PIPELINE_REGISTRY
  111  

  138  
  139: from transformers import Pipeline
  140  

  176  from pair_classification import PairClassificationPipeline
  177: from transformers.pipelines import PIPELINE_REGISTRY
  178: from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
  179  

  191  ```py
  192: from transformers import pipeline
  193  

  211  ```py
  212: from transformers import pipeline
  213  

train_real_world/transformers_4573/docs/source/zh/autoclass_tutorial.md:
   43  ```py
   44: >>> from transformers import AutoTokenizer
   45  

   63  ```py
   64: >>> from transformers import AutoImageProcessor
   65  

   76  ```py
   77: >>> from transformers import AutoFeatureExtractor
   78  

   91  ```py
   92: >>> from transformers import AutoProcessor
   93  

  102  ```py
  103: >>> from transformers import AutoModelForSequenceClassification
  104  

  111  ```py
  112: >>> from transformers import AutoModelForTokenClassification
  113  

train_real_world/transformers_4573/docs/source/zh/big_models.md:
   42  ```py
   43: from transformers import AutoModel
   44  

  111  ```py
  112: >>> from transformers.trainer_utils import load_sharded_checkpoint
  113  

train_real_world/transformers_4573/docs/source/zh/chat_templating.md:
   31  ```python
   32: >>> from transformers import AutoTokenizer
   33  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

   48  ```python
   49: >>> from transformers import AutoTokenizer
   50  >>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

   73  ```python
   74: from transformers import AutoModelForCausalLM, AutoTokenizer
   75  

  122  ```python
  123: from transformers import pipeline
  124  

  194  ```python
  195: from transformers import AutoTokenizer
  196  from datasets import Dataset

  227  
  228: >>> from transformers import AutoTokenizer
  229  >>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

train_real_world/transformers_4573/docs/source/zh/create_a_model.md:
   34  ```py
   35: >>> from transformers import DistilBertConfig
   36  

  117  ```py
  118: >>> from transformers import DistilBertModel
  119  

  144  ```py
  145: >>> from transformers import DistilBertForSequenceClassification
  146  

  152  ```py
  153: >>> from transformers import DistilBertForQuestionAnswering
  154  

  175  ```py
  176: >>> from transformers import DistilBertTokenizer
  177  

  183  ```py
  184: >>> from transformers import DistilBertTokenizer
  185  

  191  ```py
  192: >>> from transformers import DistilBertTokenizerFast
  193  

  209  ```py
  210: >>> from transformers import ViTImageProcessor
  211  

  241  ```py
  242: >>> from transformers import ViTImageProcessor
  243  

  271  ```py
  272: >>> from transformers import Wav2Vec2FeatureExtractor
  273  

  295  ```py
  296: >>> from transformers import Wav2Vec2FeatureExtractor
  297  

  318  ```py
  319: >>> from transformers import Wav2Vec2FeatureExtractor
  320  

  326  ```py
  327: >>> from transformers import Wav2Vec2CTCTokenizer
  328  

  334  ```py
  335: >>> from transformers import Wav2Vec2Processor
  336  

train_real_world/transformers_4573/docs/source/zh/custom_models.md:
   31  ```python
   32: from transformers import PreTrainedConfig
   33  from typing import List

   99  ```py
  100: from transformers import PreTrainedModel
  101  from timm.models.resnet import BasicBlock, Bottleneck, ResNet

  274  ```py
  275: from transformers import AutoModelForImageClassification
  276  

  297  ```py
  298: from transformers import AutoConfig, AutoModel, AutoModelForImageClassification
  299  

train_real_world/transformers_4573/docs/source/zh/debugging.md:
   82  ```python
   83: from transformers.debug_utils import DebugUnderflowOverflow
   84  

  251  ```python
  252: from transformers.debug_utils import DebugUnderflowOverflow
  253  

train_real_world/transformers_4573/docs/source/zh/fast_tokenizers.md:
  43  ```python
  44: >>> from transformers import PreTrainedTokenizerFast
  45  

  61  ```python
  62: >>> from transformers import PreTrainedTokenizerFast
  63  

train_real_world/transformers_4573/docs/source/zh/gguf.md:
  83  ```py
  84: from transformers import AutoTokenizer, AutoModelForCausalLM
  85  

train_real_world/transformers_4573/docs/source/zh/installation.md:
   64  ```bash
   65: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
   66  ```

   86  ```bash
   87: python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
   88  ```

  175      ```py
  176:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  177  

  214  ```py
  215: >>> from transformers import AutoConfig
  216  

train_real_world/transformers_4573/docs/source/zh/llm_tutorial.md:
   81  ```py
   82: >>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   83  

   98  ```py
   99: >>> from transformers import AutoTokenizer
  100  

  135  ```py
  136: >>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  137  

  168  >>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
  169: >>> from transformers import set_seed
  170  >>> set_seed(42)

train_real_world/transformers_4573/docs/source/zh/model_sharing.md:
  117  ```py
  118: >>> from transformers import AutoModel
  119  

train_real_world/transformers_4573/docs/source/zh/multilingual.md:
   44  >>> import torch
   45: >>> from transformers import XLMTokenizer, XLMWithLMHeadModel
   46  

  118  ```py
  119: >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  120  

  154  ```py
  155: >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  156  

train_real_world/transformers_4573/docs/source/zh/peft.md:
   58  ```py
   59: from transformers import AutoModelForCausalLM, AutoTokenizer
   60  

   74  ```py
   75: from transformers import AutoModelForCausalLM, AutoTokenizer
   76  

   88  ```py
   89: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
   90  

   99  ```py
  100: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  101  from peft import PeftConfig

  139  ```py
  140: from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
  141  from peft import PeftConfig

train_real_world/transformers_4573/docs/source/zh/perf_infer_gpu_multi.md:
  24  import torch
  25: from transformers import AutoModelForCausalLM, AutoTokenizer
  26  

train_real_world/transformers_4573/docs/source/zh/perf_torch_compile.md:
  29  ```diff
  30: from transformers import AutoModelForImageClassification
  31  

  50  import numpy as np
  51: from transformers import AutoImageProcessor, AutoModelForImageClassification
  52  

  69  ```python 
  70: from transformers import AutoImageProcessor, AutoModelForObjectDetection
  71  

  85  ```python 
  86: from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation
  87  

train_real_world/transformers_4573/docs/source/zh/pipeline_tutorial.md:
   37  ```py
   38: >>> from transformers import pipeline
   39  

  192  # KeyDataset is a util that will just output the item we're interested in.
  193: from transformers.pipelines.pt_utils import KeyDataset
  194  from datasets import load_dataset

  219  ```py
  220: >>> from transformers import pipeline
  221  

  236  ```py
  237: >>> from transformers import pipeline
  238  

  256  ```py
  257: >>> from transformers import pipeline
  258  

  290  import torch
  291: from transformers import pipeline
  292  

  302  import torch
  303: from transformers import pipeline
  304  

train_real_world/transformers_4573/docs/source/zh/preprocessing.md:
   56  ```py
   57: >>> from transformers import AutoTokenizer
   58  

  250  ```py
  251: >>> from transformers import AutoFeatureExtractor
  252  

  354  ```py
  355: >>> from transformers import AutoImageProcessor
  356  

  489  ```py
  490: >>> from transformers import AutoProcessor
  491  

train_real_world/transformers_4573/docs/source/zh/quicktour.md:
   60  ```py
   61: >>> from transformers import pipeline
   62  

   86  >>> import torch
   87: >>> from transformers import pipeline
   88  

  127  ```py
  128: >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  129  

  158  ```py
  159: >>> from transformers import AutoTokenizer
  160  

  203  ```py
  204: >>> from transformers import AutoModelForSequenceClassification
  205  

  262  ```py
  263: >>> from transformers import AutoConfig
  264  

  270  ```py
  271: >>> from transformers import AutoModel
  272  

  286     ```py
  287:    >>> from transformers import AutoModelForSequenceClassification
  288  

  294     ```py
  295:    >>> from transformers import TrainingArguments
  296  

  308     ```py
  309:    >>> from transformers import AutoTokenizer
  310  

  333     ```py
  334:    >>> from transformers import DataCollatorWithPadding
  335  

  341  ```py
  342: >>> from transformers import Trainer
  343  

  376     ```py
  377:    >>> from transformers import TFAutoModelForSequenceClassification
  378  

  384     ```py
  385:    >>> from transformers import AutoTokenizer
  386  

train_real_world/transformers_4573/docs/source/zh/serialization.md:
   87  ```python
   88: >>> from transformers import AutoTokenizer
   89  >>> from optimum.onnxruntime import ORTModelForQuestionAnswering

  102  >>> from optimum.onnxruntime import ORTModelForSequenceClassification
  103: >>> from transformers import AutoTokenizer
  104  

train_real_world/transformers_4573/docs/source/zh/task_summary.md:
   38  ```py
   39: >>> from transformers import pipeline
   40  

   57  ```py
   58: >>> from transformers import pipeline
   59  

   83  ```py
   84: >>> from transformers import pipeline
   85  

  108  ```py
  109: >>> from transformers import pipeline
  110  

  131  ```py
  132: >>> from transformers import pipeline
  133  

  155  ```py
  156: >>> from transformers import pipeline
  157  

  176  ```py
  177: >>> from transformers import pipeline
  178  

  195  ```py
  196: >>> from transformers import pipeline
  197  

  231  ```py
  232: >>> from transformers import pipeline
  233  

  255  ```py
  256: >>> from transformers import pipeline
  257  

  271  ```py
  272: >>> from transformers import pipeline
  273  

  288      ```py
  289:     >>> from transformers import pipeline
  290  

  328  ```py
  329: >>> from transformers import pipeline
  330  >>> from PIL import Image

train_real_world/transformers_4573/docs/source/zh/tiktoken.md:
  29  ```py
  30: from transformers import AutoTokenizer
  31  

  42  
  43: from transformers.integrations.tiktoken import convert_tiktoken_to_fast
  44  from tiktoken import get_encoding

train_real_world/transformers_4573/docs/source/zh/tokenizer_summary.md:
   92  ```py
   93: >>> from transformers import BertTokenizer
   94  

  106  ```py
  107: >>> from transformers import XLNetTokenizer
  108  

train_real_world/transformers_4573/docs/source/zh/training.md:
   48  ```py
   49: >>> from transformers import AutoTokenizer
   50  

   83  ```py
   84: >>> from transformers import AutoModelForSequenceClassification
   85  

  101  ```py
  102: >>> from transformers import TrainingArguments
  103  

  128  ```py
  129: >>> from transformers import TrainingArguments, Trainer
  130  

  210  ```py
  211: >>> from transformers import AutoModelForSequenceClassification
  212  

  229  ```py
  230: >>> from transformers import get_scheduler
  231  

train_real_world/transformers_4573/docs/source/zh/internal/generation_utils.md:
  27  ```python
  28: from transformers import GPT2Tokenizer, GPT2LMHeadModel
  29  

train_real_world/transformers_4573/docs/source/zh/main_classes/deepspeed.md:
  1503  ```python
  1504: from transformers.trainer_utils import get_last_checkpoint
  1505  from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

  1592  ```python
  1593: from transformers import T5ForConditionalGeneration, T5Config
  1594  import deepspeed

  1605  ```python
  1606: from transformers import AutoModel, Trainer, TrainingArguments
  1607  

  1679  ```bash
  1680: $ python -c 'from transformers import AutoModel; \
  1681  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

  1705  ```bash
  1706: $ python -c 'from transformers import AutoModel; \
  1707  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

  1831  ```python
  1832: from transformers.integrations import HfDeepSpeedConfig
  1833: from transformers import AutoModel
  1834  import deepspeed

  1845  ```python
  1846: from transformers.integrations import HfDeepSpeedConfig
  1847: from transformers import AutoModel, AutoConfig
  1848  import deepspeed

  1914  
  1915: from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
  1916: from transformers.integrations import HfDeepSpeedConfig
  1917  import deepspeed

train_real_world/transformers_4573/docs/source/zh/main_classes/logging.md:
  45  ```python
  46: from transformers.utils import logging
  47  

train_real_world/transformers_4573/docs/source/zh/main_classes/model.md:
  34  ```python
  35: from transformers import AutoModelForSeq2SeqLM
  36  

train_real_world/transformers_4573/docs/source/zh/main_classes/output.md:
  23  ```python
  24: from transformers import BertTokenizer, BertForSequenceClassification
  25  import torch

train_real_world/transformers_4573/docs/source/zh/main_classes/pipelines.md:
   59  import datasets
   60: from transformers import pipeline
   61: from transformers.pipelines.pt_utils import KeyDataset
   62  from tqdm.auto import tqdm

   79  ```python
   80: from transformers import pipeline
   81  

  108  ```python
  109: from transformers import pipeline
  110: from transformers.pipelines.pt_utils import KeyDataset
  111  import datasets

  130  ```python
  131: from transformers import pipeline
  132  from torch.utils.data import Dataset

train_real_world/transformers_4573/docs/source/zh/main_classes/quantization.md:
   51  ```python
   52: from transformers import AutoModelForCausalLM, AutoTokenizer
   53  

   60  ```python
   61: from transformers import AutoModelForCausalLM, AutoTokenizer
   62  

   71  ```python
   72: from transformers import AutoModelForCausalLM, AutoTokenizer
   73  

  173  ```python
  174: from transformers import AutoModelForCausalLM
  175  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)

  184  ```python
  185: from transformers import AutoModelForCausalLM
  186  model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)

  225  ```python
  226: from transformers import AutoModelForCausalLM
  227  model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq")

  232  ```python
  233: from transformers import AutoModelForCausalLM
  234  model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")

  291  ```python
  292: from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  293  

  301  >>> import torch
  302: >>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig
  303  

  341  # pip install transformers accelerate bitsandbytes
  342: from transformers import AutoModelForCausalLM, AutoTokenizer
  343  

  362  # pip install transformers accelerate bitsandbytes
  363: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  364  

  401  import torch
  402: from transformers import BitsAndBytesConfig
  403  

  411  ```python
  412: from transformers import BitsAndBytesConfig
  413  

  426  ```python
  427: from transformers import BitsAndBytesConfig
  428  

  443  ```python
  444: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  445  

  462  ```python
  463: from transformers import AutoModelForCausalLM, AutoTokenizer
  464  

  482  ```python
  483: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  484  

  518  ```python
  519: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  520  

  540  ```python
  541: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
  542  

train_real_world/transformers_4573/docs/source/zh/main_classes/trainer.md:
   59  from torch import nn
   60: from transformers import Trainer
   61  

  653  ```python
  654: from transformers import Trainer, TrainingArguments
  655  

train_real_world/transformers_4573/docs/source/zh/model_doc/bert.md:
  42  import torch
  43: from transformers import pipeline
  44  

  58  import torch
  59: from transformers import AutoModelForMaskedLM, AutoTokenizer
  60  

train_real_world/transformers_4573/docs/source/zh/tasks/asr.md:
  114  ```py
  115: >>> from transformers import AutoProcessor
  116  

  254  ```py
  255: >>> from transformers import AutoModelForCTC, TrainingArguments, Trainer
  256  

  339  ```py
  340: >>> from transformers import pipeline
  341  

  358  ```py
  359: >>> from transformers import AutoProcessor
  360  

  367  ```py
  368: >>> from transformers import AutoModelForCTC
  369  

train_real_world/transformers_4573/examples/3D_parallel.py:
  54  
  55: from transformers import AutoModelForCausalLM, AutoTokenizer
  56  

train_real_world/transformers_4573/examples/metrics-monitoring/metrics_example.py:
  2  
  3: from transformers.utils.metrics import attach_tracer, traced
  4  

train_real_world/transformers_4573/examples/modular-transformers/configuration_duplicated_method.py:
  83      ```python
  84:     >>> from transformers import DuplicatedMethodModel, DuplicatedMethodConfig
  85  

train_real_world/transformers_4573/examples/modular-transformers/configuration_my_new_model.py:
  118      ```python
  119:     >>> from transformers import MyNewModelModel, MyNewModelConfig
  120  

train_real_world/transformers_4573/examples/modular-transformers/configuration_my_new_model2.py:
  25      ```python
  26:     >>> from transformers import GemmaModel, GemmaConfig
  27      >>> # Initializing a Gemma gemma-7b style configuration

train_real_world/transformers_4573/examples/modular-transformers/configuration_new_model.py:
  74      ```python
  75:     >>> from transformers import NewModelModel, NewModelConfig
  76      >>> # Initializing a NewModel new_model-7b style configuration

train_real_world/transformers_4573/examples/modular-transformers/modeling_add_function.py:
  55  
  56:     Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:
  57      The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.

train_real_world/transformers_4573/examples/modular-transformers/modeling_global_indexing.py:
  12  
  13: from transformers.modeling_utils import AttentionInterface
  14  

train_real_world/transformers_4573/examples/modular-transformers/modeling_multimodal2.py:
   13  
   14: from transformers.utils import add_start_docstrings
   15  

  381          >>> import requests
  382:         >>> from transformers import AutoProcessor, Multimodal2VisionModel
  383  

train_real_world/transformers_4573/examples/modular-transformers/modeling_new_task_model.py:
  311          >>> import requests
  312:         >>> from transformers import AutoProcessor, NewTaskModelForConditionalGeneration
  313  

train_real_world/transformers_4573/examples/modular-transformers/modeling_super.py:
  12  
  13: from transformers.modeling_outputs import CausalLMOutputWithPast
  14  

train_real_world/transformers_4573/examples/modular-transformers/modeling_test_detr.py:
  1400          ```python
  1401:         >>> from transformers import AutoImageProcessor, TestDetrModel
  1402          >>> from PIL import Image

train_real_world/transformers_4573/examples/modular-transformers/modular_add_function.py:
  1  # Note that zamba does not have the `apply_rotary_pos_emb` function!
  2: from transformers.models.llama.modeling_llama import apply_rotary_pos_emb
  3: from transformers.models.zamba.modeling_zamba import ZambaAttention
  4  

train_real_world/transformers_4573/examples/modular-transformers/modular_dummy_bert.py:
  4  
  5: from transformers.models.bert.modeling_bert import BertModel
  6  

train_real_world/transformers_4573/examples/modular-transformers/modular_duplicated_method.py:
  1: from transformers.models.llama.configuration_llama import LlamaConfig
  2  

train_real_world/transformers_4573/examples/modular-transformers/modular_from_uppercase_model.py:
  1: from transformers.models.clip.modeling_clip import CLIPEncoderLayer
  2  

train_real_world/transformers_4573/examples/modular-transformers/modular_global_indexing.py:
  1: from transformers.modeling_utils import AttentionInterface
  2: from transformers.models.llama.modeling_llama import LlamaAttention
  3  

train_real_world/transformers_4573/examples/modular-transformers/modular_multimodal2.py:
  13  
  14: from transformers.models.clip.modeling_clip import (
  15      CLIPMLP,

  22  )
  23: from transformers.utils import add_start_docstrings
  24  

train_real_world/transformers_4573/examples/modular-transformers/modular_my_new_model.py:
    1: from transformers.models.llama.configuration_llama import LlamaConfig
    2  

  109      ```python
  110:     >>> from transformers import MyNewModelModel, MyNewModelConfig
  111  

train_real_world/transformers_4573/examples/modular-transformers/modular_my_new_model2.py:
   1: from transformers.models.gemma.modeling_gemma import GemmaForSequenceClassification
   2: from transformers.models.llama.configuration_llama import LlamaConfig
   3  

  18      ```python
  19:     >>> from transformers import GemmaModel, GemmaConfig
  20      >>> # Initializing a Gemma gemma-7b style configuration

train_real_world/transformers_4573/examples/modular-transformers/modular_new_imgproc_model.py:
  3  
  4: from transformers.models.blip.image_processing_blip import BlipImageProcessor
  5  

train_real_world/transformers_4573/examples/modular-transformers/modular_new_model.py:
  2  
  3: from transformers.models.gemma.configuration_gemma import GemmaConfig
  4  

train_real_world/transformers_4573/examples/modular-transformers/modular_new_task_model.py:
  6  
  7: from transformers.models.paligemma.modeling_paligemma import PaliGemmaForConditionalGeneration
  8  

train_real_world/transformers_4573/examples/modular-transformers/modular_roberta.py:
  2  
  3: from transformers.models.bert.modeling_bert import BertEmbeddings, BertModel
  4  

train_real_world/transformers_4573/examples/modular-transformers/modular_super.py:
  4  
  5: from transformers.modeling_outputs import CausalLMOutputWithPast
  6: from transformers.models.llama.modeling_llama import LlamaModel
  7  

train_real_world/transformers_4573/examples/modular-transformers/modular_switch_function.py:
  1  # Note that llama and cohere have different definitions for rotate_half
  2: from transformers.models.cohere.modeling_cohere import rotate_half  # noqa
  3: from transformers.models.llama.modeling_llama import LlamaAttention
  4  

train_real_world/transformers_4573/examples/modular-transformers/modular_test_detr.py:
  1: from transformers.models.deformable_detr.modeling_deformable_detr import DeformableDetrModel
  2  

train_real_world/transformers_4573/examples/modular-transformers/modular_test_suffix.py:
  2  
  3: from transformers.models.llama.modeling_llama import LlamaDecoderLayer
  4  

train_real_world/transformers_4573/examples/pytorch/3d_parallel_checks.py:
  55  
  56: from transformers import AutoModelForCausalLM, AutoTokenizer
  57  

train_real_world/transformers_4573/examples/pytorch/conftest.py:
  34  def pytest_addoption(parser):
  35:     from transformers.testing_utils import pytest_addoption_shared
  36  

  40  def pytest_terminal_summary(terminalreporter):
  41:     from transformers.testing_utils import pytest_terminal_summary_main
  42  

train_real_world/transformers_4573/examples/pytorch/context_parallel.py:
  22  
  23: from transformers import AutoModelForCausalLM
  24: from transformers.loss.loss_utils import ForCausalLMLoss
  25  

train_real_world/transformers_4573/examples/pytorch/continuous_batching_simple.py:
  20  
  21: from transformers import AutoModelForCausalLM, AutoTokenizer
  22: from transformers.generation import GenerationConfig
  23  

train_real_world/transformers_4573/examples/pytorch/continuous_batching.py:
  27  
  28: from transformers import AutoModelForCausalLM, AutoTokenizer, CompileConfig
  29: from transformers.generation import GenerationConfig
  30: from transformers.generation.continuous_batching.requests import logger
  31  

train_real_world/transformers_4573/examples/pytorch/old_test_xla_examples.py:
  22  
  23: from transformers.testing_utils import TestCasePlus, require_torch_xla
  24  

train_real_world/transformers_4573/examples/pytorch/test_accelerate_examples.py:
  27  
  28: from transformers.testing_utils import (
  29      TestCasePlus,

train_real_world/transformers_4573/examples/pytorch/test_pytorch_examples.py:
  21  
  22: from transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining
  23: from transformers.testing_utils import (
  24      CaptureLogger,

train_real_world/transformers_4573/examples/pytorch/audio-classification/run_audio_classification.py:
  39  import transformers
  40: from transformers import (
  41      AutoConfig,

  48  )
  49: from transformers.utils import check_min_version
  50: from transformers.utils.versions import require_version
  51  

train_real_world/transformers_4573/examples/pytorch/contrastive-image-text/README.md:
  58  ```python3
  59: from transformers import (
  60      VisionTextDualEncoderModel,

train_real_world/transformers_4573/examples/pytorch/contrastive-image-text/run_clip.py:
  48  import transformers
  49: from transformers import (
  50      AutoImageProcessor,

  57  )
  58: from transformers.utils import check_min_version
  59: from transformers.utils.versions import require_version
  60  

train_real_world/transformers_4573/examples/pytorch/image-classification/run_image_classification_no_trainer.py:
  57  import transformers
  58: from transformers import AutoConfig, AutoImageProcessor, AutoModelForImageClassification, SchedulerType, get_scheduler
  59: from transformers.utils import check_min_version
  60: from transformers.utils.versions import require_version
  61  

train_real_world/transformers_4573/examples/pytorch/image-classification/run_image_classification.py:
  49  import transformers
  50: from transformers import (
  51      MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,

  60  )
  61: from transformers.utils import check_min_version
  62: from transformers.utils.versions import require_version
  63  

train_real_world/transformers_4573/examples/pytorch/image-pretraining/README.md:
  69  ```python
  70: from transformers import SwinConfig
  71  

train_real_world/transformers_4573/examples/pytorch/image-pretraining/run_mae.py:
  35  import transformers
  36: from transformers import (
  37      HfArgumentParser,

  43  )
  44: from transformers.utils import check_min_version
  45: from transformers.utils.versions import require_version
  46  

train_real_world/transformers_4573/examples/pytorch/image-pretraining/run_mim_no_trainer.py:
  41  import transformers
  42: from transformers import (
  43      CONFIG_MAPPING,

  51  )
  52: from transformers.utils import check_min_version
  53: from transformers.utils.versions import require_version
  54  

train_real_world/transformers_4573/examples/pytorch/image-pretraining/run_mim.py:
  35  import transformers
  36: from transformers import (
  37      CONFIG_MAPPING,

  46  )
  47: from transformers.utils import check_min_version
  48: from transformers.utils.versions import require_version
  49  

train_real_world/transformers_4573/examples/pytorch/instance-segmentation/README.md:
  132  from PIL import Image
  133: from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor
  134  

train_real_world/transformers_4573/examples/pytorch/instance-segmentation/run_instance_segmentation_no_trainer.py:
  51  import transformers
  52: from transformers import (
  53      AutoImageProcessor,

  57  )
  58: from transformers.image_processing_utils import BatchFeature
  59: from transformers.utils import check_min_version
  60: from transformers.utils.versions import require_version
  61  

train_real_world/transformers_4573/examples/pytorch/instance-segmentation/run_instance_segmentation.py:
  42  import transformers
  43: from transformers import (
  44      AutoImageProcessor,

  49  )
  50: from transformers.image_processing_utils import BatchFeature
  51: from transformers.trainer import EvalPrediction
  52: from transformers.utils import check_min_version
  53: from transformers.utils.versions import require_version
  54  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_clm_no_trainer.py:
  58  import transformers
  59: from transformers import (
  60      CONFIG_MAPPING,

  68  )
  69: from transformers.utils import check_min_version
  70: from transformers.utils.versions import require_version
  71  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_clm.py:
  51  import transformers
  52: from transformers import (
  53      CONFIG_MAPPING,

  64  )
  65: from transformers.testing_utils import CaptureLogger
  66: from transformers.utils import check_min_version
  67: from transformers.utils.versions import require_version
  68  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_fim_no_trainer.py:
  59  import transformers
  60: from transformers import (
  61      CONFIG_MAPPING,

  70  )
  71: from transformers.integrations import is_deepspeed_zero3_enabled
  72: from transformers.utils import check_min_version
  73: from transformers.utils.versions import require_version
  74  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_fim.py:
  53  import transformers
  54: from transformers import (
  55      CONFIG_MAPPING,

  66  )
  67: from transformers.integrations import is_deepspeed_zero3_enabled
  68: from transformers.testing_utils import CaptureLogger
  69: from transformers.utils import check_min_version
  70: from transformers.utils.versions import require_version
  71  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_mlm_no_trainer.py:
  58  import transformers
  59: from transformers import (
  60      CONFIG_MAPPING,

  68  )
  69: from transformers.utils import check_min_version
  70: from transformers.utils.versions import require_version
  71  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_mlm.py:
  51  import transformers
  52: from transformers import (
  53      CONFIG_MAPPING,

  64  )
  65: from transformers.utils import check_min_version
  66: from transformers.utils.versions import require_version
  67  

train_real_world/transformers_4573/examples/pytorch/language-modeling/run_plm.py:
  46  import transformers
  47: from transformers import (
  48      AutoConfig,

  57  )
  58: from transformers.utils import check_min_version
  59: from transformers.utils.versions import require_version
  60  

train_real_world/transformers_4573/examples/pytorch/multiple-choice/run_swag_no_trainer.py:
  52  import transformers
  53: from transformers import (
  54      CONFIG_MAPPING,

  63  )
  64: from transformers.utils import check_min_version
  65  

train_real_world/transformers_4573/examples/pytorch/multiple-choice/run_swag.py:
  43  import transformers
  44: from transformers import (
  45      AutoConfig,

  54  )
  55: from transformers.utils import check_min_version
  56  

train_real_world/transformers_4573/examples/pytorch/object-detection/README.md:
  132  from PIL import Image
  133: from transformers import AutoImageProcessor, AutoModelForObjectDetection
  134  

train_real_world/transformers_4573/examples/pytorch/object-detection/run_object_detection_no_trainer.py:
  51  import transformers
  52: from transformers import (
  53      AutoConfig,

  58  )
  59: from transformers.image_processing_utils import BatchFeature
  60: from transformers.image_transforms import center_to_corners_format
  61: from transformers.utils import check_min_version
  62: from transformers.utils.versions import require_version
  63  

train_real_world/transformers_4573/examples/pytorch/object-detection/run_object_detection.py:
  42  import transformers
  43: from transformers import (
  44      AutoConfig,

  50  )
  51: from transformers.image_processing_utils import BatchFeature
  52: from transformers.image_transforms import center_to_corners_format
  53: from transformers.trainer import EvalPrediction
  54: from transformers.utils import check_min_version
  55: from transformers.utils.versions import require_version
  56  

train_real_world/transformers_4573/examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py:
  41  import transformers
  42: from transformers import (
  43      DataCollatorWithPadding,

  51  )
  52: from transformers.utils import check_min_version
  53: from transformers.utils.versions import require_version
  54  

train_real_world/transformers_4573/examples/pytorch/question-answering/run_qa_beam_search.py:
  32  import transformers
  33: from transformers import (
  34      DataCollatorWithPadding,

  43  )
  44: from transformers.utils import check_min_version
  45: from transformers.utils.versions import require_version
  46  

train_real_world/transformers_4573/examples/pytorch/question-answering/run_qa_no_trainer.py:
  41  import transformers
  42: from transformers import (
  43      CONFIG_MAPPING,

  53  )
  54: from transformers.utils import check_min_version
  55: from transformers.utils.versions import require_version
  56  

train_real_world/transformers_4573/examples/pytorch/question-answering/run_qa.py:
  33  import transformers
  34: from transformers import (
  35      AutoConfig,

  44  )
  45: from transformers.utils import check_min_version
  46: from transformers.utils.versions import require_version
  47  

train_real_world/transformers_4573/examples/pytorch/question-answering/run_seq2seq_qa.py:
  32  import transformers
  33: from transformers import (
  34      AutoConfig,

  41  )
  42: from transformers.trainer_utils import EvalLoopOutput, EvalPrediction
  43: from transformers.utils import check_min_version
  44: from transformers.utils.versions import require_version
  45  

train_real_world/transformers_4573/examples/pytorch/question-answering/trainer_qa.py:
  20  
  21: from transformers import Trainer, is_torch_xla_available
  22: from transformers.trainer_utils import PredictionOutput, speed_metrics
  23  

train_real_world/transformers_4573/examples/pytorch/question-answering/trainer_seq2seq_qa.py:
  23  
  24: from transformers import Seq2SeqTrainer, is_torch_xla_available
  25: from transformers.trainer_utils import PredictionOutput, speed_metrics
  26  

train_real_world/transformers_4573/examples/pytorch/semantic-segmentation/README.md:
  166  ```python
  167: from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
  168  

train_real_world/transformers_4573/examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py:
  51  import transformers
  52: from transformers import (
  53      AutoConfig,

  59  )
  60: from transformers.utils import check_min_version
  61: from transformers.utils.versions import require_version
  62  

train_real_world/transformers_4573/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py:
  45  import transformers
  46: from transformers import (
  47      AutoConfig,

  54  )
  55: from transformers.utils import check_min_version
  56: from transformers.utils.versions import require_version
  57  

train_real_world/transformers_4573/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py:
  45  import transformers
  46: from transformers import (
  47      SchedulerType,

  54  )
  55: from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
  56  

train_real_world/transformers_4573/examples/pytorch/speech-recognition/README.md:
  451  ```python
  452: from transformers import SpeechEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, Wav2Vec2Processor
  453  

train_real_world/transformers_4573/examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py:
  47  import transformers
  48: from transformers import (
  49      AutoConfig,

  59  )
  60: from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE
  61: from transformers.trainer_utils import is_main_process
  62: from transformers.utils import check_min_version
  63: from transformers.utils.versions import require_version
  64  

train_real_world/transformers_4573/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py:
  45  import transformers
  46: from transformers import (
  47      AutoConfig,

  57  )
  58: from transformers.trainer_utils import is_main_process
  59: from transformers.utils import check_min_version
  60: from transformers.utils.versions import require_version
  61  

train_real_world/transformers_4573/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py:
  45  import transformers
  46: from transformers import (
  47      AutoConfig,

  54  )
  55: from transformers.trainer_utils import is_main_process
  56: from transformers.utils import check_min_version
  57: from transformers.utils.versions import require_version
  58  

train_real_world/transformers_4573/examples/pytorch/summarization/run_summarization_no_trainer.py:
  58  import transformers
  59: from transformers import (
  60      CONFIG_MAPPING,

  68  )
  69: from transformers.utils import check_min_version
  70: from transformers.utils.versions import require_version
  71  

train_real_world/transformers_4573/examples/pytorch/summarization/run_summarization.py:
  50  import transformers
  51: from transformers import (
  52      AutoConfig,

  64  )
  65: from transformers.utils import check_min_version
  66: from transformers.utils.versions import require_version
  67  

train_real_world/transformers_4573/examples/pytorch/text-classification/run_classification.py:
  45  import transformers
  46: from transformers import (
  47      AutoConfig,

  57  )
  58: from transformers.utils import check_min_version
  59: from transformers.utils.versions import require_version
  60  

train_real_world/transformers_4573/examples/pytorch/text-classification/run_glue_no_trainer.py:
  50  import transformers
  51: from transformers import (
  52      AutoConfig,

  60  )
  61: from transformers.utils import check_min_version
  62: from transformers.utils.versions import require_version
  63  

train_real_world/transformers_4573/examples/pytorch/text-classification/run_glue.py:
  46  import transformers
  47: from transformers import (
  48      AutoConfig,

  59  )
  60: from transformers.utils import check_min_version
  61: from transformers.utils.versions import require_version
  62  

train_real_world/transformers_4573/examples/pytorch/text-classification/run_xnli.py:
  46  import transformers
  47: from transformers import (
  48      AutoConfig,

  58  )
  59: from transformers.utils import check_min_version
  60: from transformers.utils.versions import require_version
  61  

train_real_world/transformers_4573/examples/pytorch/text-generation/run_generation.py:
  36  
  37: from transformers import (
  38      AutoTokenizer,

  54  )
  55: from transformers.modeling_outputs import CausalLMOutputWithPast
  56  

train_real_world/transformers_4573/examples/pytorch/token-classification/run_ner_no_trainer.py:
  52  import transformers
  53: from transformers import (
  54      CONFIG_MAPPING,

  64  )
  65: from transformers.utils import check_min_version
  66: from transformers.utils.versions import require_version
  67  

train_real_world/transformers_4573/examples/pytorch/token-classification/run_ner.py:
  44  import transformers
  45: from transformers import (
  46      AutoConfig,

  55  )
  56: from transformers.utils import check_min_version
  57: from transformers.utils.versions import require_version
  58  

train_real_world/transformers_4573/examples/pytorch/translation/run_translation_no_trainer.py:
  55  import transformers
  56: from transformers import (
  57      CONFIG_MAPPING,

  68  )
  69: from transformers.utils import check_min_version
  70: from transformers.utils.versions import require_version
  71  

train_real_world/transformers_4573/examples/pytorch/translation/run_translation.py:
  46  import transformers
  47: from transformers import (
  48      AutoConfig,

  62  )
  63: from transformers.tokenization_utils_sentencepiece import SentencePieceBackend
  64: from transformers.utils import check_min_version
  65: from transformers.utils.versions import require_version
  66  

train_real_world/transformers_4573/examples/quantization/custom_quantization_int8_example.py:
   9  
  10: from transformers import AutoModelForCausalLM, AutoTokenizer
  11: from transformers.quantizers import HfQuantizer, get_module_from_name, register_quantization_config, register_quantizer
  12: from transformers.utils.quantization_config import QuantizationConfigMixin
  13  

train_real_world/transformers_4573/examples/quantization/custom_quantization.py:
  5  
  6: from transformers import AutoModelForCausalLM, AutoTokenizer
  7: from transformers.quantizers import HfQuantizer, register_quantization_config, register_quantizer
  8: from transformers.utils.quantization_config import QuantizationConfigMixin
  9  

train_real_world/transformers_4573/i18n/README_ar.md:
  139  ```python
  140: >>> from transformers import pipeline
  141  

  154  >>> from PIL import Image
  155: >>> from transformers import pipeline
  156  

  192  ```python
  193: >>> from transformers import AutoTokenizerØŒ AutoModel
  194  

  203  ```python
  204: >>> from transformers import AutoTokenizerØŒ TFAutoModel
  205  

train_real_world/transformers_4573/i18n/README_bn.md:
  122  ```py
  123: from transformers import pipeline
  124  

  139  import torch
  140: from transformers import pipeline
  141  

  157  ```Python
  158: from transformers import pipeline
  159  

  174  ```py
  175: from transformers import pipeline
  176  

  196  ```py
  197: from transformers import pipeline
  198  

train_real_world/transformers_4573/i18n/README_de.md:
  138  ```python
  139: >>> from transformers import pipeline
  140  

  153  >>> from PIL import Image
  154: >>> from transformers import pipeline
  155  

  192  ```python
  193: >>> from transformers import AutoTokenizer, AutoModel
  194  

  204  ```python
  205: >>> from transformers import AutoTokenizer, TFAutoModel
  206  

train_real_world/transformers_4573/i18n/README_es.md:
  121  ```python
  122: >>> from transformers import pipeline
  123  

  136  >>> from PIL import Image
  137: >>> from transformers import pipeline
  138  

  174  ```python
  175: >>> from transformers import AutoTokenizer, AutoModel
  176  

  185  ```python
  186: >>> from transformers import AutoTokenizer, TFAutoModel
  187  

train_real_world/transformers_4573/i18n/README_fr.md:
  135  ```python
  136: >>> from transformers import pipeline
  137  

  150  >>> from PIL import Image
  151: >>> from transformers import pipeline
  152  

  189  ```python
  190: >>> from transformers import AutoTokenizer, AutoModel
  191  

  201  ```python
  202: from transformers import AutoTokenizer, TFAutoModel
  203  

train_real_world/transformers_4573/i18n/README_hd.md:
  122  ```python
  123: >>> from transformers import pipeline
  124  

  135  ``` python
  136: >>> from transformers import pipeline
  137  

  151  ```python
  152: >>> from transformers import AutoTokenizer, AutoModel
  153  

  161  ```python
  162: >>> from transformers import AutoTokenizer, TFAutoModel
  163  

train_real_world/transformers_4573/i18n/README_it.md:
  127  ```py
  128: from transformers import pipeline
  129  

  144  import torch
  145: from transformers import pipeline
  146  

  162  ```py
  163: from transformers import pipeline
  164  

  179  ```py
  180: from transformers import pipeline
  181  

  202  ```py
  203: from transformers import pipeline
  204  

train_real_world/transformers_4573/i18n/README_ja.md:
  155  ```python
  156: >>> from transformers import pipeline
  157  

  170  >>> from PIL import Image
  171: >>> from transformers import pipeline
  172  

  208  ```python
  209: >>> from transformers import AutoTokenizer, AutoModel
  210  

  219  ```python
  220: >>> from transformers import AutoTokenizer, TFAutoModel
  221  

train_real_world/transformers_4573/i18n/README_ko.md:
  139  ```python
  140: >>> from transformers import pipeline
  141  

  154  >>> from PIL import Image
  155: >>> from transformers import pipeline
  156  

  190  ```python
  191: >>> from transformers import AutoTokenizer, AutoModel
  192  

  200  ```python
  201: >>> from transformers import AutoTokenizer, TFAutoModel
  202  

train_real_world/transformers_4573/i18n/README_pt-br.md:
  141  ```python
  142: from transformers import pipeline
  143  

  158  >>> from PIL import Image
  159: >>> from transformers import pipeline
  160  

  199  ```python
  200: >>> from transformers import AutoTokenizer, AutoModel
  201  

  211  ```python
  212: >>> from transformers import AutoTokenizer, TFAutoModel
  213  

train_real_world/transformers_4573/i18n/README_ru.md:
  138  ```python
  139: >>> from transformers import pipeline
  140  

  153  >>> from PIL import Image
  154: >>> from transformers import pipeline
  155  

  191  ```python
  192: >>> from transformers import AutoTokenizer, AutoModel
  193  

  202  ```python
  203: >>> from transformers import AutoTokenizer, TFAutoModel
  204  

train_real_world/transformers_4573/i18n/README_te.md:
  140  ```python
  141: >>> from transformers import pipeline
  142  

  155  >>> from PIL import Image
  156: >>> from transformers import pipeline
  157  

  193  ```python
  194: >>> from transformers import AutoTokenizer, AutoModel
  195  

  204  ```python
  205: >>> from transformers import AutoTokenizer, TFAutoModel
  206  

train_real_world/transformers_4573/i18n/README_ur.md:
  145  ```python
  146: >>> from transformers import pipeline
  147  

  161  >>> from PIL import Image
  162: >>> from transformers import pipeline
  163  

  201  ```python
  202: >>> from transformers import AutoTokenizerØŒ AutoModel
  203  

  212  ```python
  213: >>> from transformers import AutoTokenizerØŒ TFAutoModel
  214  

train_real_world/transformers_4573/i18n/README_vi.md:
  139  ```python
  140: >>> from transformers import pipeline
  141  

  154  >>> from PIL import Image
  155: >>> from transformers import pipeline
  156  

  192  ```python
  193: >>> from transformers import AutoTokenizer, AutoModel
  194  

  203  ```python
  204: >>> from transformers import AutoTokenizer, TFAutoModel
  205  

train_real_world/transformers_4573/i18n/README_zh-hans.md:
  146  ```py
  147: from transformers import pipeline
  148  

  163  import torch
  164: from transformers import pipeline
  165  

  181  ```py
  182: from transformers import pipeline
  183  

  198  ```py
  199: from transformers import pipeline
  200  

  221  ```py
  222: from transformers import pipeline
  223  

train_real_world/transformers_4573/i18n/README_zh-hant.md:
  121  ```py
  122: from transformers import pipeline
  123  

  138  import torch
  139: from transformers import pipeline
  140  

  156  ```py
  157: from transformers import pipeline
  158  

  173  ```py
  174: from transformers import pipeline
  175  

  196  ```py
  197: from transformers import pipeline
  198  

train_real_world/transformers_4573/scripts/check_tokenizers.py:
  5  import transformers
  6: from transformers.convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS
  7: from transformers.tokenization_utils_base import PreTrainedTokenizerBase
  8: from transformers.utils import logging
  9  

train_real_world/transformers_4573/src/transformers/cache_utils.py:
   923      ```python
   924:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
   925  

  1025      ```python
  1026:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache
  1027  

  1154      ```python
  1155:     >>> from transformers import AutoProcessor, AutoModelForCausalLM, DynamicCache, EncoderDecoderCache
  1156  

train_real_world/transformers_4573/src/transformers/convert_slow_tokenizer.py:
  104          if version.parse(google.protobuf.__version__) < version.parse("4.0.0"):
  105:             from transformers.utils import sentencepiece_model_pb2
  106          else:
  107:             from transformers.utils import sentencepiece_model_pb2_new as sentencepiece_model_pb2
  108          return sentencepiece_model_pb2

train_real_world/transformers_4573/src/transformers/model_debugging_utils.py:
  424      from PIL import Image
  425:     from transformers import LlavaProcessor, LlavaForConditionalGeneration, model_addition_debugger_context
  426  

train_real_world/transformers_4573/src/transformers/modeling_attn_mask_utils.py:
  40      >>> import torch
  41:     >>> from transformers.modeling_attn_mask_utils import AttentionMaskConverter
  42  

train_real_world/transformers_4573/src/transformers/modeling_utils.py:
  1425          ```python
  1426:         from transformers import AutoModel
  1427  

  3744          ```python
  3745:         >>> from transformers import BertConfig, BertModel
  3746  

train_real_world/transformers_4573/src/transformers/optimization.py:
  746      ```python
  747:     from transformers.optimization import Adafactor, AdafactorSchedule
  748  

train_real_world/transformers_4573/src/transformers/testing_utils.py:
    54  
    55: from transformers import logging as transformers_logging
    56  

  1754      ```python
  1755:     >>> from transformers import logging
  1756:     >>> from transformers.testing_utils import CaptureLogger
  1757  

  2112          ```
  2113:         one_liner_str = 'from transformers import AutoModel; AutoModel.from_pretrained("google-t5/t5-large")'
  2114          max_rss = self.python_one_liner_max_rss(one_liner_str)

train_real_world/transformers_4573/src/transformers/tokenization_mistral_common.py:
   25  
   26: from transformers.audio_utils import load_audio_as
   27: from transformers.tokenization_utils_base import (
   28      LARGE_INTEGER,

   35  )
   36: from transformers.utils import PaddingStrategy, TensorType, add_end_docstrings, logging, to_py_obj
   37: from transformers.utils.generic import is_torch_tensor
   38: from transformers.utils.hub import PushToHubMixin
   39: from transformers.utils.import_utils import is_mistral_common_available, is_torch_available, requires
   40  

  194      - The `return_token_type_ids` argument is not supported.
  195:     - It is not possible to add new tokens to the tokenizer. Also the special tokens are handled differently from Transformers. In `mistral-common`, special tokens are never encoded directly. This means that: `tokenizer.encode("<s>")` will not return the ID of the `<s>` token. Instead, it will return a list of IDs corresponding to the tokenization of the string `"<s>"`. For more information, see the [mistral-common documentation](https://mistralai.github.io/mistral-common/usage/tokenizers/#special-tokens).
  196  

train_real_world/transformers_4573/src/transformers/tokenization_utils_tokenizers.py:
  1145  
  1146:         from transformers.utils.hub import cached_file
  1147  

train_real_world/transformers_4573/src/transformers/trainer_callback.py:
  178          if trial is not None:
  179:             from transformers.integrations import hp_params
  180  

train_real_world/transformers_4573/src/transformers/trainer.py:
  1837  
  1838:             from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig
  1839  

  5165          """
  5166:         from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig
  5167  

train_real_world/transformers_4573/src/transformers/training_args.py:
  1668                  )
  1669:             from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig
  1670  

  2177          ```py
  2178:         >>> from transformers import TrainingArguments
  2179  

  2234          ```py
  2235:         >>> from transformers import TrainingArguments
  2236  

  2276          ```py
  2277:         >>> from transformers import TrainingArguments
  2278  

  2322          ```py
  2323:         >>> from transformers import TrainingArguments
  2324  

  2393          ```py
  2394:         >>> from transformers import TrainingArguments
  2395  

  2470          ```py
  2471:         >>> from transformers import TrainingArguments
  2472  

  2521          ```py
  2522:         >>> from transformers import TrainingArguments
  2523  

  2566          ```py
  2567:         >>> from transformers import TrainingArguments
  2568  

  2632          ```py
  2633:         >>> from transformers import TrainingArguments
  2634  

train_real_world/transformers_4573/src/transformers/cli/add_fast_image_processor.py:
  140          # add "is_torchvision_available" import to from ...utils import (
  141:         # Regex to match import statements from transformers.utils
  142          pattern = r"""

  280      # add is_torchvision_available import to the imports
  281:     # Regex to match import statements from transformers.utils
  282      pattern = r"""

  308  
  309:         return f"from transformers.utils import {updated_imports}"
  310  

  314      # add the fast image processor to the imports
  315:     base_import_string = f"    from transformers import {fast_image_processor_name[:-4]}"
  316      fast_import_string = (
  317:         f"    if is_torchvision_available():\n        from transformers import {fast_image_processor_name}"
  318      )

train_real_world/transformers_4573/src/transformers/cli/add_new_model_like.py:
  679      """
  680:     from transformers.models.auto.configuration_auto import MODEL_NAMES_MAPPING
  681  

train_real_world/transformers_4573/src/transformers/cli/chat.py:
  29  
  30: from transformers import GenerationConfig
  31: from transformers.utils import is_rich_available
  32  

train_real_world/transformers_4573/src/transformers/cli/serve.py:
    38  import transformers
    39: from transformers import AutoTokenizer, BitsAndBytesConfig, GenerationConfig, PreTrainedTokenizerBase
    40: from transformers.utils.import_utils import (
    41      is_fastapi_available,

    56  if TYPE_CHECKING:
    57:     from transformers import (
    58          PreTrainedModel,

   737          """
   738:         from transformers.models.auto.modeling_auto import (
   739              MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,

   933  
   934:         from transformers.models.auto.modeling_auto import (
   935              MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,

  1787  
  1788:         from transformers import AutoConfig, AutoProcessor
  1789  

train_real_world/transformers_4573/src/transformers/cli/transformers.py:
  17  
  18: from transformers.cli.add_fast_image_processor import add_fast_image_processor
  19: from transformers.cli.add_new_model_like import add_new_model_like
  20: from transformers.cli.chat import Chat
  21: from transformers.cli.download import download
  22: from transformers.cli.serve import Serve
  23: from transformers.cli.system import env, version
  24  

train_real_world/transformers_4573/src/transformers/distributed/configuration_utils.py:
  50  
  51:     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.to_json_file
  52      def to_json_file(self, json_file_path: str | os.PathLike):

  74  
  75:     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__iter__
  76      def __iter__(self):

  80  
  81:     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__repr__
  82      def __repr__(self):

train_real_world/transformers_4573/src/transformers/generation/configuration_utils.py:
   870          ```python
   871:         >>> from transformers import GenerationConfig
   872  

  1399      ```python
  1400:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig
  1401  

  1486      ```python
  1487:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer, CompileConfig
  1488  

train_real_world/transformers_4573/src/transformers/generation/logits_process.py:
   113      ```python
   114:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   115  

   177      ```python
   178:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   179  

   253      >>> import torch
   254:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   255  

   317      ```py
   318:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, RepetitionPenaltyLogitsProcessor
   319  

   427      ```python
   428:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer
   429  

   480      ```python
   481:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   482  

   546      ```python
   547:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   548  

   606      ```python
   607:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
   608  

   713      ```python
   714:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   715  

   784      ```python
   785:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   786  

   869      ```python
   870:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   871  

   950      ```python
   951:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
   952  

  1098      ```py
  1099:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1100  

  1149      ```py
  1150:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1151  

  1227      ```python
  1228:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1229  

  1407      ```python
  1408:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1409  

  1488      ```py
  1489:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1490  

  1566      ```python
  1567:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  1568      >>> import torch

  1684      ```python
  1685:     >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  1686  

  1732      ```python
  1733:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1734  

  1812      ```python
  1813:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
  1814  

  1905      ```python
  1906:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  1907      >>> import torch

  1942      ```python
  1943:     >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
  1944      >>> from datasets import load_dataset

  1994      ```python
  1995:     >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
  1996      >>> from datasets import load_dataset

  2057      >>> import torch
  2058:     >>> from transformers import AutoProcessor, WhisperForConditionalGeneration, GenerationConfig
  2059      >>> from datasets import load_dataset

  2258      ```python
  2259:     >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
  2260  

  2371      ```python
  2372:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  2373  

  2543      ```python
  2544:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkingConfig
  2545  

  2561      >>> # to detect watermarked text use the WatermarkDetector class
  2562:     >>> from transformers import WatermarkDetector
  2563      >>> detector = WatermarkDetector(model_config=model.config, device="cpu", watermarking_config= watermarking_config)

  2732      ```python
  2733:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig
  2734  

train_real_world/transformers_4573/src/transformers/generation/stopping_criteria.py:
  217      ```python
  218:     >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  219  

train_real_world/transformers_4573/src/transformers/generation/streamers.py:
   61          ```python
   62:         >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
   63  

  187          ```python
  188:         >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
  189          >>> from threading import Thread

  261          ```python
  262:         >>> from transformers import AutoModelForCausalLM, AutoTokenizer, AsyncTextIteratorStreamer
  263          >>> from threading import Thread

train_real_world/transformers_4573/src/transformers/generation/utils.py:
  1475          ```python
  1476:         >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM
  1477          >>> import numpy as np

train_real_world/transformers_4573/src/transformers/generation/watermarking.py:
   95      ```python
   96:     >>> from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig
   97  

  500      ```python
  501:     >>> from transformers import (
  502      ...     AutoTokenizer, BayesianDetectorModel, SynthIDTextWatermarkLogitsProcessor, SynthIDTextWatermarkDetector

train_real_world/transformers_4573/src/transformers/integrations/fp_quant.py:
  28  
  29: from transformers.utils.quantization_config import FPQuantConfig
  30  

train_real_world/transformers_4573/src/transformers/integrations/integration_utils.py:
    37  
    38: from transformers.utils.import_utils import _is_package_available
    39  

   309          try:
   310:             from transformers.utils.notebook import NotebookProgressCallback
   311  

  2209          if self._initialized and state.is_world_process_zero:
  2210:             from transformers.trainer import Trainer
  2211  

train_real_world/transformers_4573/src/transformers/integrations/mistral.py:
  3  
  4: from transformers.convert_slow_tokenizer import bytes_to_unicode
  5: from transformers.tokenization_utils_tokenizers import PreTrainedTokenizerFast
  6  

train_real_world/transformers_4573/src/transformers/integrations/tiktoken.py:
  3  
  4: from transformers.convert_slow_tokenizer import TikTokenConverter
  5: from transformers.tokenization_utils_tokenizers import TIKTOKEN_VOCAB_FILE, TOKENIZER_FILE
  6  

train_real_world/transformers_4573/src/transformers/integrations/torchao.py:
  23  
  24: from transformers.utils import logging
  25: from transformers.utils.import_utils import is_torch_available, is_torchao_available
  26  

train_real_world/transformers_4573/src/transformers/loss/loss_d_fine.py:
  25  if is_vision_available():
  26:     from transformers.image_transforms import center_to_corners_format
  27  

train_real_world/transformers_4573/src/transformers/loss/loss_for_object_detection.py:
  30  if is_vision_available():
  31:     from transformers.image_transforms import center_to_corners_format
  32  

train_real_world/transformers_4573/src/transformers/loss/loss_rt_detr.py:
  33  if is_vision_available():
  34:     from transformers.image_transforms import center_to_corners_format
  35  

train_real_world/transformers_4573/src/transformers/models/afmoe/configuration_afmoe.py:
  109      ```python
  110:     >>> from transformers import AfmoeModel, AfmoeConfig
  111  

train_real_world/transformers_4573/src/transformers/models/afmoe/modeling_afmoe.py:
  670          ```python
  671:         >>> from transformers import AutoTokenizer, AfmoeForCausalLM
  672  

train_real_world/transformers_4573/src/transformers/models/aimv2/configuration_aimv2.py:
   75      ```python
   76:     >>> from transformers import SiglipVisionConfig, SiglipVisionModel
   77  

  239      ```python
  240:     >>> from transformers import Aimv2Config, Aimv2Model
  241  

  251      >>> # We can also initialize a Aimv2Config from a Aimv2TextConfig and a Aimv2VisionConfig
  252:     >>> from transformers import Aimv2TextConfig, Aimv2VisionConfig
  253  

train_real_world/transformers_4573/src/transformers/models/aimv2/convert_aimv2_original_pytorch_to_hf.py:
  25  
  26: from transformers import (
  27      Aimv2Config,

train_real_world/transformers_4573/src/transformers/models/aimv2/modeling_aimv2.py:
  465          >>> import requests
  466:         >>> from transformers import AutoProcessor, Siglip2VisionModel
  467  

  623          >>> import torch
  624:         >>> from transformers import AutoTokenizer, Aimv2Model
  625  

  659          >>> import torch
  660:         >>> from transformers import AutoProcessor, Aimv2Model
  661:         >>> from transformers.image_utils import load_image
  662  

  697          >>> import requests
  698:         >>> from transformers import AutoProcessor, Aimv2Model
  699  

train_real_world/transformers_4573/src/transformers/models/aimv2/modular_aimv2.py:
   87      ```python
   88:     >>> from transformers import SiglipVisionConfig, SiglipVisionModel
   89  

  258      ```python
  259:     >>> from transformers import Aimv2Config, Aimv2Model
  260  

  270      >>> # We can also initialize a Aimv2Config from a Aimv2TextConfig and a Aimv2VisionConfig
  271:     >>> from transformers import Aimv2TextConfig, Aimv2VisionConfig
  272  

  508          >>> import requests
  509:         >>> from transformers import AutoProcessor, Siglip2VisionModel
  510  

  651          >>> import requests
  652:         >>> from transformers import AutoProcessor, Aimv2Model
  653  

train_real_world/transformers_4573/src/transformers/models/albert/configuration_albert.py:
  76      ```python
  77:     >>> from transformers import AlbertConfig, AlbertModel
  78  

train_real_world/transformers_4573/src/transformers/models/albert/modeling_albert.py:
  113  
  114: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  115  def eager_attention_forward(

  483          ```python
  484:         >>> from transformers import AutoTokenizer, AlbertForPreTraining
  485          >>> import torch

  609          >>> import torch
  610:         >>> from transformers import AutoTokenizer, AlbertForMaskedLM
  611  

train_real_world/transformers_4573/src/transformers/models/align/configuration_align.py:
   71      ```python
   72:     >>> from transformers import AlignTextConfig, AlignTextModel
   73  

  180      ```python
  181:     >>> from transformers import AlignVisionConfig, AlignVisionModel
  182  

  271      ```python
  272:     >>> from transformers import AlignConfig, AlignModel
  273  

  283      >>> # We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig
  284:     >>> from transformers import AlignTextConfig, AlignVisionConfig
  285  

train_real_world/transformers_4573/src/transformers/models/align/convert_align_tf_to_hf.py:
  27  
  28: from transformers import (
  29      AlignConfig,

  36  )
  37: from transformers.utils import logging
  38  

train_real_world/transformers_4573/src/transformers/models/align/modeling_align.py:
   126  
   127: # Copied from transformers.models.efficientnet.modeling_efficientnet.round_filters with EfficientNet->AlignVision
   128  def round_filters(config: AlignVisionConfig, num_channels: int):

   142  
   143: # Copied from transformers.models.efficientnet.modeling_efficientnet.correct_pad
   144  def correct_pad(kernel_size: Union[int, tuple], adjust: bool = True):

   163  
   164: # Copied from transformers.models.efficientnet.modeling_efficientnet.EfficientNetEmbeddings with EfficientNet->AlignVision
   165  class AlignVisionEmbeddings(nn.Module):

   189  
   190: # Copied from transformers.models.efficientnet.modeling_efficientnet.EfficientNetDepthwiseConv2d with EfficientNet->AlignVision
   191  class AlignVisionDepthwiseConv2d(nn.Conv2d):

   216  
   217: # Copied from transformers.models.efficientnet.modeling_efficientnet.EfficientNetExpansionLayer with EfficientNet->AlignVision
   218  class AlignVisionExpansionLayer(nn.Module):

   243  
   244: # Copied from transformers.models.efficientnet.modeling_efficientnet.EfficientNetDepthwiseLayer with EfficientNet->AlignVision
   245  class AlignVisionDepthwiseLayer(nn.Module):

   283  
   284: # Copied from transformers.models.efficientnet.modeling_efficientnet.EfficientNetSqueezeExciteLayer with EfficientNet->AlignVision
   285  class AlignVisionSqueezeExciteLayer(nn.Module):

   650  
   651: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->AlignText
   652  class AlignTextSelfOutput(nn.Module):

   689  
   690: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->AlignText
   691  class AlignTextIntermediate(nn.Module):

   705  
   706: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->AlignText
   707  class AlignTextOutput(nn.Module):

   803  
   804: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert -> AlignText
   805  class AlignTextPooler(nn.Module):

   905          ```python
   906:         >>> from transformers import AutoTokenizer, AlignTextModel
   907  

  1020          >>> import requests
  1021:         >>> from transformers import AutoProcessor, AlignVisionModel
  1022  

  1114          >>> import torch
  1115:         >>> from transformers import AutoTokenizer, AlignModel
  1116  

  1147          >>> import torch
  1148:         >>> from transformers import AutoProcessor, AlignModel
  1149:         >>> from transformers.image_utils import load_image
  1150  

  1188          >>> import torch
  1189:         >>> from transformers import AutoProcessor, AlignModel
  1190:         >>> from transformers.image_utils import load_image
  1191  

train_real_world/transformers_4573/src/transformers/models/align/processing_align.py:
  39          ```python
  40:         from transformers import AlignProcessor
  41          from PIL import Image

train_real_world/transformers_4573/src/transformers/models/altclip/configuration_altclip.py:
   78      ```python
   79:     >>> from transformers import AltCLIPTextModel, AltCLIPTextConfig
   80  

  177      ```python
  178:     >>> from transformers import AltCLIPVisionConfig, AltCLIPVisionModel
  179  

  251      ```python
  252:     >>> from transformers import AltCLIPConfig, AltCLIPModel
  253  

train_real_world/transformers_4573/src/transformers/models/altclip/modeling_altclip.py:
    56  @auto_docstring
    57: # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->AltCLIP
    58  class AltCLIPOutput(ModelOutput):

    92  
    93: # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->AltRoberta
    94  class AltRobertaEmbeddings(nn.Module):

   257  
   258: # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput
   259  class AltRobertaSelfOutput(nn.Module):

   299  
   300: # Copied from transformers.models.roberta.modeling_roberta.RobertaIntermediate with Roberta->AltRoberta
   301  class AltRobertaIntermediate(nn.Module):

   315  
   316: # Copied from transformers.models.roberta.modeling_roberta.RobertaOutput
   317  class AltRobertaOutput(nn.Module):

   330  
   331: # Copied from transformers.models.align.modeling_align.AlignTextLayer with AlignText->AltRoberta
   332  class AltRobertaLayer(GradientCheckpointingLayer):

   369  
   370: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with AlignText->AltRoberta
   371  class AltRobertaEncoder(nn.Module):

   415  
   416: # Copied from transformers.models.roberta.modeling_roberta.RobertaPooler
   417  class AltRobertaPooler(nn.Module):

   431  
   432: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   433  def eager_attention_forward(

   528  
   529: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->AltCLIP
   530  class AltCLIPMLP(nn.Module):

   681  
   682: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->AltCLIP
   683  class AltCLIPVisionEmbeddings(nn.Module):

   905          >>> import requests
   906:         >>> from transformers import AutoProcessor, AltCLIPVisionModel
   907  

   942  
   943:     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->AltRoberta
   944      def __init__(self, config, add_pooling_layer=True):

   966      @auto_docstring
   967:     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward
   968      def forward(

  1075          ```python
  1076:         >>> from transformers import AutoProcessor, AltCLIPTextModel
  1077  

  1174          >>> import torch
  1175:         >>> from transformers import AutoProcessor, AltCLIPModel
  1176  

  1210          >>> import torch
  1211:         >>> from transformers import AutoProcessor, AltCLIPModel
  1212:         >>> from transformers.image_utils import load_image
  1213  

  1256          >>> import requests
  1257:         >>> from transformers import AutoProcessor, AltCLIPModel
  1258  

train_real_world/transformers_4573/src/transformers/models/apertus/configuration_apertus.py:
  87      ```python
  88:     >>> from transformers import ApertusModel, ApertusConfig
  89  

train_real_world/transformers_4573/src/transformers/models/apertus/modeling_apertus.py:
  473          ```python
  474:         >>> from transformers import AutoTokenizer, ApertusForCausalLM
  475  

train_real_world/transformers_4573/src/transformers/models/apertus/modular_apertus.py:
  106      ```python
  107:     >>> from transformers import ApertusModel, ApertusConfig
  108  

  322          ```python
  323:         >>> from transformers import AutoTokenizer, ApertusForCausalLM
  324  

train_real_world/transformers_4573/src/transformers/models/arcee/configuration_arcee.py:
  93      ```python
  94:     >>> from transformers import ArceeModel, ArceeConfig
  95  

train_real_world/transformers_4573/src/transformers/models/arcee/modeling_arcee.py:
   27  
   28: from transformers.utils import auto_docstring
   29  

  471          ```python
  472:         >>> from transformers import AutoTokenizer, ArceeForCausalLM
  473  

train_real_world/transformers_4573/src/transformers/models/arcee/modular_arcee.py:
   18  
   19: from transformers.utils import auto_docstring, logging
   20  

  100      ```python
  101:     >>> from transformers import ArceeModel, ArceeConfig
  102  

train_real_world/transformers_4573/src/transformers/models/aria/convert_aria_weights_to_hf.py:
  20  
  21: from transformers import (
  22      AddedToken,

train_real_world/transformers_4573/src/transformers/models/aria/modeling_aria.py:
   799          ```python
   800:         >>> from transformers import AutoTokenizer, AriaTextForCausalLM
   801  

  1118  
  1119:         >>> from transformers import AutoProcessor, AutoModel
  1120:         >>> from transformers.image_utils import load_image
  1121  

train_real_world/transformers_4573/src/transformers/models/aria/modular_aria.py:
  1415  
  1416:         >>> from transformers import AutoProcessor, AutoModel
  1417:         >>> from transformers.image_utils import load_image
  1418  

train_real_world/transformers_4573/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py:
  72      ```python
  73:     >>> from transformers import ASTConfig, ASTModel
  74  

train_real_world/transformers_4573/src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py:
  25  
  26: from transformers import ASTConfig, ASTFeatureExtractor, ASTForAudioClassification
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:
   99  
  100: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  101  def eager_attention_forward(

  129  
  130: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->AST
  131  class ASTSelfAttention(nn.Module):

  180  
  181: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->AST
  182  class ASTSelfOutput(nn.Module):

  198  
  199: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->AST
  200  class ASTAttention(nn.Module):

  211  
  212: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->AST
  213  class ASTIntermediate(nn.Module):

  227  
  228: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViT->AST
  229  class ASTOutput(nn.Module):

  241  
  242: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->AST,VIT->AST
  243  class ASTLayer(GradientCheckpointingLayer):

  272  
  273: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->AST
  274  class ASTEncoder(nn.Module):

train_real_world/transformers_4573/src/transformers/models/audioflamingo3/configuration_audioflamingo3.py:
   70      ```python
   71:     >>> from transformers import AudioFlamingo3EncoderConfig, AudioFlamingo3Encoder
   72  

  153      ```python
  154:     >>> from transformers import AudioFlamingo3ForConditionalGeneration, AudioFlamingo3Config, AudioFlamingo3EncoderConfig, Qwen2Config
  155  

train_real_world/transformers_4573/src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py:
  30  
  31: from transformers import (
  32      AudioFlamingo3Config,

train_real_world/transformers_4573/src/transformers/models/audioflamingo3/modeling_audioflamingo3.py:
  506          ```python
  507:         >>> from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  508  

train_real_world/transformers_4573/src/transformers/models/audioflamingo3/modular_audioflamingo3.py:
  206          ```python
  207:         >>> from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor
  208  

train_real_world/transformers_4573/src/transformers/models/auto/auto_factory.py:
   76          ```python
   77:         >>> from transformers import AutoConfig, BaseAutoModelClass
   78  

  164          ```python
  165:         >>> from transformers import AutoConfig, BaseAutoModelClass
  166  

train_real_world/transformers_4573/src/transformers/models/auto/configuration_auto.py:
  1293          ```python
  1294:         >>> from transformers import AutoConfig
  1295  

train_real_world/transformers_4573/src/transformers/models/auto/feature_extraction_auto.py:
  174      # Save a pretrained feature extractor locally and you can reload its config
  175:     from transformers import AutoFeatureExtractor
  176  

  301          ```python
  302:         >>> from transformers import AutoFeatureExtractor
  303  

train_real_world/transformers_4573/src/transformers/models/auto/image_processing_auto.py:
  318      # Save a pretrained image processor locally and you can reload its config
  319:     from transformers import AutoImageProcessor
  320  

  460          ```python
  461:         >>> from transformers import AutoImageProcessor
  462  

train_real_world/transformers_4573/src/transformers/models/auto/processing_auto.py:
  265          ```python
  266:         >>> from transformers import AutoProcessor
  267  

train_real_world/transformers_4573/src/transformers/models/auto/tokenization_auto.py:
   22  
   23: from transformers.utils.import_utils import is_mistral_common_available
   24  

  504      # Save a pretrained tokenizer locally and you can reload its config
  505:     from transformers import AutoTokenizer
  506  

  613          ```python
  614:         >>> from transformers import AutoTokenizer
  615  

train_real_world/transformers_4573/src/transformers/models/auto/video_processing_auto.py:
  177      # Save a pretrained video processor locally and you can reload its config
  178:     from transformers import AutoVideoProcessor
  179  

  315          ```python
  316:         >>> from transformers import AutoVideoProcessor
  317  

train_real_world/transformers_4573/src/transformers/models/autoformer/configuration_autoformer.py:
  118      ```python
  119:     >>> from transformers import AutoformerConfig, AutoformerModel
  120  

train_real_world/transformers_4573/src/transformers/models/autoformer/modeling_autoformer.py:
   129  
   130: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesFeatureEmbedder with TimeSeries->Autoformer
   131  class AutoformerFeatureEmbedder(nn.Module):

   164  
   165: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer
   166  class AutoformerStdScaler(nn.Module):

   200  
   201: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer
   202  class AutoformerMeanScaler(nn.Module):

   255  
   256: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer
   257  class AutoformerNOPScaler(nn.Module):

   283  
   284: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average
   285  def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:

   308  
   309: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll
   310  def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:

   316  
   317: # Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->Autoformer
   318  class AutoformerSinusoidalPositionalEmbedding(nn.Embedding):

   351  
   352: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Autoformer
   353  class AutoformerValueEmbedding(nn.Module):

   847  
   848:     # copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask
   849      def _update_full_mask(

   869  
   870: # copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerEncoder with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer
   871  class AutoformerEncoder(AutoformerPreTrainedModel):

  1436          >>> import torch
  1437:         >>> from transformers import AutoformerModel
  1438  

  1695          >>> import torch
  1696:         >>> from transformers import AutoformerForPrediction
  1697  

  1741          >>> import torch
  1742:         >>> from transformers import AutoformerConfig, AutoformerForPrediction
  1743  

train_real_world/transformers_4573/src/transformers/models/aya_vision/modeling_aya_vision.py:
  394          ```python
  395:         >>> from transformers import AutoProcessor, AyaVisionForConditionalGeneration
  396          >>> import torch

train_real_world/transformers_4573/src/transformers/models/aya_vision/modular_aya_vision.py:
   21  
   22: from transformers.models.llava.modeling_llava import (
   23      LlavaCausalLMOutputWithPast,

  252          ```python
  253:         >>> from transformers import AutoProcessor, AyaVisionForConditionalGeneration
  254          >>> import torch

train_real_world/transformers_4573/src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py:
   27  
   28: from transformers import AutoTokenizer
   29: from transformers.utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME
   30  

   85  
   86: # Adapted from transformers.models.mamba.convert_mamba_ssm_checkpoint_to_pytorch.py
   87  def convert_ssm_config_to_hf_config(

  169  
  170: # Adapted from transformers.models.mamba.convert_mamba_ssm_checkpoint_to_pytorch.py
  171  def convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(

train_real_world/transformers_4573/src/transformers/models/bamba/modeling_bamba.py:
    32  
    33: from transformers.activations import ACT2FN
    34  

   295  
   296: # Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb
   297  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   491  
   492: # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer
   493  class BambaMixer(nn.Module):

  1428          ```python
  1429:         >>> from transformers import AutoTokenizer, BambaForCausalLM
  1430  

train_real_world/transformers_4573/src/transformers/models/bamba/modular_bamba.py:
    26  
    27: from transformers.activations import ACT2FN
    28: from transformers.models.jamba.modeling_jamba import HybridMambaAttentionDynamicCache, JambaAttentionDecoderLayer
    29: from transformers.models.llama.modeling_llama import (
    30      LlamaAttention,

    36  )
    37: from transformers.models.mamba2.modeling_mamba2 import (
    38      MambaRMSNormGated,

    86  
    87: # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer
    88  class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache):

   144  
   145: # Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb
   146  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   193  
   194: # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer
   195  class BambaMixer(nn.Module):

  1091          ```python
  1092:         >>> from transformers import AutoTokenizer, BambaForCausalLM
  1093  

train_real_world/transformers_4573/src/transformers/models/bark/configuration_bark.py:
  108      ```python
  109:     >>> from transformers import BarkSemanticConfig, BarkSemanticModel
  110  

  131      ```python
  132:     >>> from transformers import BarkCoarseConfig, BarkCoarseModel
  133  

  159      ```python
  160:     >>> from transformers import BarkFineConfig, BarkFineModel
  161  

  206      ```python
  207:     >>> from transformers import (
  208      ...     BarkSemanticConfig,

train_real_world/transformers_4573/src/transformers/models/bark/convert_suno_to_hf.py:
  10  
  11: from transformers import EncodecConfig, EncodecModel, set_seed
  12: from transformers.models.bark.configuration_bark import (
  13      BarkCoarseConfig,

  17  )
  18: from transformers.models.bark.generation_configuration_bark import (
  19      BarkCoarseGenerationConfig,

  23  )
  24: from transformers.models.bark.modeling_bark import BarkCoarseModel, BarkFineModel, BarkModel, BarkSemanticModel
  25: from transformers.utils import logging
  26  

train_real_world/transformers_4573/src/transformers/models/bark/modeling_bark.py:
   102  
   103:     # Copied from transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoSelfAttention._split_heads
   104      def _split_heads(self, tensor, num_heads, attn_head_size):

  1457          ```python
  1458:         >>> from transformers import AutoProcessor, BarkModel
  1459  

train_real_world/transformers_4573/src/transformers/models/bart/configuration_bart.py:
  84      ```python
  85:     >>> from transformers import BartConfig, BartModel
  86  

train_real_world/transformers_4573/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py:
  25  
  26: from transformers import (
  27      BartConfig,

  32  )
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/bart/modeling_bart.py:
   111  
   112: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   113  def eager_attention_forward(

  1110          ```python
  1111:         >>> from transformers import AutoTokenizer, BartForConditionalGeneration
  1112  

  1131          ```python
  1132:         >>> from transformers import AutoTokenizer, BartForConditionalGeneration
  1133  

  1531          ```python
  1532:         >>> from transformers import AutoTokenizer, BartForCausalLM
  1533  

train_real_world/transformers_4573/src/transformers/models/beit/configuration_beit.py:
  107      ```python
  108:     >>> from transformers import BeitConfig, BeitModel
  109  

train_real_world/transformers_4573/src/transformers/models/beit/convert_beit_unilm_to_pytorch.py:
  26  
  27: from transformers import (
  28      BeitConfig,

  33  )
  34: from transformers.image_utils import PILImageResampling
  35: from transformers.utils import logging
  36  

train_real_world/transformers_4573/src/transformers/models/beit/modeling_beit.py:
   120  
   121:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   122      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   834          ```python
   835:         >>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling
   836          >>> import torch

  1259          ```python
  1260:         >>> from transformers import AutoImageProcessor, BeitForSemanticSegmentation
  1261          >>> from PIL import Image

  1383          ```python
  1384:         >>> from transformers import AutoImageProcessor, AutoBackbone
  1385          >>> import torch

train_real_world/transformers_4573/src/transformers/models/bert/configuration_bert.py:
  74      ```python
  75:     >>> from transformers import BertConfig, BertModel
  76  

train_real_world/transformers_4573/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py:
  20  
  21: from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert
  22: from transformers.utils import logging
  23  

train_real_world/transformers_4573/src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py:
  34  
  35: from transformers import BertConfig, BertModel
  36: from transformers.utils import logging
  37  

train_real_world/transformers_4573/src/transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py:
  26  
  27: from transformers import BertConfig, BertForMaskedLM
  28: from transformers.models.bert.modeling_bert import (
  29      BertIntermediate,

  35  )
  36: from transformers.utils import logging
  37  

train_real_world/transformers_4573/src/transformers/models/bert/modeling_bert.py:
   809          ```python
   810:         >>> from transformers import AutoTokenizer, BertForPreTraining
   811          >>> import torch

  1081          ```python
  1082:         >>> from transformers import AutoTokenizer, BertForNextSentencePrediction
  1083          >>> import torch

train_real_world/transformers_4573/src/transformers/models/bert_generation/configuration_bert_generation.py:
  69      ```python
  70:     >>> from transformers import BertGenerationConfig, BertGenerationEncoder
  71  

train_real_world/transformers_4573/src/transformers/models/bert_generation/modeling_bert_generation.py:
   44  
   45: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BertGeneration
   46  class BertGenerationSelfOutput(nn.Module):

   59  
   60: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   61  def eager_attention_forward(

   89  
   90: # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->BertGeneration
   91  class BertGenerationSelfAttention(nn.Module):

  163  
  164: # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->BertGeneration
  165  class BertGenerationCrossAttention(nn.Module):

  241  
  242: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BertGeneration,BERT->BERT_GENERATION
  243  class BertGenerationAttention(nn.Module):

  273  
  274: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->BertGeneration
  275  class BertGenerationIntermediate(nn.Module):

  289  
  290: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->BertGeneration
  291  class BertGenerationOutput(nn.Module):

  304  
  305: # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->BertGeneration
  306  class BertGenerationLayer(GradientCheckpointingLayer):

  372  
  373: # Copied from transformers.models.bert.modeling_bert.BertEncoder
  374  class BertEncoder(nn.Module):

  584  
  585:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
  586      def _create_attention_masks(

  687          ```python
  688:         >>> from transformers import AutoTokenizer, BertGenerationDecoder, BertGenerationConfig
  689          >>> import torch

train_real_world/transformers_4573/src/transformers/models/big_bird/configuration_big_bird.py:
  85      ```python
  86:     >>> from transformers import BigBirdConfig, BigBirdModel
  87  

train_real_world/transformers_4573/src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py:
  22  
  23: from transformers import BigBirdConfig, BigBirdForPreTraining, BigBirdForQuestionAnswering
  24: from transformers.utils import logging
  25  

train_real_world/transformers_4573/src/transformers/models/big_bird/modeling_big_bird.py:
    70  
    71:     # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__
    72      def __init__(self, config):

  1113  
  1114: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BigBird
  1115  class BigBirdSelfOutput(nn.Module):

  1215  
  1216: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->BigBird
  1217  class BigBirdIntermediate(nn.Module):

  1231  
  1232: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->BigBird
  1233  class BigBirdOutput(nn.Module):

  1441  
  1442: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->BigBird
  1443  class BigBirdPredictionHeadTransform(nn.Module):

  1459  
  1460: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->BigBird
  1461  class BigBirdLMPredictionHead(nn.Module):

  1476  
  1477: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->BigBird
  1478  class BigBirdOnlyMLMHead(nn.Module):

  1487  
  1488: # Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->BigBird
  1489  class BigBirdOnlyNSPHead(nn.Module):

  1498  
  1499: # Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->BigBird
  1500  class BigBirdPreTrainingHeads(nn.Module):

  1941          ```python
  1942:         >>> from transformers import AutoTokenizer, BigBirdForPreTraining
  1943          >>> import torch

  2046          >>> import torch
  2047:         >>> from transformers import AutoTokenizer, BigBirdForMaskedLM
  2048          >>> from datasets import load_dataset

  2296          >>> import torch
  2297:         >>> from transformers import AutoTokenizer, BigBirdForSequenceClassification
  2298          >>> from datasets import load_dataset

  2611          >>> import torch
  2612:         >>> from transformers import AutoTokenizer, BigBirdForQuestionAnswering
  2613          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py:
  92      ```python
  93:     >>> from transformers import BigBirdPegasusConfig, BigBirdPegasusModel
  94  

train_real_world/transformers_4573/src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py:
  21  
  22: from transformers import BigBirdPegasusConfig, BigBirdPegasusForConditionalGeneration
  23  

train_real_world/transformers_4573/src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:
    90  
    91: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->BigBirdPegasus
    92  class BigBirdPegasusScaledWordEmbedding(nn.Embedding):

   104  
   105: # Copied from transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention with BigBird->BigBirdPegasus
   106  class BigBirdPegasusSelfAttention(nn.Module):

   194  
   195: # Copied from transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention with BigBird->BigBirdPegasus
   196  class BigBirdPegasusBlockSparseAttention(nn.Module):

  1186  
  1187: # Copied from transformers.models.bart.modeling_bart.BartAttention with BartConfig->BigBirdPegasusConfig, Bart->BigBirdPegasusDecoder
  1188  class BigBirdPegasusDecoderAttention(nn.Module):

  1429  
  1430:     # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward
  1431      def forward(

  1505  
  1506: # Copied from transformers.models.bart.modeling_bart.BartClassificationHead with Bart->BigBirdPegasus
  1507  class BigBirdPegasusClassificationHead(nn.Module):

  1766  
  1767:     @staticmethod  # Copied from transformers.models.big_bird.modeling_big_bird.BigBirdModel.create_masks_for_block_sparse_attn
  1768      def create_masks_for_block_sparse_attn(attention_mask: torch.Tensor, block_size: int):

  2200  
  2201:     # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.__init__ with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS
  2202      def __init__(self, config: BigBirdPegasusConfig):

  2210  
  2211:     # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.resize_token_embeddings with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS
  2212      def resize_token_embeddings(

  2218  
  2219:     # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration._resize_final_logits_bias with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS
  2220      def _resize_final_logits_bias(self, new_num_tokens: int) -> None:

  2266          ```python
  2267:         >>> from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration
  2268  

  2339  
  2340:     # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.prepare_decoder_input_ids_from_labels with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS
  2341      def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):

  2580  
  2581: # Copied from transformers.models.pegasus.modeling_pegasus.PegasusDecoderWrapper with Pegasus->BigBirdPegasus
  2582  class BigBirdPegasusDecoderWrapper(BigBirdPegasusPreTrainedModel):

  2641          ```python
  2642:         >>> from transformers import AutoTokenizer, BigBirdPegasusForCausalLM
  2643  

train_real_world/transformers_4573/src/transformers/models/biogpt/configuration_biogpt.py:
  79      ```python
  80:     >>> from transformers import BioGptModel, BioGptConfig
  81  

train_real_world/transformers_4573/src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import BioGptConfig, BioGptForCausalLM
  26: from transformers.models.biogpt.tokenization_biogpt import VOCAB_FILES_NAMES
  27: from transformers.tokenization_utils_base import TOKENIZER_CONFIG_FILE
  28: from transformers.utils import WEIGHTS_NAME, logging
  29  

train_real_world/transformers_4573/src/transformers/models/bit/configuration_bit.py:
  73      ```python
  74:     >>> from transformers import BitConfig, BitModel
  75  

train_real_world/transformers_4573/src/transformers/models/bit/convert_bit_to_pytorch.py:
  28  
  29: from transformers import BitConfig, BitForImageClassification, BitImageProcessor
  30: from transformers.image_utils import PILImageResampling
  31: from transformers.utils import logging
  32  

train_real_world/transformers_4573/src/transformers/models/bit/image_processing_bit.py:
  124  
  125:     # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize
  126      def resize(

train_real_world/transformers_4573/src/transformers/models/bit/modeling_bit.py:
  276  
  277: # Copied from transformers.models.convnext.modeling_convnext.drop_path
  278  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  292  
  293: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Bit
  294  class BitDropPath(nn.Module):

  788          ```python
  789:         >>> from transformers import AutoImageProcessor, AutoBackbone
  790          >>> import torch

train_real_world/transformers_4573/src/transformers/models/bitnet/configuration_bitnet.py:
  85      ```python
  86:     >>> from transformers import BitNetModel, BitNetConfig
  87  

train_real_world/transformers_4573/src/transformers/models/bitnet/modeling_bitnet.py:
  475          ```python
  476:         >>> from transformers import AutoTokenizer, BitNetForCausalLM
  477  

train_real_world/transformers_4573/src/transformers/models/bitnet/modular_bitnet.py:
  133          ```python
  134:         >>> from transformers import AutoTokenizer, BitNetForCausalLM
  135  

train_real_world/transformers_4573/src/transformers/models/blenderbot/configuration_blenderbot.py:
  83      ```python
  84:     >>> from transformers import BlenderbotConfig, BlenderbotModel
  85  

train_real_world/transformers_4573/src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py:
  20  
  21: from transformers import BlenderbotConfig, BlenderbotForConditionalGeneration
  22: from transformers.utils import logging
  23  

train_real_world/transformers_4573/src/transformers/models/blenderbot/modeling_blenderbot.py:
    55  
    56: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    57  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    92  
    93: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->Blenderbot
    94  class BlenderbotScaledWordEmbedding(nn.Embedding):

   106  
   107: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   108  def eager_attention_forward(

   136  
   137: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Blenderbot
   138  class BlenderbotAttention(nn.Module):

   262  
   263: # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Blenderbot, MBART->BLENDERBOT
   264  class BlenderbotEncoderLayer(GradientCheckpointingLayer):

   322  
   323: # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Blenderbot, MBART->BLENDERBOT
   324  class BlenderbotDecoderLayer(GradientCheckpointingLayer):

   916          ```python
   917:         >>> from transformers import AutoTokenizer, BlenderbotModel
   918  

  1075          ```python
  1076:         >>> from transformers import AutoTokenizer, BlenderbotForConditionalGeneration
  1077  

  1154  
  1155: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Blenderbot
  1156  class BlenderbotDecoderWrapper(BlenderbotPreTrainedModel):

  1170  
  1171: # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Blenderbot, facebook/bart-base->facebook/blenderbot-400M-distill
  1172  class BlenderbotForCausalLM(BlenderbotPreTrainedModel, GenerationMixin):

  1220          ```python
  1221:         >>> from transformers import AutoTokenizer, BlenderbotForCausalLM
  1222  

train_real_world/transformers_4573/src/transformers/models/blenderbot/tokenization_blenderbot.py:
  43      ```python
  44:     >>> from transformers import BlenderbotTokenizerFast
  45  

train_real_world/transformers_4573/src/transformers/models/blenderbot_small/configuration_blenderbot_small.py:
  83      ```python
  84:     >>> from transformers import BlenderbotSmallConfig, BlenderbotSmallModel
  85  

train_real_world/transformers_4573/src/transformers/models/blenderbot_small/modeling_blenderbot_small.py:
    52  
    53: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    54  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    69  
    70: # Copied from transformers.models.blenderbot.modeling_blenderbot.BlenderbotLearnedPositionalEmbedding with Blenderbot->BlenderbotSmall
    71  class BlenderbotSmallLearnedPositionalEmbedding(nn.Embedding):

    90  
    91: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
    92  def eager_attention_forward(

   120  
   121: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->BlenderbotSmall
   122  class BlenderbotSmallAttention(nn.Module):

   246  
   247: # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL
   248  class BlenderbotSmallEncoderLayer(GradientCheckpointingLayer):

   314  
   315: # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL
   316  class BlenderbotSmallDecoderLayer(GradientCheckpointingLayer):

   889          ```python
   890:         >>> from transformers import AutoTokenizer, BlenderbotSmallModel
   891  

  1035          ```python
  1036:         >>> from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration
  1037  

  1114  
  1115: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->BlenderbotSmall
  1116  class BlenderbotSmallDecoderWrapper(BlenderbotSmallPreTrainedModel):

  1130  
  1131: # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->BlenderbotSmall, facebook/bart-base->facebook/blenderbot_small-90M
  1132  class BlenderbotSmallForCausalLM(BlenderbotSmallPreTrainedModel, GenerationMixin):

  1180          ```python
  1181:         >>> from transformers import AutoTokenizer, BlenderbotSmallForCausalLM
  1182  

train_real_world/transformers_4573/src/transformers/models/blip/configuration_blip.py:
   82      ```python
   83:     >>> from transformers import BlipTextConfig, BlipTextModel
   84  

  184      ```python
  185:     >>> from transformers import BlipVisionConfig, BlipVisionModel
  186  

  260      ```python
  261:     >>> from transformers import BlipConfig, BlipModel
  262  

train_real_world/transformers_4573/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py:
  29  
  30: from transformers import (
  31      BertTokenizer,

train_real_world/transformers_4573/src/transformers/models/blip/image_processing_blip.py:
  109  
  110:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC
  111      def resize(

train_real_world/transformers_4573/src/transformers/models/blip/modeling_blip_text.py:
  215  
  216: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert -> BlipText
  217  class BlipTextSelfOutput(nn.Module):

  259  
  260: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert -> BlipText
  261  class BlipTextIntermediate(nn.Module):

  275  
  276: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert -> BlipText
  277  class BlipTextOutput(nn.Module):

  434  
  435: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->BlipText
  436  class BlipTextPooler(nn.Module):

  450  
  451: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->BlipText
  452  class BlipTextPredictionHeadTransform(nn.Module):

  468  
  469: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->BlipText
  470  class BlipTextLMPredictionHead(nn.Module):

  485  
  486: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->BlipText
  487  class BlipTextOnlyMLMHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/blip/modeling_blip.py:
    40  
    41: # Copied from transformers.models.clip.modeling_clip.contrastive_loss
    42  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

    45  
    46: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->blip
    47  def blip_loss(similarity: torch.Tensor) -> torch.Tensor:

   262  
   263: # Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->Blip
   264  class BlipTextEmbeddings(nn.Module):

   365  
   366: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Blip
   367  class BlipMLP(nn.Module):

   586          ```python
   587:         >>> from transformers import AutoProcessor, BlipModel
   588  

   621          >>> import requests
   622:         >>> from transformers import AutoProcessor, BlipModel
   623  

   661          >>> import requests
   662:         >>> from transformers import AutoProcessor, BlipModel
   663  

   714          >>> import requests
   715:         >>> from transformers import AutoProcessor, BlipModel
   716  

   827          >>> import requests
   828:         >>> from transformers import AutoProcessor, BlipForConditionalGeneration
   829  

   893          >>> import requests
   894:         >>> from transformers import AutoProcessor, BlipForConditionalGeneration
   895  

   998          >>> import requests
   999:         >>> from transformers import AutoProcessor, BlipForQuestionAnswering
  1000  

  1105          >>> import requests
  1106:         >>> from transformers import AutoProcessor, BlipForQuestionAnswering
  1107  

  1229          >>> import requests
  1230:         >>> from transformers import AutoProcessor, BlipForImageTextRetrieval
  1231  

train_real_world/transformers_4573/src/transformers/models/blip_2/configuration_blip_2.py:
   62      ```python
   63:     >>> from transformers import Blip2VisionConfig, Blip2VisionModel
   64  

  156      ```python
  157:     >>> from transformers import Blip2QFormerConfig, Blip2QFormerModel
  158  

  237      ```python
  238:     >>> from transformers import (
  239      ...     Blip2VisionConfig,

train_real_world/transformers_4573/src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py:
  31  
  32: from transformers import (
  33      AutoTokenizer,

  45  )
  46: from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  47  

train_real_world/transformers_4573/src/transformers/models/blip_2/modeling_blip_2.py:
   135  )
   136: # Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Blip2
   137  class Blip2TextModelOutput(ModelOutput):

   154  )
   155: # Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Blip2
   156  class Blip2VisionModelOutput(ModelOutput):

   167  
   168: # Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->Blip2
   169  class Blip2VisionEmbeddings(nn.Module):

   242  
   243: # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> BLIP doesn't cast attn weights to fp32
   244  def eager_attention_forward(

   341  
   342: # Copied from transformers.models.blip.modeling_blip.BlipMLP
   343  class Blip2MLP(nn.Module):

   357  
   358: # Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->Blip2
   359  class Blip2EncoderLayer(GradientCheckpointingLayer):

   434  
   435: # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2
   436  class Blip2Encoder(nn.Module):

   468  @auto_docstring
   469: # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->Blip2, BLIP->BLIP_2
   470  class Blip2VisionModel(Blip2PreTrainedModel):

   621  
   622: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Blip2QFormer
   623  class Blip2QFormerSelfOutput(nn.Module):

   661  
   662: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Blip2QFormer
   663  class Blip2QFormerIntermediate(nn.Module):

   677  
   678: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Blip2QFormer
   679  class Blip2QFormerOutput(nn.Module):

  1105          >>> import torch
  1106:         >>> from transformers import AutoTokenizer, Blip2Model
  1107  

  1149          >>> import torch
  1150:         >>> from transformers import AutoProcessor, Blip2Model
  1151:         >>> from transformers.image_utils import load_image
  1152  

  1186          >>> import torch
  1187:         >>> from transformers import AutoProcessor, Blip2Model
  1188:         >>> from transformers.image_utils import load_image
  1189  

  1260          >>> import requests
  1261:         >>> from transformers import Blip2Processor, Blip2Model
  1262          >>> import torch

  1398          >>> import torch
  1399:         >>> from transformers import AutoProcessor, Blip2TextModelWithProjection
  1400  

  1480          >>> import torch
  1481:         >>> from transformers import AutoProcessor, Blip2VisionModelWithProjection
  1482:         >>> from transformers.image_utils import load_image
  1483  

  1705          >>> import requests
  1706:         >>> from transformers import Blip2Processor, Blip2ForConditionalGeneration
  1707          >>> import torch

  1971          >>> import requests
  1972:         >>> from transformers import AutoProcessor, Blip2ForImageTextRetrieval
  1973  

train_real_world/transformers_4573/src/transformers/models/bloom/configuration_bloom.py:
  75      ```python
  76:     >>> from transformers import BloomConfig, BloomModel
  77  

train_real_world/transformers_4573/src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py:
  23  
  24: from transformers import BloomConfig, BloomModel
  25: from transformers.file_utils import CONFIG_NAME, WEIGHTS_NAME
  26: from transformers.utils import logging
  27  

train_real_world/transformers_4573/src/transformers/models/bloom/modeling_bloom.py:
  562  
  563:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  564      def _update_causal_mask(

  632      @staticmethod
  633:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  634      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/blt/configuration_blt.py:
  291      ```python
  292:     >>> from transformers import BltModel, BltConfig
  293  

train_real_world/transformers_4573/src/transformers/models/blt/convert_blt_weights_to_hf.py:
  12  
  13: from transformers import PreTrainedTokenizerFast
  14: from transformers.convert_slow_tokenizer import bytes_to_unicode
  15: from transformers.utils import logging as transformers_logging
  16  

train_real_world/transformers_4573/src/transformers/models/blt/modeling_blt.py:
   153  
   154: # Modified from transformers.models.llama.modeling_llama.LlamaDecoderLayer
   155  class BltTransformerLayer(GradientCheckpointingLayer):

  1437          ```python
  1438:         >>> from transformers import AutoTokenizer, BltForCausalLM
  1439  

train_real_world/transformers_4573/src/transformers/models/bridgetower/configuration_bridgetower.py:
   57      ```python
   58:     >>> from transformers import BridgeTowerVisionConfig
   59  

  144      ```python
  145:     >>> from transformers import BridgeTowerTextConfig
  146  

  237      ```python
  238:     >>> from transformers import BridgeTowerModel, BridgeTowerConfig
  239  

train_real_world/transformers_4573/src/transformers/models/bridgetower/image_processing_bridgetower.py:
   47  
   48: # Copied from transformers.models.vilt.image_processing_vilt.max_across_indices
   49  def max_across_indices(values: Iterable[Any]) -> list[Any]:

   55  
   56: # Copied from transformers.models.vilt.image_processing_vilt.make_pixel_mask
   57  def make_pixel_mask(

   74  
   75: # Copied from transformers.models.vilt.image_processing_vilt.get_max_height_width
   76  def get_max_height_width(

   93  
   94: # Copied from transformers.models.vilt.image_processing_vilt.get_resize_output_image_size
   95  def get_resize_output_image_size(

  211  
  212:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.resize
  213      def resize(

  292  
  293:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor._pad_image
  294      def _pad_image(

  320  
  321:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.pad
  322      def pad(

train_real_world/transformers_4573/src/transformers/models/bridgetower/modeling_bridgetower.py:
   176  
   177: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->BridgeTower
   178  class BridgeTowerVisionEmbeddings(nn.Module):

   343  
   344: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BridgeTower
   345  class BridgeTowerSelfOutput(nn.Module):

   358  
   359: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->BridgeTower
   360  class BridgeTowerIntermediate(nn.Module):

   374  
   375: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->BridgeTower
   376  class BridgeTowerOutput(nn.Module):

   389  
   390: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->BridgeTower
   391  class BridgeTowerPooler(nn.Module):

   405  
   406: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   407  def eager_attention_forward(

   435  
   436: # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->BridgeTower
   437  class BridgeTowerSelfAttention(nn.Module):

   509  
   510: # Copied from transformers.models.roberta.modeling_roberta.RobertaCrossAttention with Roberta->BridgeTower
   511  class BridgeTowerCrossAttention(nn.Module):

   587  
   588: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BridgeTower,BERT->BRIDGE_TOWER
   589  class BridgeTowerAttention(nn.Module):

   698  
   699:     # copied from transformers.models.bert.modeling_bert.BertLayer.forward
   700      def forward(

   751  
   752: # copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->BridgeTowerText
   753  class BridgeTowerTextEncoder(nn.Module):

   806  
   807: # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->BridgeTowerText
   808  class BridgeTowerTextEmbeddings(nn.Module):

  1017      # for now we skip the copies from bert but stay close to the original
  1018:     # copied from transformers.models.bert.modeling_bert.BertModel.forward
  1019      def forward(

  1111  
  1112:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
  1113      def _create_attention_masks(

  1252          ```python
  1253:         >>> from transformers import BridgeTowerProcessor, BridgeTowerModel
  1254          >>> from PIL import Image

  1458  
  1459: # Copied from transformers.models.vilt.modeling_vilt.ViltPredictionHeadTransform with Vilt->BridgeTower
  1460  class BridgeTowerPredictionHeadTransform(nn.Module):

  1553          ```python
  1554:         >>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
  1555          >>> from PIL import Image

  1653          ```python
  1654:         >>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
  1655          >>> import requests

  1765          ```python
  1766:         >>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
  1767          >>> import requests

train_real_world/transformers_4573/src/transformers/models/bros/configuration_bros.py:
  76      ```python
  77:     >>> from transformers import BrosConfig, BrosModel
  78  

train_real_world/transformers_4573/src/transformers/models/bros/convert_bros_to_pytorch.py:
  21  
  22: from transformers import BrosConfig, BrosModel, BrosProcessor
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/bros/modeling_bros.py:
  263  
  264: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Bros
  265  class BrosSelfOutput(nn.Module):

  306  
  307: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Bros
  308  class BrosIntermediate(nn.Module):

  462  
  463: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->Bros
  464  class BrosPooler(nn.Module):

  585          >>> import torch
  586:         >>> from transformers import BrosProcessor, BrosModel
  587  

  729          >>> import torch
  730:         >>> from transformers import BrosProcessor, BrosForTokenClassification
  731  

  854          >>> import torch
  855:         >>> from transformers import BrosProcessor, BrosSpadeEEForTokenClassification
  856  

  987          >>> import torch
  988:         >>> from transformers import BrosProcessor, BrosSpadeELForTokenClassification
  989  

train_real_world/transformers_4573/src/transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import T5Config, T5ForConditionalGeneration
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/camembert/configuration_camembert.py:
  74      ```python
  75:     >>> from transformers import CamembertConfig, CamembertModel
  76  

train_real_world/transformers_4573/src/transformers/models/camembert/modeling_camembert.py:
  1241          ```python
  1242:         >>> from transformers import AutoTokenizer, CamembertForCausalLM, AutoConfig
  1243          >>> import torch

train_real_world/transformers_4573/src/transformers/models/camembert/modular_camembert.py:
  474          ```python
  475:         >>> from transformers import AutoTokenizer, CamembertForCausalLM, AutoConfig
  476          >>> import torch

train_real_world/transformers_4573/src/transformers/models/canine/configuration_canine.py:
  81      ```python
  82:     >>> from transformers import CanineConfig, CanineModel
  83  

train_real_world/transformers_4573/src/transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import CanineConfig, CanineModel, CanineTokenizer
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/canine/modeling_canine.py:
  1212          ```python
  1213:         >>> from transformers import AutoTokenizer, CanineForTokenClassification
  1214          >>> import torch

train_real_world/transformers_4573/src/transformers/models/chameleon/configuration_chameleon.py:
  172      ```python
  173:     >>> from transformers import ChameleonModel, ChameleonConfig
  174  

train_real_world/transformers_4573/src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py:
  23  
  24: from transformers import (
  25      ChameleonConfig,

  32  try:
  33:     from transformers import LlamaTokenizerFast
  34  except ImportError:

  50  ```py
  51: from transformers import ChameleonForConditionalGeneration, LlamaTokenizerFast
  52  

train_real_world/transformers_4573/src/transformers/models/chameleon/image_processing_chameleon.py:
  116  
  117:     # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize
  118      def resize(

train_real_world/transformers_4573/src/transformers/models/chameleon/modeling_chameleon.py:
    47  
    48: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Chameleon
    49  class ChameleonRMSNorm(nn.Module):

    68  
    69: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Chameleon
    70  class ChameleonRotaryEmbedding(nn.Module):

   134  
   135: # Copied from transformers.models.llama.modeling_llama.rotate_half
   136  def rotate_half(x):

   142  
   143: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
   144  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   170  
   171: # Copied from transformers.models.llama.modeling_llama.LlamaMLP with Llama->Chameleon
   172  class ChameleonMLP(nn.Module):

   206  
   207: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   208  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   219  
   220: # Copied from transformers.models.llama.modeling_llama.eager_attention_forward
   221  def eager_attention_forward(

   342  
   343: # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with Llama->Chameleon, LLAMA->CHAMELEON
   344  class ChameleonDecoderLayer(GradientCheckpointingLayer):

  1056          ```python
  1057:         >>> from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
  1058          >>> import torch

train_real_world/transformers_4573/src/transformers/models/chinese_clip/configuration_chinese_clip.py:
   75      ```python
   76:     >>> from transformers import ChineseCLIPTextConfig, ChineseCLIPTextModel
   77  

  169      ```python
  170:     >>> from transformers import ChineseCLIPVisionConfig, ChineseCLIPVisionModel
  171  

  245      ```python
  246:     >>> from transformers import ChineseCLIPConfig, ChineseCLIPModel
  247  

train_real_world/transformers_4573/src/transformers/models/chinese_clip/convert_chinese_clip_original_pytorch_to_hf.py:
  19  
  20: from transformers import ChineseCLIPConfig, ChineseCLIPModel
  21  

train_real_world/transformers_4573/src/transformers/models/chinese_clip/modeling_chinese_clip.py:
    42  # https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html
    43: # Copied from transformers.models.clip.modeling_clip.contrastive_loss
    44  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

    92  
    93: # Copied from transformers.models.align.modeling_align.AlignTextEmbeddings with Align->ChineseCLIP
    94  class ChineseCLIPTextEmbeddings(nn.Module):

   153  
   154: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->ChineseCLIP
   155  class ChineseCLIPVisionEmbeddings(nn.Module):

   237  
   238: # Copied from transformers.models.align.modeling_align.eager_attention_forward
   239  def eager_attention_forward(

   261  
   262: # Copied from transformers.models.align.modeling_align.AlignTextSelfAttention with Align->ChineseCLIP
   263  class ChineseCLIPTextSelfAttention(nn.Module):

   318  
   319: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->ChineseCLIPText
   320  class ChineseCLIPTextSelfOutput(nn.Module):

   333  
   334: # Copied from transformers.models.align.modeling_align.AlignTextAttention with Align->ChineseCLIP
   335  class ChineseCLIPTextAttention(nn.Module):

   413  
   414: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->ChineseCLIPText
   415  class ChineseCLIPTextIntermediate(nn.Module):

   429  
   430: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->ChineseCLIPText
   431  class ChineseCLIPTextOutput(nn.Module):

   444  
   445: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->ChineseCLIPVision
   446  class ChineseCLIPVisionMLP(nn.Module):

   460  
   461: # Copied from transformers.models.align.modeling_align.AlignTextLayer with Align->ChineseCLIP
   462  class ChineseCLIPTextLayer(GradientCheckpointingLayer):

   542  
   543: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->ChineseCLIPText
   544  class ChineseCLIPTextPooler(nn.Module):

   617  
   618: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with Align->ChineseCLIP
   619  class ChineseCLIPTextEncoder(nn.Module):

   940          >>> import requests
   941:         >>> from transformers import CLIPProcessor, ChineseCLIPVisionModel
   942  

  1021          >>> import torch
  1022:         >>> from transformers import AutoTokenizer, ChineseCLIPModel
  1023  

  1059          >>> import torch
  1060:         >>> from transformers import AutoProcessor, ChineseCLIPModel
  1061:         >>> from transformers.image_utils import load_image
  1062  

  1108          >>> import torch
  1109:         >>> from transformers import AutoProcessor, ChineseCLIPModel
  1110:         >>> from transformers.image_utils import load_image
  1111  

train_real_world/transformers_4573/src/transformers/models/clap/configuration_clap.py:
   74      ```python
   75:     >>> from transformers import ClapTextConfig, ClapTextModel
   76  

  206      ```python
  207:     >>> from transformers import ClapAudioConfig, ClapAudioModel
  208  

  312      ```python
  313:     >>> from transformers import ClapConfig, ClapModel
  314  

  324      >>> # We can also initialize a ClapConfig from a ClapTextConfig and a ClapAudioConfig
  325:     >>> from transformers import ClapTextConfig, ClapAudioConfig
  326  

train_real_world/transformers_4573/src/transformers/models/clap/convert_clap_original_pytorch_to_hf.py:
  20  
  21: from transformers import AutoFeatureExtractor, ClapConfig, ClapModel
  22  

train_real_world/transformers_4573/src/transformers/models/clap/modeling_clap.py:
   114  )
   115: # Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Clap
   116  class ClapTextModelOutput(ModelOutput):

   147  @auto_docstring
   148: # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->Clap, vision->audio, Vision->Audio, image->audio
   149  class ClapOutput(ModelOutput):

   183  
   184: # Adapted from transformers.models.swin.modeling_swin.SwinDropPath
   185  class ClapDropPath(nn.Module):

   347  
   348: # Copied from transformers.models.swin.modeling_swin.SwinSelfAttention with Swin->ClapAudio
   349  class ClapAudioSelfAttention(nn.Module):

   441  
   442: # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->ClapAudio
   443  class ClapAudioSelfOutput(nn.Module):

   455  
   456: # Copied from transformers.models.swin.modeling_swin.SwinAttention with Swin->ClapAudio
   457  class ClapAudioAttention(nn.Module):

   474  
   475: # Copied from transformers.models.swin.modeling_swin.SwinIntermediate with Swin->ClapAudio
   476  class ClapAudioIntermediate(nn.Module):

   490  
   491: # Copied from transformers.models.swin.modeling_swin.SwinOutput with Swin->ClapAudio
   492  class ClapAudioOutput(nn.Module):

   503  
   504: # Copied from transformers.models.swin.modeling_swin.SwinLayer with SwinDropPath->ClapDropPath, Swin->ClapAudio
   505  class ClapAudioLayer(nn.Module):

   626  
   627: # Copied from transformers.models.swin.modeling_swin.SwinStage with Swin->ClapAudio
   628  class ClapAudioStage(GradientCheckpointingLayer):

   682  
   683: # Copied from transformers.models.swin.modeling_swin.SwinPatchMerging with Swin->ClapAudio
   684  class ClapAudioPatchMerging(nn.Module):

   947  
   948: # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->ClapText, persistent=False->persistent=True
   949  class ClapTextEmbeddings(nn.Module):

  1054  
  1055: # Copied from transformers.models.align.modeling_align.eager_attention_forward
  1056  def eager_attention_forward(

  1078  
  1079: # Copied from transformers.models.align.modeling_align.AlignTextSelfAttention with Align->Clap
  1080  class ClapTextSelfAttention(nn.Module):

  1135  
  1136: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  1137  class ClapTextSelfOutput(nn.Module):

  1150  
  1151: # Copied from transformers.models.align.modeling_align.AlignTextAttention with Align->Clap
  1152  class ClapTextAttention(nn.Module):

  1175  
  1176: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  1177  class ClapTextIntermediate(nn.Module):

  1191  
  1192: # Copied from transformers.models.bert.modeling_bert.BertOutput
  1193  class ClapTextOutput(nn.Module):

  1206  
  1207: # Copied from transformers.models.align.modeling_align.AlignTextLayer with Align->Clap
  1208  class ClapTextLayer(GradientCheckpointingLayer):

  1245  
  1246: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with Align->Clap
  1247  class ClapTextEncoder(nn.Module):

  1291  
  1292: # Copied from transformers.models.bert.modeling_bert.BertPooler
  1293  class ClapTextPooler(nn.Module):

  1379          >>> from datasets import load_dataset
  1380:         >>> from transformers import AutoProcessor, ClapAudioModel
  1381  

  1572          >>> import torch
  1573:         >>> from transformers import AutoTokenizer, ClapModel
  1574  

  1610          >>> import torch
  1611:         >>> from transformers import AutoFeatureExtractor, ClapModel
  1612  

  1654          >>> from datasets import load_dataset
  1655:         >>> from transformers import AutoProcessor, ClapModel
  1656  

  1761          ```python
  1762:         >>> from transformers import AutoTokenizer, ClapTextModelWithProjection
  1763  

  1830          >>> from datasets import load_dataset
  1831:         >>> from transformers import ClapAudioModelWithProjection, ClapProcessor
  1832  

train_real_world/transformers_4573/src/transformers/models/clip/configuration_clip.py:
   72      ```python
   73:     >>> from transformers import CLIPTextConfig, CLIPTextModel
   74  

  167      ```python
  168:     >>> from transformers import CLIPVisionConfig, CLIPVisionModel
  169  

  241      ```python
  242:     >>> from transformers import CLIPConfig, CLIPModel
  243  

  253      >>> # We can also initialize a CLIPConfig from a CLIPTextConfig and a CLIPVisionConfig
  254:     >>> from transformers import CLIPTextConfig, CLIPVisionConfig
  255  

train_real_world/transformers_4573/src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py:
  20  
  21: from transformers import CLIPConfig, CLIPModel
  22  

train_real_world/transformers_4573/src/transformers/models/clip/modeling_clip.py:
   630          ```python
   631:         >>> from transformers import AutoTokenizer, CLIPTextModel
   632  

   723          >>> import requests
   724:         >>> from transformers import AutoProcessor, CLIPVisionModel
   725  

   802          >>> import torch
   803:         >>> from transformers import AutoTokenizer, CLIPModel
   804  

   838          >>> import torch
   839:         >>> from transformers import AutoProcessor, CLIPModel
   840:         >>> from transformers.image_utils import load_image
   841  

   881          >>> import torch
   882:         >>> from transformers import AutoProcessor, CLIPModel
   883:         >>> from transformers.image_utils import load_image
   884  

   981          >>> import torch
   982:         >>> from transformers import AutoTokenizer, CLIPTextModelWithProjection
   983  

  1041          >>> import torch
  1042:         >>> from transformers import AutoProcessor, CLIPVisionModelWithProjection
  1043:         >>> from transformers.image_utils import load_image
  1044  

train_real_world/transformers_4573/src/transformers/models/clipseg/configuration_clipseg.py:
   70      ```python
   71:     >>> from transformers import CLIPSegTextConfig, CLIPSegTextModel
   72  

  159      ```python
  160:     >>> from transformers import CLIPSegVisionConfig, CLIPSegVisionModel
  161  

  250      ```python
  251:     >>> from transformers import CLIPSegConfig, CLIPSegModel
  252  

train_real_world/transformers_4573/src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py:
  23  
  24: from transformers import (
  25      CLIPSegConfig,

train_real_world/transformers_4573/src/transformers/models/clipseg/modeling_clipseg.py:
    44  
    45: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->clipseg
    46  def clipseg_loss(similarity: torch.Tensor) -> torch.Tensor:

    53  @auto_docstring
    54: # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->CLIPSeg
    55  class CLIPSegOutput(ModelOutput):

   135  class CLIPSegVisionEmbeddings(nn.Module):
   136:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.__init__ with CLIP->CLIPSeg
   137      def __init__(self, config: CLIPSegVisionConfig):

   217  
   218: # Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->CLIPSeg
   219  class CLIPSegTextEmbeddings(nn.Module):

   258  
   259: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   260  def eager_attention_forward(

   356  
   357: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->CLIPSeg
   358  class CLIPSegMLP(nn.Module):

   372  
   373: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->CLIPSeg
   374  class CLIPSegEncoderLayer(GradientCheckpointingLayer):

   475  
   476: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->CLIPSeg
   477  class CLIPSegEncoder(nn.Module):

   686          ```python
   687:         >>> from transformers import AutoTokenizer, CLIPSegTextModel
   688  

   708  class CLIPSegVisionTransformer(nn.Module):
   709:     # Copied from transformers.models.altclip.modeling_altclip.AltCLIPVisionTransformer.__init__ with AltCLIP->CLIPSeg
   710      def __init__(self, config: CLIPSegVisionConfig):

   789          >>> import requests
   790:         >>> from transformers import AutoProcessor, CLIPSegVisionModel
   791  

   869          >>> import torch
   870:         >>> from transformers import AutoTokenizer, CLIPSegModel
   871  

   904          >>> import torch
   905:         >>> from transformers import AutoProcessor, CLIPSegModel
   906:         >>> from transformers.image_utils import load_image
   907  

   949          >>> import torch
   950:         >>> from transformers import AutoProcessor, CLIPSegModel
   951:         >>> from transformers.image_utils import load_image
   952  

  1031  
  1032:     # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer.__init__ with AltCLIP->CLIPSeg
  1033      def __init__(self, config: CLIPSegConfig):

  1266          >>> import torch
  1267:         >>> from transformers import AutoProcessor, CLIPSegForImageSegmentation
  1268:         >>> from transformers.image_utils import load_image
  1269  

train_real_world/transformers_4573/src/transformers/models/clvp/configuration_clvp.py:
   76      ```python
   77:     >>> from transformers import ClvpEncoderConfig, ClvpEncoder
   78  

  237      ```python
  238:     >>> from transformers import ClvpDecoderConfig, ClvpDecoder
  239  

  343      ```python
  344:     >>> from transformers import ClvpConfig, ClvpModelForConditionalGeneration
  345  

  355      >>> # We can also initialize a CLVPConfig from a CLVPTextConfig, CLVPSpeechConfig and a CLVPAutoRegressiveConfig
  356:     >>> from transformers import ClvpEncoderConfig, ClvpDecoderConfig
  357  

train_real_world/transformers_4573/src/transformers/models/clvp/convert_clvp_to_hf.py:
  25  
  26: from transformers import ClvpConfig, ClvpModelForConditionalGeneration
  27  

train_real_world/transformers_4573/src/transformers/models/clvp/modeling_clvp.py:
    55  
    56: # Copied from transformers.models.clip.modeling_clip.contrastive_loss
    57  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

    60  
    61: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->clvp, image_loss->speech_loss
    62  def clvp_loss(similarity: torch.Tensor) -> torch.Tensor:

    67  
    68: # Copied from transformers.models.llama.modeling_llama.rotate_half
    69  def rotate_half(x):

   216  
   217: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Clvp
   218  class ClvpRMSNorm(nn.Module):

   475  
   476: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Clvp
   477  class ClvpSequenceSummary(nn.Module):

   575  
   576: # Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP with GPT2->ClvpDecoderMLP
   577  class ClvpDecoderMLP(nn.Module):

  1521          ```python
  1522:         >>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration
  1523  

  1587          >>> import datasets
  1588:         >>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration
  1589  

  1666          >>> import datasets
  1667:         >>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration
  1668  

train_real_world/transformers_4573/src/transformers/models/clvp/tokenization_clvp.py:
  81      ```python
  82:     >>> from transformers import ClvpTokenizer
  83  

train_real_world/transformers_4573/src/transformers/models/code_llama/tokenization_code_llama.py:
  50      ```python
  51:     >>> from transformers import CodeLlamaTokenizer
  52  

train_real_world/transformers_4573/src/transformers/models/codegen/configuration_codegen.py:
  76      ```python
  77:     >>> from transformers import CodeGenConfig, CodeGenModel
  78  

train_real_world/transformers_4573/src/transformers/models/codegen/modeling_codegen.py:
   47  
   48: # Copied from transformers.models.gptj.modeling_gptj.create_sinusoidal_positions
   49  def create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:

   54  
   55: # Copied from transformers.models.gptj.modeling_gptj.rotate_every_two
   56  def rotate_every_two(x: torch.Tensor) -> torch.Tensor:

   62  
   63: # Copied from transformers.models.gptj.modeling_gptj.apply_rotary_pos_emb
   64  def apply_rotary_pos_emb(tensor: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor) -> torch.Tensor:

  219  
  220: # Copied from transformers.models.gptj.modeling_gptj.GPTJMLP with GPTJ->CodeGen
  221  class CodeGenMLP(nn.Module):

  239  
  240: # Copied from transformers.models.gptj.modeling_gptj.GPTJBlock with GPTJ->CodeGen
  241  class CodeGenBlock(GradientCheckpointingLayer):

  421  
  422:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  423      def _update_causal_mask(

  491      @staticmethod
  492:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  493      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/codegen/tokenization_codegen.py:
  46      ```python
  47:     >>> from transformers import CodeGenTokenizer
  48  

train_real_world/transformers_4573/src/transformers/models/cohere/configuration_cohere.py:
  94      ```python
  95:     >>> from transformers import CohereModel, CohereConfig
  96  

train_real_world/transformers_4573/src/transformers/models/cohere/modeling_cohere.py:
  512          ```python
  513:         >> from transformers import AutoTokenizer, CohereForCausalLM
  514  

train_real_world/transformers_4573/src/transformers/models/cohere/modular_cohere.py:
  297          ```python
  298:         >> from transformers import AutoTokenizer, CohereForCausalLM
  299  

train_real_world/transformers_4573/src/transformers/models/cohere/tokenization_cohere.py:
  52      ```python
  53:     >>> from transformers import AutoTokenizer
  54  

train_real_world/transformers_4573/src/transformers/models/cohere2/configuration_cohere2.py:
  92      ```python
  93:     >>> from transformers import Cohere2Model, Cohere2Config
  94  

train_real_world/transformers_4573/src/transformers/models/cohere2/modeling_cohere2.py:
  491          ```python
  492:         >> from transformers import AutoTokenizer, Cohere2ForCausalLM
  493  

train_real_world/transformers_4573/src/transformers/models/cohere2/modular_cohere2.py:
  114      ```python
  115:     >>> from transformers import Cohere2Model, Cohere2Config
  116  

train_real_world/transformers_4573/src/transformers/models/cohere2_vision/modeling_cohere2_vision.py:
  310          ```python
  311:         >>> from transformers import AutoProcessor, Cohere2VisionForConditionalGeneration
  312          >>> import torch

train_real_world/transformers_4573/src/transformers/models/cohere2_vision/modular_cohere2_vision.py:
   23  
   24: from transformers.models.aya_vision.modeling_aya_vision import (
   25      AyaVisionCausalLMOutputWithPast,

   30  )
   31: from transformers.models.got_ocr2.image_processing_got_ocr2_fast import GotOcr2ImageProcessorFast
   32  

  195          ```python
  196:         >>> from transformers import AutoProcessor, Cohere2VisionForConditionalGeneration
  197          >>> import torch

train_real_world/transformers_4573/src/transformers/models/colpali/configuration_colpali.py:
  52      ```python
  53:     from transformers.models.colpali import ColPaliConfig, ColPaliForRetrieval
  54  

train_real_world/transformers_4573/src/transformers/models/colpali/convert_colpali_weights_to_hf.py:
  47  
  48: from transformers import AutoConfig
  49: from transformers.models.colpali import ColPaliForRetrieval
  50: from transformers.models.colpali.configuration_colpali import ColPaliConfig
  51: from transformers.utils import logging
  52  

train_real_world/transformers_4573/src/transformers/models/colpali/modeling_colpali.py:
  22  
  23: from transformers import AutoModelForImageTextToText
  24  

train_real_world/transformers_4573/src/transformers/models/colpali/modular_colpali.py:
  18  
  19: from transformers.models.paligemma.processing_paligemma import IMAGE_TOKEN, PaliGemmaProcessor, build_string_from_input
  20  

train_real_world/transformers_4573/src/transformers/models/colqwen2/configuration_colqwen2.py:
  48      ```python
  49:     from transformers.models.colqwen2 import ColQwen2Config, ColQwen2ForRetrieval
  50  

train_real_world/transformers_4573/src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py:
  44  
  45: from transformers import AutoConfig, AutoModel
  46: from transformers.models.colqwen2 import ColQwen2ForRetrieval
  47: from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config
  48: from transformers.utils import logging
  49  

train_real_world/transformers_4573/src/transformers/models/colqwen2/modeling_colqwen2.py:
  26  
  27: from transformers import AutoModelForImageTextToText
  28  

train_real_world/transformers_4573/src/transformers/models/conditional_detr/configuration_conditional_detr.py:
  118      ```python
  119:     >>> from transformers import ConditionalDetrConfig, ConditionalDetrModel
  120  

train_real_world/transformers_4573/src/transformers/models/conditional_detr/convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py:
  26  
  27: from transformers import (
  28      ConditionalDetrConfig,

  32  )
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/conditional_detr/image_processing_conditional_detr.py:
    24  
    25: from transformers.image_transforms import get_size_with_aspect_ratio
    26  

    81  
    82: # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size
    83  def get_resize_output_image_size(

   110  
   111: # Copied from transformers.models.detr.image_processing_detr.get_image_size_for_max_height_width
   112  def get_image_size_for_max_height_width(

   146  
   147: # Copied from transformers.models.detr.image_processing_detr.safe_squeeze
   148  def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:

   160  
   161: # Copied from transformers.models.detr.image_processing_detr.normalize_annotation
   162  def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:

   175  
   176: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
   177  def max_across_indices(values: Iterable[Any]) -> list[Any]:

   183  
   184: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   185  def get_max_height_width(

   202  
   203: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
   204  def make_pixel_mask(

   221  
   222: # Copied from transformers.models.detr.image_processing_detr.convert_coco_poly_to_mask
   223  def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndarray:

   256  
   257: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_detection_annotation with DETR->ConditionalDetr
   258  def prepare_coco_detection_annotation(

   317  
   318: # Copied from transformers.models.detr.image_processing_detr.masks_to_boxes
   319  def masks_to_boxes(masks: np.ndarray) -> np.ndarray:

   352  
   353: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_panoptic_annotation with DETR->ConditionalDetr
   354  def prepare_coco_panoptic_annotation(

   394  
   395: # Copied from transformers.models.detr.image_processing_detr.get_segmentation_image
   396  def get_segmentation_image(

   420  
   421: # Copied from transformers.models.detr.image_processing_detr.get_mask_area
   422  def get_mask_area(seg_img: np.ndarray, target_size: tuple[int, int], n_classes: int) -> np.ndarray:

   430  
   431: # Copied from transformers.models.detr.image_processing_detr.score_labels_from_class_probabilities
   432  def score_labels_from_class_probabilities(logits: np.ndarray) -> tuple[np.ndarray, np.ndarray]:

   439  
   440: # Copied from transformers.models.detr.image_processing_detr.post_process_panoptic_sample with DetrForSegmentation->ConditionalDetrForSegmentation
   441  def post_process_panoptic_sample(

   525  
   526: # Copied from transformers.models.detr.image_processing_detr.resize_annotation
   527  def resize_annotation(

   577  
   578: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
   579  def binary_mask_to_rle(mask):

   600  
   601: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
   602  def convert_segmentation_to_rle(segmentation):

   622  
   623: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
   624  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

   651  
   652: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
   653  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

   670  
   671: # Copied from transformers.models.detr.image_processing_detr.compute_segments
   672  def compute_segments(

   811  
   812:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__
   813      def __init__(

   871  
   872:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->ConditionalDetr
   873      def prepare_annotation(

   904  
   905:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize
   906      def resize(

   965  
   966:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize_annotation
   967      def resize_annotation(

   979  
   980:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
   981      def rescale(

  1008  
  1009:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation
  1010      def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:

  1016  
  1017:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image
  1018      def _update_annotation_for_padded_image(

  1060  
  1061:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._pad_image
  1062      def _pad_image(

  1094  
  1095:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.pad
  1096      def pad(

  1178  
  1179:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.preprocess
  1180      def preprocess(

  1427  
  1428:     # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection with DeformableDetr->ConditionalDetr
  1429      def post_process_object_detection(

  1487  
  1488:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation with Detr->ConditionalDetr
  1489      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):

  1535  
  1536:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_instance_segmentation with Detr->ConditionalDetr
  1537      def post_process_instance_segmentation(

  1619  
  1620:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_panoptic_segmentation with Detr->ConditionalDetr
  1621      def post_process_panoptic_segmentation(

train_real_world/transformers_4573/src/transformers/models/conditional_detr/modeling_conditional_detr.py:
    97  )
    98: # Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->ConditionalDetr
    99  class ConditionalDetrObjectDetectionOutput(ModelOutput):

   141  )
   142: # Copied from transformers.models.detr.modeling_detr.DetrSegmentationOutput with Detr->ConditionalDetr
   143  class ConditionalDetrSegmentationOutput(ModelOutput):

   186  
   187: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->ConditionalDetr
   188  class ConditionalDetrFrozenBatchNorm2d(nn.Module):

   226  
   227: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->ConditionalDetr
   228  def replace_batch_norm(model):

   251  
   252: # Copied from transformers.models.detr.modeling_detr.DetrConvEncoder with Detr->ConditionalDetr
   253  class ConditionalDetrConvEncoder(nn.Module):

   324  
   325: # Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->ConditionalDetr
   326  class ConditionalDetrConvModel(nn.Module):

   383  
   384: # Copied from transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding with Detr->ConditionalDetr
   385  class ConditionalDetrLearnedPositionEmbedding(nn.Module):

   407  
   408: # Copied from transformers.models.detr.modeling_detr.build_position_encoding with Detr->ConditionalDetr
   409  def build_position_encoding(config):

   444  
   445: # Copied from transformers.models.detr.modeling_detr.DetrAttention
   446  class DetrAttention(nn.Module):

   699  
   700: # Copied from transformers.models.detr.modeling_detr.DetrEncoderLayer with DetrEncoderLayer->ConditionalDetrEncoderLayer,DetrConfig->ConditionalDetrConfig
   701  class ConditionalDetrEncoderLayer(nn.Module):

   942  
   943: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with DetrMLPPredictionHead->MLP
   944  class MLP(nn.Module):

   965  @auto_docstring
   966: # Copied from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->ConditionalDetr
   967  class ConditionalDetrPreTrainedModel(PreTrainedModel):

  1000  
  1001: # Copied from transformers.models.detr.modeling_detr.DetrEncoder with Detr->ConditionalDetr,DETR->ConditionalDETR
  1002  class ConditionalDetrEncoder(ConditionalDetrPreTrainedModel):

  1366          ```python
  1367:         >>> from transformers import AutoImageProcessor, AutoModel
  1368          >>> from PIL import Image

  1474  
  1475: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->ConditionalDetr
  1476  class ConditionalDetrMLPPredictionHead(nn.Module):

  1557          ```python
  1558:         >>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
  1559          >>> from PIL import Image

  1730  
  1731:         >>> from transformers import (
  1732          ...     AutoImageProcessor,

  1735          ... )
  1736:         >>> from transformers.image_transforms import rgb_to_id
  1737  

  1882  
  1883: # Copied from transformers.models.detr.modeling_detr.DetrMaskHeadSmallConv with Detr->ConditionalDetr
  1884  class ConditionalDetrMaskHeadSmallConv(nn.Module):

  1963  
  1964: # Copied from transformers.models.detr.modeling_detr.DetrMHAttentionMap with Detr->ConditionalDetr
  1965  class ConditionalDetrMHAttentionMap(nn.Module):

train_real_world/transformers_4573/src/transformers/models/conditional_detr/modular_conditional_detr.py:
  4  
  5: from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast
  6  

train_real_world/transformers_4573/src/transformers/models/convbert/configuration_convbert.py:
  74      ```python
  75:     >>> from transformers import ConvBertConfig, ConvBertModel
  76  

train_real_world/transformers_4573/src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch.py:
  22  
  23: from transformers import ConvBertConfig, ConvBertModel
  24: from transformers.utils import logging
  25  

train_real_world/transformers_4573/src/transformers/models/convbert/modeling_convbert.py:
  503  
  504: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->ConvBert
  505  class ConvBertSequenceSummary(nn.Module):

train_real_world/transformers_4573/src/transformers/models/convnext/configuration_convnext.py:
  69      ```python
  70:     >>> from transformers import ConvNextConfig, ConvNextModel
  71  

train_real_world/transformers_4573/src/transformers/models/convnext/convert_convnext_to_pytorch.py:
  27  
  28: from transformers import ConvNextConfig, ConvNextForImageClassification, ConvNextImageProcessor
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/convnext/modeling_convnext.py:
   39  
   40: # Copied from transformers.models.beit.modeling_beit.drop_path
   41  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   55  
   56: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->ConvNext
   57  class ConvNextDropPath(nn.Module):

  378          ```python
  379:         >>> from transformers import AutoImageProcessor, AutoBackbone
  380          >>> import torch

train_real_world/transformers_4573/src/transformers/models/convnextv2/configuration_convnextv2.py:
  69      ```python
  70:     >>> from transformers import ConvNeXTV2Config, ConvNextV2Model
  71  

train_real_world/transformers_4573/src/transformers/models/convnextv2/convert_convnextv2_to_pytorch.py:
  27  
  28: from transformers import ConvNextImageProcessor, ConvNextV2Config, ConvNextV2ForImageClassification
  29: from transformers.image_utils import PILImageResampling
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/convnextv2/modeling_convnextv2.py:
   39  
   40: # Copied from transformers.models.beit.modeling_beit.drop_path
   41  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   55  
   56: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->ConvNextV2
   57  class ConvNextV2DropPath(nn.Module):

   87  
   88: # Copied from transformers.models.convnext.modeling_convnext.ConvNextLayerNorm with ConvNext->ConvNextV2
   89  class ConvNextV2LayerNorm(nn.LayerNorm):

  114  
  115: # Copied from transformers.models.convnext.modeling_convnext.ConvNextEmbeddings with ConvNext->ConvNextV2
  116  class ConvNextV2Embeddings(nn.Module):

  182  
  183: # Copied from transformers.models.convnext.modeling_convnext.ConvNextStage with ConvNeXT->ConvNeXTV2, ConvNext->ConvNextV2
  184  class ConvNextV2Stage(nn.Module):

  219  
  220: # Copied from transformers.models.convnext.modeling_convnext.ConvNextEncoder with ConvNext->ConvNextV2
  221  class ConvNextV2Encoder(nn.Module):

  273  @auto_docstring
  274: # Copied from transformers.models.convnext.modeling_convnext.ConvNextModel with CONVNEXT->CONVNEXTV2, ConvNext->ConvNextV2
  275  class ConvNextV2Model(ConvNextV2PreTrainedModel):

  321  )
  322: # Copied from transformers.models.convnext.modeling_convnext.ConvNextForImageClassification with CONVNEXT->CONVNEXTV2,ConvNext->ConvNextV2,convnext->convnextv2
  323  class ConvNextV2ForImageClassification(ConvNextV2PreTrainedModel):

  371  )
  372: # Copied from transformers.models.convnext.modeling_convnext.ConvNextBackbone with CONVNEXT->CONVNEXTV2,ConvNext->ConvNextV2,facebook/convnext-tiny-224->facebook/convnextv2-tiny-1k-224
  373  class ConvNextV2Backbone(ConvNextV2PreTrainedModel, BackboneMixin):

  401          ```python
  402:         >>> from transformers import AutoImageProcessor, AutoBackbone
  403          >>> import torch

train_real_world/transformers_4573/src/transformers/models/cpmant/configuration_cpmant.py:
  70      ```python
  71:     >>> from transformers import CpmAntModel, CpmAntConfig
  72  

train_real_world/transformers_4573/src/transformers/models/cpmant/modeling_cpmant.py:
  400  
  401: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->CPMAnt
  402  class CpmAntIntermediate(nn.Module):

  508  
  509: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->CPMAnt
  510  class CpmAntOutput(nn.Module):

  734          ```python
  735:         >>> from transformers import CPMAntTokenizer, CpmAntForCausalLM
  736  

train_real_world/transformers_4573/src/transformers/models/cpmant/tokenization_cpmant.py:
  20  
  21: from transformers.utils import is_rjieba_available, requires_backends
  22  

train_real_world/transformers_4573/src/transformers/models/csm/configuration_csm.py:
   92      ```python
   93:     >>> from transformers import CsmDepthDecoder, CsmDepthDecoderConfig
   94  

  246      ```python
  247:     >>> from transformers import CsmForConditionalGeneration, CsmConfig
  248  

train_real_world/transformers_4573/src/transformers/models/csm/convert_csm.py:
  23  
  24: from transformers import (
  25      AutoFeatureExtractor,

  32  )
  33: from transformers.utils.hub import cached_file
  34  

train_real_world/transformers_4573/src/transformers/models/csm/generation_csm.py:
  410          ```python
  411:         >>> from transformers import CsmProcessor, CsmForConditionalGeneration
  412          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/csm/modeling_csm.py:
    28  
    29: from transformers.utils.generic import check_model_inputs
    30  

  1000          >>> import torch
  1001:         >>> from transformers import CsmForConditionalGeneration, AutoProcessor
  1002          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/csm/modular_csm.py:
   21  
   22: from transformers.utils.generic import check_model_inputs
   23  

  647          >>> import torch
  648:         >>> from transformers import CsmForConditionalGeneration, AutoProcessor
  649          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/csm/processing_csm.py:
  69          ```python
  70:         from transformers import CsmProcessor
  71          from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/ctrl/configuration_ctrl.py:
  63      ```python
  64:     >>> from transformers import CTRLConfig, CTRLModel
  65  

train_real_world/transformers_4573/src/transformers/models/ctrl/modeling_ctrl.py:
  250          ```python
  251:         >>> from transformers import AutoTokenizer, CTRLModel
  252          >>> import torch

  419          >>> import torch
  420:         >>> from transformers import AutoTokenizer, CTRLLMHeadModel
  421  

  560          >>> import torch
  561:         >>> from transformers import AutoTokenizer, CTRLForSequenceClassification
  562  

  595          >>> import torch
  596:         >>> from transformers import AutoTokenizer, CTRLForSequenceClassification
  597  

train_real_world/transformers_4573/src/transformers/models/cvt/configuration_cvt.py:
  82      ```python
  83:     >>> from transformers import CvtConfig, CvtModel
  84  

train_real_world/transformers_4573/src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py:
  26  
  27: from transformers import AutoImageProcessor, CvtConfig, CvtForImageClassification
  28  

train_real_world/transformers_4573/src/transformers/models/cvt/modeling_cvt.py:
  51  
  52: # Copied from transformers.models.beit.modeling_beit.drop_path
  53  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  67  
  68: # Copied from transformers.models.beit.modeling_beit.BeitDropPath
  69  class CvtDropPath(nn.Module):

train_real_world/transformers_4573/src/transformers/models/cwm/modeling_cwm.py:
  473          ```python
  474:         >>> from transformers import AutoTokenizer, CwmForCausalLM
  475  

train_real_world/transformers_4573/src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py:
  27  
  28: from transformers import DFineConfig, DFineForObjectDetection, RTDetrImageProcessor
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/d_fine/modeling_d_fine.py:
  1275          ```python
  1276:         >>> from transformers import AutoImageProcessor, DFineModel
  1277          >>> from PIL import Image

  1611          >>> import torch
  1612:         >>> from transformers.image_utils import load_image
  1613:         >>> from transformers import AutoImageProcessor, DFineForObjectDetection
  1614  

train_real_world/transformers_4573/src/transformers/models/d_fine/modular_d_fine.py:
  932          >>> import torch
  933:         >>> from transformers.image_utils import load_image
  934:         >>> from transformers import AutoImageProcessor, DFineForObjectDetection
  935  

train_real_world/transformers_4573/src/transformers/models/dab_detr/configuration_dab_detr.py:
  125      ```python
  126:     >>> from transformers import DabDetrConfig, DabDetrModel
  127  

train_real_world/transformers_4573/src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py:
  26  
  27: from transformers import ConditionalDetrImageProcessor, DabDetrConfig, DabDetrForObjectDetection
  28: from transformers.utils import logging
  29  

  89  
  90: # Copied from transformers.models.mllama.convert_mllama_weights_to_hf.convert_old_keys_to_new_keys
  91  def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):

train_real_world/transformers_4573/src/transformers/models/dab_detr/modeling_dab_detr.py:
    50  )
    51: # Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)
    52  class DabDetrDecoderOutput(BaseModelOutputWithCrossAttentions):

    77  )
    78: # Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)
    79  class DabDetrModelOutput(Seq2SeqModelOutput):

    99  )
   100: # Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->DabDetr
   101  class DabDetrObjectDetectionOutput(ModelOutput):

   137  
   138: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->DabDetr
   139  class DabDetrFrozenBatchNorm2d(nn.Module):

   177  
   178: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->DabDetr
   179  def replace_batch_norm(model):

   202  
   203: # Modified from transformers.models.detr.modeling_detr.DetrConvEncoder with Detr->DabDetr
   204  class DabDetrConvEncoder(nn.Module):

   235  
   236: # Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->DabDetr
   237  class DabDetrConvModel(nn.Module):

   257  
   258: # Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrSinePositionEmbedding with ConditionalDetr->DabDetr
   259  class DabDetrSinePositionEmbedding(nn.Module):

   351  
   352: # Modified from transformers.models.detr.modeling_detr.DetrAttention
   353  class DetrAttention(nn.Module):

   429  
   430: # Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrAttention with ConditionalDetr->DABDETR,Conditional DETR->DabDetr
   431  class DabDetrAttention(nn.Module):

   657  
   658: # Modified from transformers.models.detr.modeling_detr.DetrEncoderLayer with DetrEncoderLayer->DabDetrEncoderLayer,DetrConfig->DabDetrConfig
   659  class DabDetrEncoderLayer(GradientCheckpointingLayer):

   719  
   720: # Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderLayer with ConditionalDetr->DabDetr
   721  class DabDetrDecoderLayer(GradientCheckpointingLayer):

   787  
   788: # Modified from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with DetrMLPPredictionHead->DabDetrMLP
   789  class DabDetrMLP(nn.Module):

   809  
   810: # Modified from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->DabDetr
   811  @auto_docstring

   852  
   853: # Modified from transformers.models.detr.modeling_detr.DetrEncoder with Detr->DabDetr,DETR->ConditionalDETR
   854  class DabDetrEncoder(DabDetrPreTrainedModel):

   964  
   965: # Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoder with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR
   966  class DabDetrDecoder(DabDetrPreTrainedModel):

  1241          ```python
  1242:         >>> from transformers import AutoImageProcessor, AutoModel
  1243          >>> from PIL import Image

  1397  
  1398: # Copied from transformers.models.detr.modeling_detr.DetrMHAttentionMap with Detr->DabDetr
  1399  class DabDetrMHAttentionMap(nn.Module):

  1494          ```python
  1495:         >>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
  1496          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/dac/configuration_dac.py:
  61      ```python
  62:     >>> from transformers import DacModel, DacConfig
  63  

train_real_world/transformers_4573/src/transformers/models/dac/convert_dac_checkpoint.py:
  22  
  23: from transformers import (
  24      DacConfig,

train_real_world/transformers_4573/src/transformers/models/dac/modeling_dac.py:
   76  @auto_docstring
   77: # Copied from transformers.models.encodec.modeling_encodec.EncodecDecoderOutput with Encodec->Dac, segment_length->input_length
   78  class DacDecoderOutput(ModelOutput):

  656          >>> from datasets import load_dataset, Audio
  657:         >>> from transformers import DacModel, AutoProcessor
  658          >>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

train_real_world/transformers_4573/src/transformers/models/data2vec/configuration_data2vec_audio.py:
  154      ```python
  155:     >>> from transformers import Data2VecAudioConfig, Data2VecAudioModel
  156  

train_real_world/transformers_4573/src/transformers/models/data2vec/configuration_data2vec_text.py:
  73      ```python
  74:     >>> from transformers import Data2VecTextConfig, Data2VecTextModel
  75  

train_real_world/transformers_4573/src/transformers/models/data2vec/configuration_data2vec_vision.py:
  91      ```python
  92:     >>> from transformers import Data2VecVisionConfig, Data2VecVisionModel
  93  

train_real_world/transformers_4573/src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import Wav2Vec2Processor, logging
  26: from transformers.models.data2vec.configuration_data2vec_audio import Data2VecAudioConfig
  27  
  28  # Copied from https://github.com/pytorch/fairseq/blob/main/examples/data2vec/models/data2vec_audio.py
  29: from transformers.models.data2vec.data2vec_audio import Data2VecAudioModel as Dummy  # noqa: F401
  30: from transformers.models.data2vec.modeling_data2vec_audio import Data2VecAudioForCTC, Data2VecAudioModel
  31  

train_real_world/transformers_4573/src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py:
  25  
  26: from transformers import (
  27      Data2VecTextConfig,

  31  )
  32: from transformers.models.bert.modeling_bert import (
  33      BertIntermediate,

  41  # File copied from https://github.com/pytorch/fairseq/blob/main/examples/data2vec/models/data2vec_text.py
  42: from transformers.utils import logging
  43  

train_real_world/transformers_4573/src/transformers/models/data2vec/convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py:
   9  
  10: from transformers import (
  11      BeitImageProcessor,

train_real_world/transformers_4573/src/transformers/models/data2vec/modeling_data2vec_text.py:
  794          ```python
  795:         >>> from transformers import AutoTokenizer, Data2VecTextForCausalLM, Data2VecTextConfig
  796          >>> import torch

train_real_world/transformers_4573/src/transformers/models/data2vec/modeling_data2vec_vision.py:
    49  )
    50: # Copied from transformers.models.beit.modeling_beit.BeitModelOutputWithPooling with Beit->Data2VecVision
    51  class Data2VecVisionModelOutputWithPooling(BaseModelOutputWithPooling):

    59  
    60: # Copied from transformers.models.beit.modeling_beit.drop_path
    61  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

    75  
    76: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Data2VecVision
    77  class Data2VecVisionDropPath(nn.Module):

    90  
    91: # Copied from transformers.models.beit.modeling_beit.BeitEmbeddings with Beit->Data2VecVision
    92  class Data2VecVisionEmbeddings(nn.Module):

   119  
   120:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   121      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   186  
   187: # Copied from transformers.models.beit.modeling_beit.BeitPatchEmbeddings with Beit->Data2VecVision
   188  class Data2VecVisionPatchEmbeddings(nn.Module):

   225  
   226: # Copied from transformers.models.beit.modeling_beit.BeitSelfAttention with Beit->Data2VecVision
   227  class Data2VecVisionSelfAttention(nn.Module):

   310  
   311: # Copied from transformers.models.beit.modeling_beit.BeitSdpaSelfAttention with Beit->Data2VecVision
   312  class Data2VecVisionSdpaSelfAttention(Data2VecVisionSelfAttention):

   373  
   374: # Copied from transformers.models.beit.modeling_beit.BeitSelfOutput with Beit->Data2VecVision
   375  class Data2VecVisionSelfOutput(nn.Module):

   425  
   426: # Copied from transformers.models.beit.modeling_beit.BeitIntermediate with Beit->Data2VecVision
   427  class Data2VecVisionIntermediate(nn.Module):

   442  
   443: # Copied from transformers.models.beit.modeling_beit.BeitOutput with Beit->Data2VecVision
   444  class Data2VecVisionOutput(nn.Module):

   456  
   457: # Copied from transformers.models.beit.modeling_beit.BeitLayer with Beit->Data2VecVision,BEiT->Data2VecVision
   458  class Data2VecVisionLayer(GradientCheckpointingLayer):

   522  
   523: # Copied from transformers.models.beit.modeling_beit.BeitRelativePositionBias with Beit->Data2VecVision
   524  class Data2VecVisionRelativePositionBias(nn.Module):

   606  
   607: # Copied from transformers.models.beit.modeling_beit.BeitEncoder with Beit->Data2VecVision
   608  class Data2VecVisionEncoder(nn.Module):

   680  @auto_docstring
   681: # Copied from transformers.models.beit.modeling_beit.BeitPreTrainedModel with Beit->Data2VecVision,beit->data2vec_vision
   682  class Data2VecVisionPreTrainedModel(PreTrainedModel):

   710  @auto_docstring
   711: # Copied from transformers.models.beit.modeling_beit.BeitModel with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,True->False
   712  class Data2VecVisionModel(Data2VecVisionPreTrainedModel):

   782  
   783: # Copied from transformers.models.beit.modeling_beit.BeitPooler with Beit->Data2VecVision
   784  class Data2VecVisionPooler(nn.Module):

   808  )
   809: # Copied from transformers.models.beit.modeling_beit.BeitForImageClassification with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,beit->data2vec_vision
   810  class Data2VecVisionForImageClassification(Data2VecVisionPreTrainedModel):

   868  
   869: # Copied from transformers.models.beit.modeling_beit.BeitConvModule with Beit->Data2VecVision
   870  class Data2VecVisionConvModule(nn.Module):

   906  
   907: # Copied from transformers.models.beit.modeling_beit.BeitPyramidPoolingBlock with Beit->Data2VecVision
   908  class Data2VecVisionPyramidPoolingBlock(nn.Module):

   924  
   925: # Copied from transformers.models.beit.modeling_beit.BeitPyramidPoolingModule with Beit->Data2VecVision
   926  class Data2VecVisionPyramidPoolingModule(nn.Module):

   964  
   965: # Copied from transformers.models.beit.modeling_beit.BeitUperHead with Beit->Data2VecVision
   966  class Data2VecVisionUperHead(nn.Module):

  1050  
  1051: # Copied from transformers.models.beit.modeling_beit.BeitFCNHead with Beit->Data2VecVision
  1052  class Data2VecVisionFCNHead(nn.Module):

  1115  @auto_docstring
  1116: # Copied from transformers.models.beit.modeling_beit.BeitForSemanticSegmentation with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,microsoft/beit-base-finetuned-ade-640-640->facebook/data2vec-vision-base,beit->data2vec_vision
  1117  class Data2VecVisionForSemanticSegmentation(Data2VecVisionPreTrainedModel):

  1187          ```python
  1188:         >>> from transformers import AutoImageProcessor, Data2VecVisionForSemanticSegmentation
  1189          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/data2vec/modular_data2vec_text.py:
  160          ```python
  161:         >>> from transformers import AutoTokenizer, Data2VecTextForCausalLM, Data2VecTextConfig
  162          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dbrx/configuration_dbrx.py:
  155      ```python
  156:     >>> from transformers import DbrxConfig, DbrxModel
  157  

train_real_world/transformers_4573/src/transformers/models/dbrx/modeling_dbrx.py:
  715          ```python
  716:         >> from transformers import AutoTokenizer, DbrxForCausalLM
  717  

train_real_world/transformers_4573/src/transformers/models/dbrx/modular_dbrx.py:
  501          ```python
  502:         >> from transformers import AutoTokenizer, DbrxForCausalLM
  503  

train_real_world/transformers_4573/src/transformers/models/deberta/configuration_deberta.py:
  83      ```python
  84:     >>> from transformers import DebertaConfig, DebertaModel
  85  

train_real_world/transformers_4573/src/transformers/models/deberta/modeling_deberta.py:
  464  
  465: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Deberta
  466  class DebertaIntermediate(nn.Module):

  768  
  769: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->LegacyDeberta
  770  class LegacyDebertaOnlyMLMHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/deberta/tokenization_deberta.py:
  39      ```python
  40:     >>> from transformers import DebertaTokenizer
  41  

train_real_world/transformers_4573/src/transformers/models/deberta_v2/configuration_deberta_v2.py:
  83      ```python
  84:     >>> from transformers import DebertaV2Config, DebertaV2Model
  85  

train_real_world/transformers_4573/src/transformers/models/deberta_v2/modeling_deberta_v2.py:
    42  
    43: # Copied from transformers.models.deberta.modeling_deberta.DebertaSelfOutput with DebertaLayerNorm->LayerNorm
    44  class DebertaV2SelfOutput(nn.Module):

   106  @torch.jit.script
   107: # Copied from transformers.models.deberta.modeling_deberta.c2p_dynamic_expand
   108  def c2p_dynamic_expand(c2p_pos, query_layer, relative_pos):

   112  @torch.jit.script
   113: # Copied from transformers.models.deberta.modeling_deberta.p2c_dynamic_expand
   114  def p2c_dynamic_expand(c2p_pos, query_layer, key_layer):

   118  @torch.jit.script
   119: # Copied from transformers.models.deberta.modeling_deberta.pos_dynamic_expand
   120  def pos_dynamic_expand(pos_index, p2c_att, key_layer):

   352  
   353: # Copied from transformers.models.deberta.modeling_deberta.DebertaAttention with Deberta->DebertaV2
   354  class DebertaV2Attention(nn.Module):

   387  
   388: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->DebertaV2
   389  class DebertaV2Intermediate(nn.Module):

   403  
   404: # Copied from transformers.models.deberta.modeling_deberta.DebertaOutput with DebertaLayerNorm->LayerNorm
   405  class DebertaV2Output(nn.Module):

   419  
   420: # Copied from transformers.models.deberta.modeling_deberta.DebertaLayer with Deberta->DebertaV2
   421  class DebertaV2Layer(GradientCheckpointingLayer):

   489  
   490: # Copied from transformers.models.deberta.modeling_deberta.DebertaEmbeddings with DebertaLayerNorm->LayerNorm,Deberta->DebertaV2
   491  class DebertaV2Embeddings(nn.Module):

   707  @auto_docstring
   708: # Copied from transformers.models.deberta.modeling_deberta.DebertaModel with Deberta->DebertaV2
   709  class DebertaV2Model(DebertaV2PreTrainedModel):

   808  
   809: # Copied from transformers.models.deberta.modeling_deberta.LegacyDebertaPredictionHeadTransform with Deberta->DebertaV2
   810  class LegacyDebertaV2PredictionHeadTransform(nn.Module):

   929      @auto_docstring
   930:     # Copied from transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.forward with Deberta->DebertaV2
   931      def forward(

   986  
   987: # Copied from transformers.models.deberta.modeling_deberta.ContextPooler
   988  class ContextPooler(nn.Module):

  1041      @auto_docstring
  1042:     # Copied from transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.forward with Deberta->DebertaV2
  1043      def forward(

  1124  @auto_docstring
  1125: # Copied from transformers.models.deberta.modeling_deberta.DebertaForTokenClassification with Deberta->DebertaV2
  1126  class DebertaV2ForTokenClassification(DebertaV2PreTrainedModel):

  1200      @auto_docstring
  1201:     # Copied from transformers.models.deberta.modeling_deberta.DebertaForQuestionAnswering.forward with Deberta->DebertaV2
  1202      def forward(

train_real_world/transformers_4573/src/transformers/models/decision_transformer/configuration_decision_transformer.py:
  83      ```python
  84:     >>> from transformers import DecisionTransformerConfig, DecisionTransformerModel
  85  

train_real_world/transformers_4573/src/transformers/models/decision_transformer/modeling_decision_transformer.py:
   43  
   44: # Copied from transformers.models.gpt2.modeling_gpt2.eager_attention_forward
   45  def eager_attention_forward(module, query, key, value, attention_mask, **kwargs):

   83  
   84: # Copied from transformers.models.gpt2.modeling_gpt2.GPT2Attention with GPT2->DecisionTransformerGPT2
   85  class DecisionTransformerGPT2Attention(nn.Module):

  264  
  265: # Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP with GPT2->DecisionTransformerGPT2
  266  class DecisionTransformerGPT2MLP(nn.Module):

  282  
  283: # Copied from transformers.models.gpt2.modeling_gpt2.GPT2Block with GPT2->DecisionTransformerGPT2
  284  class DecisionTransformerGPT2Block(GradientCheckpointingLayer):

  681          ```python
  682:         >>> from transformers import DecisionTransformerModel
  683          >>> import torch

train_real_world/transformers_4573/src/transformers/models/deepseek_v2/configuration_deepseek_v2.py:
  114      ```python
  115:     >>> from transformers import DeepseekV2Model, DeepseekV2Config
  116      >>> # Initializing a DeepSeek-V2 style configuration

train_real_world/transformers_4573/src/transformers/models/deepseek_v2/modeling_deepseek_v2.py:
  584          ```python
  585:         >>> from transformers import AutoTokenizer, DeepseekV2ForCausalLM
  586  

train_real_world/transformers_4573/src/transformers/models/deepseek_v2/modular_deepseek_v2.py:
  133      ```python
  134:     >>> from transformers import DeepseekV2Model, DeepseekV2Config
  135      >>> # Initializing a DeepSeek-V2 style configuration

train_real_world/transformers_4573/src/transformers/models/deepseek_v3/configuration_deepseek_v3.py:
  122      ```python
  123:     >>> from transformers import DeepseekV3Model, DeepseekV3Config
  124  

train_real_world/transformers_4573/src/transformers/models/deepseek_v3/modeling_deepseek_v3.py:
  679          ```python
  680:         >>> from transformers import AutoTokenizer, DeepseekV3ForCausalLM
  681  

train_real_world/transformers_4573/src/transformers/models/deepseek_vl/configuration_deepseek_vl.py:
  51      ```python
  52:     >>> from transformers import DeepseekVLConfig, DeepseekVLModel
  53  

train_real_world/transformers_4573/src/transformers/models/deepseek_vl/convert_deepseek_vl_weights_to_hf.py:
  26  
  27: from transformers import (
  28      AutoTokenizer,

  33  )
  34: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  35  

train_real_world/transformers_4573/src/transformers/models/deepseek_vl/modular_deepseek_vl.py:
  62      ```python
  63:     >>> from transformers import DeepseekVLConfig, DeepseekVLModel
  64  

train_real_world/transformers_4573/src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py:
  53      ```python
  54:     >>> from transformers import DeepseekVLHybridConfig, DeepseekVLHybridModel
  55  

train_real_world/transformers_4573/src/transformers/models/deepseek_vl_hybrid/convert_deepseek_vl_hybrid_weights_to_hf.py:
  26  
  27: from transformers import (
  28      AutoTokenizer,

  33  )
  34: from transformers.image_utils import (
  35      IMAGENET_STANDARD_MEAN,

train_real_world/transformers_4573/src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py:
  105      ```python
  106:     >>> from transformers import DeepseekVLHybridConfig, DeepseekVLHybridModel
  107  

train_real_world/transformers_4573/src/transformers/models/deformable_detr/configuration_deformable_detr.py:
  133      ```python
  134:     >>> from transformers import DeformableDetrConfig, DeformableDetrModel
  135  

train_real_world/transformers_4573/src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py:
  25  
  26: from transformers import DeformableDetrConfig, DeformableDetrForObjectDetection, DeformableDetrImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/deformable_detr/image_processing_deformable_detr.py:
   138  
   139: # Copied from transformers.models.detr.image_processing_detr.get_image_size_for_max_height_width
   140  def get_image_size_for_max_height_width(

   174  
   175: # Copied from transformers.models.detr.image_processing_detr.safe_squeeze
   176  def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:

   188  
   189: # Copied from transformers.models.detr.image_processing_detr.normalize_annotation
   190  def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:

   203  
   204: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
   205  def max_across_indices(values: Iterable[Any]) -> list[Any]:

   211  
   212: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   213  def get_max_height_width(

   230  
   231: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
   232  def make_pixel_mask(

   249  
   250: # Copied from transformers.models.detr.image_processing_detr.convert_coco_poly_to_mask
   251  def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndarray:

   284  
   285: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_detection_annotation with DETR->DeformableDetr
   286  def prepare_coco_detection_annotation(

   345  
   346: # Copied from transformers.models.detr.image_processing_detr.masks_to_boxes
   347  def masks_to_boxes(masks: np.ndarray) -> np.ndarray:

   380  
   381: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_panoptic_annotation with DETR->DeformableDetr
   382  def prepare_coco_panoptic_annotation(

   422  
   423: # Copied from transformers.models.detr.image_processing_detr.get_segmentation_image
   424  def get_segmentation_image(

   448  
   449: # Copied from transformers.models.detr.image_processing_detr.get_mask_area
   450  def get_mask_area(seg_img: np.ndarray, target_size: tuple[int, int], n_classes: int) -> np.ndarray:

   458  
   459: # Copied from transformers.models.detr.image_processing_detr.score_labels_from_class_probabilities
   460  def score_labels_from_class_probabilities(logits: np.ndarray) -> tuple[np.ndarray, np.ndarray]:

   467  
   468: # Copied from transformers.models.detr.image_processing_detr.post_process_panoptic_sample
   469  def post_process_panoptic_sample(

   553  
   554: # Copied from transformers.models.detr.image_processing_detr.resize_annotation
   555  def resize_annotation(

   605  
   606: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
   607  def binary_mask_to_rle(mask):

   628  
   629: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
   630  def convert_segmentation_to_rle(segmentation):

   650  
   651: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
   652  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

   679  
   680: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
   681  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

   698  
   699: # Copied from transformers.models.detr.image_processing_detr.compute_segments
   700  def compute_segments(

   816  
   817:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__
   818      def __init__(

   876  
   877:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->DeformableDetr
   878      def prepare_annotation(

   909  
   910:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize
   911      def resize(

   970  
   971:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize_annotation
   972      def resize_annotation(

   984  
   985:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
   986      def rescale(

  1013  
  1014:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation
  1015      def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:

  1021  
  1022:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image
  1023      def _update_annotation_for_padded_image(

  1065  
  1066:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._pad_image
  1067      def _pad_image(

  1099  
  1100:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.pad
  1101      def pad(

  1183  
  1184:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.preprocess
  1185      def preprocess(

train_real_world/transformers_4573/src/transformers/models/deformable_detr/modeling_deformable_detr.py:
   243  
   244: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->DeformableDetr
   245  class DeformableDetrFrozenBatchNorm2d(nn.Module):

   283  
   284: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->DeformableDetr
   285  def replace_batch_norm(model):

   368  
   369:     # Copied from transformers.models.detr.modeling_detr.DetrConvEncoder.forward with Detr->DeformableDetr
   370      def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):

   381  
   382: # Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->DeformableDetr
   383  class DeformableDetrConvModel(nn.Module):

   441  
   442: # Copied from transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding
   443  class DeformableDetrLearnedPositionEmbedding(nn.Module):

   465  
   466: # Copied from transformers.models.detr.modeling_detr.build_position_encoding with Detr->DeformableDetr
   467  def build_position_encoding(config):

  1487          ```python
  1488:         >>> from transformers import AutoImageProcessor, DeformableDetrModel
  1489          >>> from PIL import Image

  1673  
  1674: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead
  1675  class DeformableDetrMLPPredictionHead(nn.Module):

  1770          ```python
  1771:         >>> from transformers import AutoImageProcessor, DeformableDetrForObjectDetection
  1772          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/deformable_detr/modular_deformable_detr.py:
  4  
  5: from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast
  6  

train_real_world/transformers_4573/src/transformers/models/deit/configuration_deit.py:
  73      ```python
  74:     >>> from transformers import DeiTConfig, DeiTModel
  75  

train_real_world/transformers_4573/src/transformers/models/deit/convert_deit_timm_to_pytorch.py:
  26  
  27: from transformers import DeiTConfig, DeiTForImageClassificationWithTeacher, DeiTImageProcessor
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/deit/image_processing_deit.py:
  114  
  115:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC
  116      def resize(

train_real_world/transformers_4573/src/transformers/models/deit/modeling_deit.py:
  164  
  165: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  166  def eager_attention_forward(

  194  
  195: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->DeiT
  196  class DeiTSelfAttention(nn.Module):

  245  
  246: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->DeiT
  247  class DeiTSelfOutput(nn.Module):

  263  
  264: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->DeiT
  265  class DeiTAttention(nn.Module):

  276  
  277: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->DeiT
  278  class DeiTIntermediate(nn.Module):

  292  
  293: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViT->DeiT
  294  class DeiTOutput(nn.Module):

  306  
  307: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->DeiT,VIT->DEIT
  308  class DeiTLayer(GradientCheckpointingLayer):

  337  
  338: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->DeiT
  339  class DeiTEncoder(nn.Module):

  448  
  449: # Copied from transformers.models.vit.modeling_vit.ViTPooler with ViT->DeiT
  450  class DeiTPooler(nn.Module):

  509          ```python
  510:         >>> from transformers import AutoImageProcessor, DeiTForMaskedImageModeling
  511          >>> import torch

  607          ```python
  608:         >>> from transformers import AutoImageProcessor, DeiTForImageClassification
  609          >>> import torch

train_real_world/transformers_4573/src/transformers/models/depth_anything/configuration_depth_anything.py:
  75      ```python
  76:     >>> from transformers import DepthAnythingConfig, DepthAnythingForDepthEstimation
  77  

train_real_world/transformers_4573/src/transformers/models/depth_anything/convert_depth_anything_to_hf.py:
  25  
  26: from transformers import DepthAnythingConfig, DepthAnythingForDepthEstimation, Dinov2Config, DPTImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/depth_anything/convert_distill_any_depth_to_hf.py:
  27  
  28: from transformers import DepthAnythingConfig, DepthAnythingForDepthEstimation, Dinov2Config, DPTImageProcessor
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/depth_anything/modeling_depth_anything.py:
   47  
   48:     # Copied from transformers.models.dpt.modeling_dpt.DPTReassembleLayer.forward
   49      def forward(self, hidden_state):

  180  class DepthAnythingFeatureFusionStage(nn.Module):
  181:     # Copied from transformers.models.dpt.modeling_dpt.DPTFeatureFusionStage.__init__ with DPT->DepthAnything
  182      def __init__(self, config: DepthAnythingConfig):

  208  
  209: # Modified from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->DepthAnything,dpt->depth_anything
  210  # avoiding sdpa and flash_attn_2 support, it's done in the backend

  347          ```python
  348:         >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  349          >>> import torch

train_real_world/transformers_4573/src/transformers/models/depth_pro/configuration_depth_pro.py:
  76      ```python
  77:     >>> from transformers import DepthProConfig, DepthProModel
  78  

train_real_world/transformers_4573/src/transformers/models/depth_pro/convert_depth_pro_weights_to_hf.py:
  23  
  24: from transformers import (
  25      DepthProConfig,

train_real_world/transformers_4573/src/transformers/models/depth_pro/image_processing_depth_pro_fast.py:
   99  
  100:     # Copied from transformers.models.depth_pro.image_processing_depth_pro.DepthProImageProcessor.post_process_depth_estimation
  101      def post_process_depth_estimation(

train_real_world/transformers_4573/src/transformers/models/depth_pro/modeling_depth_pro.py:
   656          >>> import requests
   657:         >>> from transformers import AutoProcessor, DepthProModel
   658  

   700  
   701: # Copied from transformers.models.dpt.modeling_dpt.DPTPreActResidualLayer DPT->DepthPro
   702  class DepthProPreActResidualLayer(nn.Module):

   762  
   763: # Modified from transformers.models.dpt.modeling_dpt.DPTFeatureFusionLayer
   764  # except it uses deconv and skip_add and needs no interpolation

   798  
   799: # Modified from transformers.models.dpt.modeling_dpt.DPTFeatureFusionStage with DPT->DepthPro
   800  # with deconv and reversed layers

  1039          ```python
  1040:         >>> from transformers import AutoImageProcessor, DepthProForDepthEstimation
  1041          >>> import torch

train_real_world/transformers_4573/src/transformers/models/detr/configuration_detr.py:
  116      ```python
  117:     >>> from transformers import DetrConfig, DetrModel
  118  

train_real_world/transformers_4573/src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py:
  26  
  27: from transformers import DetrConfig, DetrForObjectDetection, DetrForSegmentation, DetrImageProcessor
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/detr/convert_detr_to_pytorch.py:
  25  
  26: from transformers import DetrConfig, DetrForObjectDetection, DetrForSegmentation, DetrImageProcessor, ResNetConfig
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/detr/image_processing_detr_fast.py:
  628  
  629:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection
  630      def post_process_object_detection(

  682  
  683:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation
  684      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):

  730  
  731:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_instance_segmentation
  732      def post_process_instance_segmentation(

  814  
  815:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_panoptic_segmentation
  816      def post_process_panoptic_segmentation(

train_real_world/transformers_4573/src/transformers/models/detr/image_processing_detr.py:
  200  
  201: # Copied from transformers.models.vilt.image_processing_vilt.max_across_indices
  202  def max_across_indices(values: Iterable[Any]) -> list[Any]:

  208  
  209: # Copied from transformers.models.vilt.image_processing_vilt.get_max_height_width
  210  def get_max_height_width(

  227  
  228: # Copied from transformers.models.vilt.image_processing_vilt.make_pixel_mask
  229  def make_pixel_mask(

train_real_world/transformers_4573/src/transformers/models/detr/modeling_detr.py:
  1100          ```python
  1101:         >>> from transformers import AutoImageProcessor, DetrModel
  1102          >>> from PIL import Image

  1286          ```python
  1287:         >>> from transformers import AutoImageProcessor, DetrForObjectDetection
  1288          >>> import torch

  1441  
  1442:         >>> from transformers import AutoImageProcessor, DetrForSegmentation
  1443:         >>> from transformers.image_transforms import rgb_to_id
  1444  

train_real_world/transformers_4573/src/transformers/models/dia/configuration_dia.py:
  234      ```python
  235:     >>> from transformers import DiaConfig, DiaModel
  236  

train_real_world/transformers_4573/src/transformers/models/dia/convert_dia_to_hf.py:
  24  
  25: from transformers import (
  26      DacModel,

  33  )
  34: from transformers.utils.import_utils import _is_package_available
  35  

train_real_world/transformers_4573/src/transformers/models/dia/processing_dia.py:
  363  
  364:     # Copied from transformers.models.csm.processing_csm.CsmProcessor.save_audio with Csm->Dia
  365      def save_audio(

train_real_world/transformers_4573/src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py:
  19  
  20: from transformers.utils import WEIGHTS_NAME
  21  

train_real_world/transformers_4573/src/transformers/models/diffllama/configuration_diffllama.py:
  88      ```python
  89:     >>> from transformers import DiffLlamaModel, DiffLlamaConfig
  90  

train_real_world/transformers_4573/src/transformers/models/diffllama/modeling_diffllama.py:
  725          ```python
  726:         >>> from transformers import AutoTokenizer, DiffLlamaForCausalLM
  727  

train_real_world/transformers_4573/src/transformers/models/dinat/configuration_dinat.py:
  82      ```python
  83:     >>> from transformers import DinatConfig, DinatModel
  84  

train_real_world/transformers_4573/src/transformers/models/dinat/modeling_dinat.py:
  210  
  211: # Copied from transformers.models.beit.modeling_beit.drop_path
  212  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  226  
  227: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Dinat
  228  class DinatDropPath(nn.Module):

  750          ```python
  751:         >>> from transformers import AutoImageProcessor, AutoBackbone
  752          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dinov2/configuration_dinov2.py:
  90      ```python
  91:     >>> from transformers import Dinov2Config, Dinov2Model
  92  

train_real_world/transformers_4573/src/transformers/models/dinov2/convert_dinov2_to_hf.py:
  30  
  31: from transformers import BitImageProcessor, Dinov2Config, Dinov2ForImageClassification, Dinov2Model
  32: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/dinov2/modeling_dinov2.py:
  152  
  153: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  154  def eager_attention_forward(

  182  
  183: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Dinov2
  184  class Dinov2SelfAttention(nn.Module):

  233  
  234: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Dinov2
  235  class Dinov2SelfOutput(nn.Module):

  251  
  252: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->Dinov2
  253  class Dinov2Attention(nn.Module):

  273  
  274: # Copied from transformers.models.beit.modeling_beit.drop_path
  275  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  289  
  290: # Copied from transformers.models.beit.modeling_beit.BeitDropPath
  291  class Dinov2DropPath(nn.Module):

  574          ```python
  575:         >>> from transformers import AutoImageProcessor, AutoBackbone
  576          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dinov2_with_registers/configuration_dinov2_with_registers.py:
  92      ```python
  93:     >>> from transformers import Dinov2WithRegistersConfig, Dinov2WithRegistersModel
  94  

train_real_world/transformers_4573/src/transformers/models/dinov2_with_registers/convert_dinov2_with_registers_to_hf.py:
  30  
  31: from transformers import (
  32      BitImageProcessor,

  36  )
  37: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
  38: from transformers.utils import logging
  39  

train_real_world/transformers_4573/src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py:
  597          ```python
  598:         >>> from transformers import AutoImageProcessor, AutoBackbone
  599          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py:
  106      ```python
  107:     >>> from transformers import Dinov2WithRegistersConfig, Dinov2WithRegistersModel
  108  

  367          ```python
  368:         >>> from transformers import AutoImageProcessor, AutoBackbone
  369          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dinov3_convnext/configuration_dinov3_convnext.py:
  69      ```python
  70:     >>> from transformers import DINOv3ConvNextConfig, DINOv3ConvNextModel
  71  

train_real_world/transformers_4573/src/transformers/models/dinov3_convnext/convert_dinov3_convnext_to_hf.py:
  30  
  31: from transformers import DINOv3ConvNextConfig, DINOv3ConvNextModel, DINOv3ViTImageProcessorFast
  32  

train_real_world/transformers_4573/src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py:
  35  
  36: # Copied from transformers.models.beit.modeling_beit.drop_path
  37  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  51  
  52: # Copied from transformers.models.convnext.modeling_convnext.ConvNextDropPath with ConvNext->DINOv3ConvNext
  53  class DINOv3ConvNextDropPath(nn.Module):

train_real_world/transformers_4573/src/transformers/models/dinov3_vit/configuration_dinov3_vit.py:
  103      ```python
  104:     >>> from transformers import DINOv3ViTConfig, DINOv3ViTModel
  105  

train_real_world/transformers_4573/src/transformers/models/dinov3_vit/convert_dinov3_vit_to_hf.py:
  30  
  31: from transformers import DINOv3ViTConfig, DINOv3ViTImageProcessorFast, DINOv3ViTModel
  32  

train_real_world/transformers_4573/src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py:
  21  
  22: from transformers.image_processing_base import BatchFeature
  23: from transformers.image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images
  24: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict
  25: from transformers.utils import (
  26      TensorType,

  29  )
  30: from transformers.utils.import_utils import requires
  31  

train_real_world/transformers_4573/src/transformers/models/dinov3_vit/modular_dinov3_vit.py:
  24  
  25: from transformers.models.arcee.modeling_arcee import ArceeMLP
  26: from transformers.models.dinov2.modeling_dinov2 import (
  27      Dinov2DropPath,

  31  )
  32: from transformers.models.llama.modeling_llama import LlamaMLP
  33: from transformers.models.pixtral.modeling_pixtral import PixtralAttention, rotate_half
  34  

train_real_world/transformers_4573/src/transformers/models/distilbert/configuration_distilbert.py:
  68      ```python
  69:     >>> from transformers import DistilBertConfig, DistilBertModel
  70  

train_real_world/transformers_4573/src/transformers/models/distilbert/modeling_distilbert.py:
  123  
  124: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  125  def eager_attention_forward(

  873          ```python
  874:         >>> from transformers import AutoTokenizer, DistilBertForMultipleChoice
  875          >>> import torch

train_real_world/transformers_4573/src/transformers/models/dit/convert_dit_unilm_to_pytorch.py:
  25  
  26: from transformers import BeitConfig, BeitForImageClassification, BeitForMaskedImageModeling, BeitImageProcessor
  27: from transformers.image_utils import PILImageResampling
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/doge/configuration_doge.py:
  98      ```python
  99:     >>> from transformers import DogeConfig, DogeModel
  100  

train_real_world/transformers_4573/src/transformers/models/doge/convert_doge_weights_to_hf.py:
  8  
  9: from transformers import DogeConfig, DogeForCausalLM
  10  

train_real_world/transformers_4573/src/transformers/models/doge/modeling_doge.py:
  771          ```python
  772:         >>> from transformers import AutoTokenizer, DogeForCausalLM
  773  

train_real_world/transformers_4573/src/transformers/models/doge/modular_doge.py:
  127      ```python
  128:     >>> from transformers import DogeConfig, DogeModel
  129  

  683          ```python
  684:         >>> from transformers import AutoTokenizer, DogeForCausalLM
  685  

train_real_world/transformers_4573/src/transformers/models/donut/configuration_donut_swin.py:
  71      ```python
  72:     >>> from transformers import DonutSwinConfig, DonutSwinModel
  73  

train_real_world/transformers_4573/src/transformers/models/donut/convert_donut_to_pytorch.py:
  22  
  23: from transformers import (
  24      DonutImageProcessor,

train_real_world/transformers_4573/src/transformers/models/donut/modeling_donut_swin.py:
   45  )
   46: # Copied from transformers.models.swin.modeling_swin.SwinEncoderOutput with Swin->DonutSwin
   47  class DonutSwinEncoderOutput(ModelOutput):

   68  )
   69: # Copied from transformers.models.swin.modeling_swin.SwinModelOutput with Swin->DonutSwin
   70  class DonutSwinModelOutput(ModelOutput):

   94  )
   95: # Copied from transformers.models.swin.modeling_swin.SwinImageClassifierOutput with Swin->DonutSwin
   96  class DonutSwinImageClassifierOutput(ModelOutput):

  116  
  117: # Copied from transformers.models.swin.modeling_swin.window_partition
  118  def window_partition(input_feature, window_size):

  129  
  130: # Copied from transformers.models.swin.modeling_swin.window_reverse
  131  def window_reverse(windows, window_size, height, width):

  140  
  141: # Copied from transformers.models.swin.modeling_swin.SwinEmbeddings with Swin->DonutSwin
  142  class DonutSwinEmbeddings(nn.Module):

  164  
  165:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
  166      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

  233  
  234: # Copied from transformers.models.swin.modeling_swin.SwinPatchEmbeddings with Swin->DonutSwin
  235  class DonutSwinPatchEmbeddings(nn.Module):

  277  
  278: # Copied from transformers.models.swin.modeling_swin.SwinPatchMerging
  279  class DonutSwinPatchMerging(nn.Module):

  332  
  333: # Copied from transformers.models.beit.modeling_beit.drop_path
  334  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  348  
  349: # Copied from transformers.models.swin.modeling_swin.SwinDropPath
  350  class DonutSwinDropPath(nn.Module):

  363  
  364: # Copied from transformers.models.swin.modeling_swin.SwinSelfAttention with Swin->DonutSwin
  365  class DonutSwinSelfAttention(nn.Module):

  457  
  458: # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput
  459  class DonutSwinSelfOutput(nn.Module):

  471  
  472: # Copied from transformers.models.swin.modeling_swin.SwinAttention with Swin->DonutSwin
  473  class DonutSwinAttention(nn.Module):

  490  
  491: # Copied from transformers.models.swin.modeling_swin.SwinIntermediate
  492  class DonutSwinIntermediate(nn.Module):

  506  
  507: # Copied from transformers.models.swin.modeling_swin.SwinOutput
  508  class DonutSwinOutput(nn.Module):

  519  
  520: # Copied from transformers.models.swin.modeling_swin.SwinLayer with Swin->DonutSwin
  521  class DonutSwinLayer(nn.Module):

  642  
  643: # Copied from transformers.models.swin.modeling_swin.SwinStage with Swin->DonutSwin
  644  class DonutSwinStage(GradientCheckpointingLayer):

  698  
  699: # Copied from transformers.models.swin.modeling_swin.SwinEncoder with Swin->DonutSwin
  700  class DonutSwinEncoder(nn.Module):

  786  @auto_docstring
  787: # Copied from transformers.models.swin.modeling_swin.SwinPreTrainedModel with Swin->DonutSwin,swin->donut
  788  class DonutSwinPreTrainedModel(PreTrainedModel):

  905  )
  906: # Copied from transformers.models.swin.modeling_swin.SwinForImageClassification with Swin->DonutSwin,swin->donut
  907  class DonutSwinForImageClassification(DonutSwinPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/dots1/configuration_dots1.py:
  98          ```python
  99:         >>> from transformers import Dots1Model, Dots1Config
  100  

train_real_world/transformers_4573/src/transformers/models/dots1/modeling_dots1.py:
  614          ```python
  615:         >>> from transformers import AutoTokenizer, Dots1ForCausalLM
  616  

train_real_world/transformers_4573/src/transformers/models/dots1/modular_dots1.py:
  115          ```python
  116:         >>> from transformers import AutoTokenizer, Dots1ForCausalLM
  117  

train_real_world/transformers_4573/src/transformers/models/dpr/configuration_dpr.py:
  73      ```python
  74:     >>> from transformers import DPRConfig, DPRContextEncoder
  75  

train_real_world/transformers_4573/src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py:
  21  
  22: from transformers import BertConfig, DPRConfig, DPRContextEncoder, DPRQuestionEncoder, DPRReader
  23  

train_real_world/transformers_4573/src/transformers/models/dpr/modeling_dpr.py:
  318          ```python
  319:         >>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
  320  

  424          ```python
  425:         >>> from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
  426  

  522          ```python
  523:         >>> from transformers import DPRReader, DPRReaderTokenizer
  524  

train_real_world/transformers_4573/src/transformers/models/dpr/tokenization_dpr_fast.py:
  215          ```python
  216:         >>> from transformers import DPRReader, DPRReaderTokenizer
  217  

train_real_world/transformers_4573/src/transformers/models/dpr/tokenization_dpr.py:
  223          ```python
  224:         >>> from transformers import DPRReader, DPRReaderTokenizer
  225  

train_real_world/transformers_4573/src/transformers/models/dpt/configuration_dpt.py:
  127      ```python
  128:     >>> from transformers import DPTModel, DPTConfig
  129  

train_real_world/transformers_4573/src/transformers/models/dpt/convert_dinov2_depth_to_hf.py:
  27  
  28: from transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation, DPTImageProcessor
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/dpt/convert_dpt_beit_to_hf.py:
  23  
  24: from transformers import BeitConfig, DPTConfig, DPTForDepthEstimation, DPTImageProcessor
  25: from transformers.utils import logging
  26  

train_real_world/transformers_4573/src/transformers/models/dpt/convert_dpt_hybrid_to_pytorch.py:
  25  
  26: from transformers import DPTConfig, DPTForDepthEstimation, DPTForSemanticSegmentation, DPTImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/dpt/convert_dpt_swinv2_to_hf.py:
  23  
  24: from transformers import DPTConfig, DPTForDepthEstimation, DPTImageProcessor, Swinv2Config
  25: from transformers.utils import logging
  26  

train_real_world/transformers_4573/src/transformers/models/dpt/convert_dpt_to_pytorch.py:
  25  
  26: from transformers import DPTConfig, DPTForDepthEstimation, DPTForSemanticSegmentation, DPTImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/dpt/image_processing_dpt.py:
  308  
  309:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label
  310      def reduce_label(self, label: ImageInput) -> np.ndarray:

  451  
  452:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.__call__
  453      def __call__(self, images, segmentation_maps=None, **kwargs):

  607  
  608:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->DPT
  609      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):

train_real_world/transformers_4573/src/transformers/models/dpt/modeling_dpt.py:
   271  
   272: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   273  def eager_attention_forward(

   301  
   302: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->DPT
   303  class DPTSelfAttention(nn.Module):

   352  
   353: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViTConfig->DPTConfig, ViTSelfOutput->DPTViTSelfOutput
   354  class DPTViTSelfOutput(nn.Module):

   370  
   371: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViTConfig->DPTConfig, ViTSelfAttention->DPTSelfAttention, ViTSelfOutput->DPTViTSelfOutput
   372  class DPTViTAttention(nn.Module):

   383  
   384: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViTConfig->DPTConfig, ViTIntermediate->DPTViTIntermediate
   385  class DPTViTIntermediate(nn.Module):

   399  
   400: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViTConfig->DPTConfig, ViTOutput->DPTViTOutput
   401  class DPTViTOutput(nn.Module):

   413  
   414: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViTConfig->DPTConfig, ViTAttention->DPTViTAttention, ViTIntermediate->DPTViTIntermediate, ViTOutput->DPTViTOutput, ViTLayer->DPTViTLayer
   415  class DPTViTLayer(GradientCheckpointingLayer):

   444  
   445: # Copied from transformers.models.dinov2.modeling_dinov2.Dinov2Encoder with Dinov2Config->DPTConfig, Dinov2->DPTViT
   446  class DPTViTEncoder(nn.Module):

   803  
   804: # Copied from transformers.models.vit.modeling_vit.ViTPooler with ViTConfig->DPTConfig, ViTPooler->DPTViTPooler
   805  class DPTViTPooler(nn.Module):

   956          ```python
   957:         >>> from transformers import AutoImageProcessor, DPTForDepthEstimation
   958          >>> import torch

  1106          ```python
  1107:         >>> from transformers import AutoImageProcessor, DPTForSemanticSegmentation
  1108          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/edgetam/configuration_edgetam.py:
  266      ```python
  267:     >>> from transformers import (
  268      ...     EdgeTamVisionConfig,

train_real_world/transformers_4573/src/transformers/models/edgetam/convert_edgetam_to_hf.py:
  29  
  30: from transformers import (
  31      EdgeTamConfig,

train_real_world/transformers_4573/src/transformers/models/edgetam/modeling_edgetam.py:
    32  
    33: from transformers.utils.generic import OutputRecorder
    34  

  1093          >>> import requests
  1094:         >>> from transformers import AutoModel, AutoProcessor
  1095  

train_real_world/transformers_4573/src/transformers/models/edgetam_video/configuration_edgetam_video.py:
  267      ```python
  268:     >>> from transformers import (
  269      ...     EdgeTamVisionConfig,

train_real_world/transformers_4573/src/transformers/models/edgetam_video/convert_edgetam_video_to_hf.py:
  29  
  30: from transformers import (
  31      EdgeTamVideoConfig,

train_real_world/transformers_4573/src/transformers/models/edgetam_video/modeling_edgetam_video.py:
  34  
  35: from transformers.utils.generic import OutputRecorder
  36  

train_real_world/transformers_4573/src/transformers/models/edgetam_video/modular_edgetam_video.py:
   25  
   26: from transformers.models.sam2.modeling_sam2 import (
   27      eager_attention_forward,

   29  )
   30: from transformers.utils.generic import OutputRecorder
   31  

  192      ```python
  193:     >>> from transformers import (
  194      ...     EdgeTamVisionConfig,

train_real_world/transformers_4573/src/transformers/models/efficientloftr/configuration_efficientloftr.py:
  82          ```python
  83:         >>> from transformers import EfficientLoFTRConfig, EfficientLoFTRForKeypointMatching
  84  

train_real_world/transformers_4573/src/transformers/models/efficientloftr/convert_efficientloftr_to_hf.py:
  22  
  23: from transformers.models.efficientloftr.image_processing_efficientloftr import EfficientLoFTRImageProcessor
  24: from transformers.models.efficientloftr.modeling_efficientloftr import (
  25      EfficientLoFTRConfig,

train_real_world/transformers_4573/src/transformers/models/efficientloftr/image_processing_efficientloftr.py:
   61  
   62: # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale
   63  def is_grayscale(

   76  
   77: # Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale
   78  def convert_to_grayscale(

  114  
  115: # Copied from transformers.models.superglue.image_processing_superglue.validate_and_format_image_pairs
  116  def validate_and_format_image_pairs(images: ImageInput):

  190  
  191:     # Copied from transformers.models.superpoint.image_processing_superpoint.SuperPointImageProcessor.resize
  192      def resize(

  230  
  231:     # Copied from transformers.models.superglue.image_processing_superglue.SuperGlueImageProcessor.preprocess
  232      def preprocess(

train_real_world/transformers_4573/src/transformers/models/efficientloftr/modeling_efficientloftr.py:
    88  
    89: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->EfficientLoFTR
    90  class EfficientLoFTRRotaryEmbedding(nn.Module):

   163  
   164: # Copied from transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ConvNormLayer with RTDetrV2->EfficientLoFTR
   165  class EfficientLoFTRConvNormLayer(nn.Module):

   293  
   294: # Copied from transformers.models.cohere.modeling_cohere.rotate_half
   295  def rotate_half(x):

   302  
   303: # Copied from transformers.models.cohere.modeling_cohere.apply_rotary_pos_emb
   304  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   333  
   334: # Copied from transformers.models.cohere.modeling_cohere.repeat_kv
   335  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   346  
   347: # Copied from transformers.models.llama.modeling_llama.eager_attention_forward
   348  def eager_attention_forward(

   703  
   704:     # Copied from transformers.models.superpoint.modeling_superpoint.SuperPointPreTrainedModel.extract_one_channel_pixel_values with SuperPoint->EfficientLoFTR
   705      def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:

   749          ```python
   750:         >>> from transformers import AutoImageProcessor, AutoModel
   751          >>> import torch

  1316          ```python
  1317:         >>> from transformers import AutoImageProcessor, AutoModel
  1318          >>> import torch

train_real_world/transformers_4573/src/transformers/models/efficientnet/configuration_efficientnet.py:
  81      ```python
  82:     >>> from transformers import EfficientNetConfig, EfficientNetModel
  83  

train_real_world/transformers_4573/src/transformers/models/efficientnet/convert_efficientnet_to_pytorch.py:
  31  
  32: from transformers import (
  33      EfficientNetConfig,

  36  )
  37: from transformers.utils import logging
  38  

train_real_world/transformers_4573/src/transformers/models/efficientnet/image_processing_efficientnet.py:
  135  
  136:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.NEAREST
  137      def resize(

train_real_world/transformers_4573/src/transformers/models/electra/configuration_electra.py:
  96      ```python
  97:     >>> from transformers import ElectraConfig, ElectraModel
  98  

train_real_world/transformers_4573/src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import ElectraConfig, ElectraForMaskedLM, ElectraForPreTraining
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/electra/modeling_electra.py:
    76  
    77:     # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward
    78      def forward(

   120  
   121: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   122  def eager_attention_forward(

   150  
   151: # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Electra
   152  class ElectraSelfAttention(nn.Module):

   224  
   225: # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->Electra
   226  class ElectraCrossAttention(nn.Module):

   302  
   303: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
   304  class ElectraSelfOutput(nn.Module):

   317  
   318: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Electra,BERT->ELECTRA
   319  class ElectraAttention(nn.Module):

   349  
   350: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
   351  class ElectraIntermediate(nn.Module):

   365  
   366: # Copied from transformers.models.bert.modeling_bert.BertOutput
   367  class ElectraOutput(nn.Module):

   380  
   381: # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Electra
   382  class ElectraLayer(GradientCheckpointingLayer):

   448  
   449: # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Electra
   450  class ElectraEncoder(nn.Module):

   661  
   662:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
   663      def _create_attention_masks(

   720  
   721: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Electra
   722  class ElectraSequenceSummary(nn.Module):

   938          ```python
   939:         >>> from transformers import ElectraForPreTraining, AutoTokenizer
   940          >>> import torch

  1346          ```python
  1347:         >>> from transformers import AutoTokenizer, ElectraForCausalLM, ElectraConfig
  1348          >>> import torch

train_real_world/transformers_4573/src/transformers/models/emu3/configuration_emu3.py:
   61      ```python
   62:     >>> from transformers import Emu3VQVAE, Emu3VQVAEConfig
   63  

  175      ```python
  176:     >>> from transformers import Emu3Model, Emu3Config
  177  

train_real_world/transformers_4573/src/transformers/models/emu3/convert_emu3_weights_to_hf.py:
  23  
  24: from transformers import (
  25      AutoModel,

  34  )
  35: from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode
  36  

  48  ```py
  49: from transformers import Emu3ForConditionalGeneration, Emu3Processor
  50  

train_real_world/transformers_4573/src/transformers/models/emu3/modeling_emu3.py:
  1302          ```python
  1303:         >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration
  1304          >>> import torch

  1549          ```python
  1550:         >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration
  1551          >>> import torch

train_real_world/transformers_4573/src/transformers/models/emu3/modular_emu3.py:
   880          ```python
   881:         >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration
   882          >>> import torch

  1101          ```python
  1102:         >>> from transformers import Emu3Processor, Emu3ForConditionalGeneration
  1103          >>> import torch

train_real_world/transformers_4573/src/transformers/models/encodec/configuration_encodec.py:
  93      ```python
  94:     >>> from transformers import EncodecModel, EncodecConfig
  95  

train_real_world/transformers_4573/src/transformers/models/encodec/convert_encodec_checkpoint_to_pytorch.py:
  20  
  21: from transformers import (
  22      EncodecConfig,

train_real_world/transformers_4573/src/transformers/models/encodec/modeling_encodec.py:
  778          >>> from datasets import load_dataset
  779:         >>> from transformers import AutoProcessor, EncodecModel
  780  

train_real_world/transformers_4573/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py:
  46      ```python
  47:     >>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel
  48  

train_real_world/transformers_4573/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py:
  230          ```python
  231:         >>> from transformers import EncoderDecoderModel
  232  

  367          ```python
  368:         >>> from transformers import EncoderDecoderModel, BertTokenizer
  369          >>> import torch

train_real_world/transformers_4573/src/transformers/models/eomt/configuration_eomt.py:
  92      ```python
  93:     >>> from transformers import EomtConfig, EomtForUniversalSegmentation
  94  

train_real_world/transformers_4573/src/transformers/models/eomt/convert_eomt_to_hf.py:
  25  
  26: from transformers import EomtConfig, EomtForUniversalSegmentation, EomtImageProcessorFast
  27  

train_real_world/transformers_4573/src/transformers/models/eomt/image_processing_eomt.py:
  72  
  73: # Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks
  74  def convert_segmentation_map_to_binary_masks(

train_real_world/transformers_4573/src/transformers/models/eomt/modular_eomt.py:
  119      ```python
  120:     >>> from transformers import EomtConfig, EomtForUniversalSegmentation
  121  

train_real_world/transformers_4573/src/transformers/models/ernie/configuration_ernie.py:
  78      ```python
  79:     >>> from transformers import ErnieConfig, ErnieModel
  80  

train_real_world/transformers_4573/src/transformers/models/ernie/modeling_ernie.py:
   834          ```python
   835:         >>> from transformers import AutoTokenizer, ErnieForPreTraining
   836          >>> import torch

  1147          ```python
  1148:         >>> from transformers import AutoTokenizer, ErnieForNextSentencePrediction
  1149          >>> import torch

train_real_world/transformers_4573/src/transformers/models/ernie/modular_ernie.py:
  294  
  295:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
  296      def _create_attention_masks(

  374          ```python
  375:         >>> from transformers import AutoTokenizer, ErnieForPreTraining
  376          >>> import torch

  577          ```python
  578:         >>> from transformers import AutoTokenizer, ErnieForNextSentencePrediction
  579          >>> import torch

train_real_world/transformers_4573/src/transformers/models/ernie4_5/configuration_ernie4_5.py:
  80      ```python
  81:     >>> from transformers import Ernie4_5Model, Ernie4_5Config
  82  

train_real_world/transformers_4573/src/transformers/models/ernie4_5/convert_ernie4_5_tokenizer.py:
  16  
  17: from transformers import LlamaTokenizer, LlamaTokenizerFast
  18  

train_real_world/transformers_4573/src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py:
  102      ```python
  103:     >>> from transformers import Ernie4_5_MoeModel, Ernie4_5_MoEConfig
  104  

train_real_world/transformers_4573/src/transformers/models/ernie4_5_vl_moe/configuration_ernie4_5_vl_moe.py:
  277      ```python
  278:     >>> from transformers import Ernie4_5_VL_MoeForConditionalGeneration, Ernie4_5_VL_MoeConfig
  279  

train_real_world/transformers_4573/src/transformers/models/ernie4_5_vl_moe/convert_ernie4_5_vl_moe_to_hf.py:
  25  
  26: from transformers import (
  27      AutoTokenizer,

train_real_world/transformers_4573/src/transformers/models/ernie4_5_vl_moe/modular_ernie4_5_vl_moe.py:
  346      ```python
  347:     >>> from transformers import Ernie4_5_VL_MoeForConditionalGeneration, Ernie4_5_VL_MoeConfig
  348  

train_real_world/transformers_4573/src/transformers/models/esm/configuration_esm.py:
  235      ```python
  236:     >>> from transformers import EsmModel, EsmConfig
  237  

train_real_world/transformers_4573/src/transformers/models/esm/convert_esm.py:
  26  
  27: from transformers.models.esm.configuration_esm import EsmConfig, EsmFoldConfig
  28: from transformers.models.esm.modeling_esm import (
  29      EsmForMaskedLM,

  36  )
  37: from transformers.models.esm.modeling_esmfold import EsmForProteinFolding
  38: from transformers.models.esm.tokenization_esm import EsmTokenizer
  39: from transformers.utils import logging
  40  

train_real_world/transformers_4573/src/transformers/models/esm/modeling_esm.py:
  256  
  257: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  258  def eager_attention_forward(

  518  
  519: # Copied from transformers.models.bert.modeling_bert.BertPooler
  520  class EsmPooler(nn.Module):

  677  
  678:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
  679      def _create_attention_masks(

train_real_world/transformers_4573/src/transformers/models/esm/modeling_esmfold.py:
  2073          ```python
  2074:         >>> from transformers import AutoTokenizer, EsmForProteinFolding
  2075  

train_real_world/transformers_4573/src/transformers/models/evolla/configuration_evolla.py:
  178      ```python
  179:     >>> from transformers import EvollaModel, EvollaConfig
  180  

train_real_world/transformers_4573/src/transformers/models/evolla/modeling_evolla.py:
  1445          ```python
  1446:         >>> from transformers import EvollaProcessor, EvollaForProteinText2Text
  1447          >>> model = EvollaForProteinText2Text.from_pretrained("westlake/Evolla-10B-hf")

train_real_world/transformers_4573/src/transformers/models/evolla/modular_evolla.py:
  907          ```python
  908:         >>> from transformers import EvollaProcessor, EvollaForProteinText2Text
  909          >>> model = EvollaForProteinText2Text.from_pretrained("westlake/Evolla-10B-hf")

train_real_world/transformers_4573/src/transformers/models/exaone4/configuration_exaone4.py:
   99      ```python
  100:     >>> from transformers import Exaone4Model, Exaone4Config
  101  

train_real_world/transformers_4573/src/transformers/models/exaone4/modeling_exaone4.py:
   28  
   29: from transformers.utils.generic import check_model_inputs
   30  

  495          ```python
  496:         >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  497          >>> model = AutoModelForCausalLM.from_pretrained("LGAI-EXAONE/EXAONE-4.0-32B")

train_real_world/transformers_4573/src/transformers/models/exaone4/modular_exaone4.py:
   23  
   24: from transformers.utils.generic import check_model_inputs
   25  

  132      ```python
  133:     >>> from transformers import Exaone4Model, Exaone4Config
  134  

  426          ```python
  427:         >>> from transformers import AutoModelForCausalLM, AutoTokenizer
  428          >>> model = AutoModelForCausalLM.from_pretrained("LGAI-EXAONE/EXAONE-4.0-32B")

train_real_world/transformers_4573/src/transformers/models/falcon/configuration_falcon.py:
  96      ```python
  97:     >>> from transformers import FalconModel, FalconConfig
  98  

train_real_world/transformers_4573/src/transformers/models/falcon/modeling_falcon.py:
   70  
   71: # Copied from transformers.models.llama.modeling_llama.rotate_half
   72  def rotate_half(x):

   78  
   79: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
   80  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  106  
  107: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Falcon
  108  class FalconRotaryEmbedding(nn.Module):

  200  
  201: # Copied from transformers.models.bloom.modeling_bloom.dropout_add
  202  def dropout_add(x: torch.Tensor, residual: torch.Tensor, prob: float, training: bool) -> torch.Tensor:

  294  
  295:     # Copied from transformers.models.bloom.modeling_bloom.BloomAttention._merge_heads
  296      def _merge_heads(self, x: torch.Tensor) -> torch.Tensor:

  688  
  689:     # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa
  690      @classmethod

  936      @staticmethod
  937:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  938      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/falcon_h1/convert_mamba_ssm_checkpoint.py:
  20  
  21: from transformers import AutoModelForCausalLM, AutoTokenizer, FalconH1Config, FalconH1ForCausalLM
  22  

train_real_world/transformers_4573/src/transformers/models/falcon_h1/modeling_falcon_h1.py:
    33  
    34: from transformers.activations import ACT2FN
    35  

   542  
   543: # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer
   544  class FalconH1Mixer(nn.Module):

  1251  @auto_docstring
  1252: # Adapted from transformers.models.jamba.modeling_jamba.JambaModel
  1253  class FalconH1Model(FalconH1PreTrainedModel):

  1539          ```python
  1540:         >>> from transformers import AutoTokenizer, FalconH1ForCausalLM
  1541  

train_real_world/transformers_4573/src/transformers/models/falcon_h1/modular_falcon_h1.py:
    28  
    29: from transformers.activations import ACT2FN
    30: from transformers.models.jamba.modeling_jamba import HybridMambaAttentionDynamicCache
    31: from transformers.models.llama.modeling_llama import (
    32      LlamaAttention,

    39  )
    40: from transformers.models.mamba2.modeling_mamba2 import (
    41      MambaRMSNormGated,

   289  
   290: # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer
   291  class FalconH1Mixer(nn.Module):

   974  @auto_docstring
   975: # Adapted from transformers.models.jamba.modeling_jamba.JambaModel
   976  class FalconH1Model(FalconH1PreTrainedModel):

  1246          ```python
  1247:         >>> from transformers import AutoTokenizer, FalconH1ForCausalLM
  1248  

train_real_world/transformers_4573/src/transformers/models/falcon_mamba/configuration_falcon_mamba.py:
  92      ```python
  93:     >>> from transformers import FalconMambaConfig, FalconMambaModel
  94  

train_real_world/transformers_4573/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py:
  69          >>> import torch
  70:         >>> from transformers import AutoTokenizer, FalconMambaForCausalLM, FalconMambaCache
  71  

train_real_world/transformers_4573/src/transformers/models/falcon_mamba/modular_falcon_mamba.py:
  120      ```python
  121:     >>> from transformers import FalconMambaConfig, FalconMambaModel
  122  

  216          >>> import torch
  217:         >>> from transformers import AutoTokenizer, FalconMambaForCausalLM, FalconMambaCache
  218  

train_real_world/transformers_4573/src/transformers/models/fast_vlm/configuration_fast_vlm.py:
  57      ```python
  58:     >>> from transformers import FastVlmForConditionalGeneration, FastVlmConfig
  59  

train_real_world/transformers_4573/src/transformers/models/fast_vlm/convert_fastvlm_weights_to_hf.py:
  24  
  25: from transformers import (
  26      AddedToken,

train_real_world/transformers_4573/src/transformers/models/fast_vlm/modeling_fast_vlm.py:
  351          >>> import requests
  352:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  353          >>> import torch

train_real_world/transformers_4573/src/transformers/models/fast_vlm/modular_fast_vlm.py:
   65      ```python
   66:     >>> from transformers import FastVlmForConditionalGeneration, FastVlmConfig
   67  

  240          >>> import requests
  241:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  242          >>> import torch

train_real_world/transformers_4573/src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py:
  155      ```python
  156:     >>> from transformers import FastSpeech2ConformerModel, FastSpeech2ConformerConfig
  157  

  371      ```python
  372:     >>> from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig
  373  

  438      ```python
  439:     >>> from transformers import (
  440      ...     FastSpeech2ConformerConfig,

train_real_world/transformers_4573/src/transformers/models/fastspeech2_conformer/convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py:
  25  
  26: from transformers import (
  27      FastSpeech2ConformerConfig,

train_real_world/transformers_4573/src/transformers/models/fastspeech2_conformer/convert_hifigan.py:
  22  
  23: from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig, logging
  24  

train_real_world/transformers_4573/src/transformers/models/fastspeech2_conformer/convert_model_with_hifigan.py:
  20  
  21: from transformers import (
  22      FastSpeech2ConformerConfig,

train_real_world/transformers_4573/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py:
   197  
   198: # Copied from transformers.models.speecht5.modeling_speecht5.SpeechT5BatchNormConvLayer
   199  class FastSpeech2ConformerBatchNormConvLayer(nn.Module):

  1153          ```python
  1154:         >>> from transformers import (
  1155          ...     FastSpeech2ConformerTokenizer,

  1308  
  1309: # Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
  1310  class HifiGanResidualBlock(nn.Module):

  1376  )
  1377: # Copied from transformers.models.speecht5.modeling_speecht5.SpeechT5HifiGan with SpeechT5->FastSpeech2Conformer
  1378  class FastSpeech2ConformerHifiGan(PreTrainedModel):

  1551          ```python
  1552:         >>> from transformers import (
  1553          ...     FastSpeech2ConformerTokenizer,

train_real_world/transformers_4573/src/transformers/models/flaubert/modeling_flaubert.py:
    47  
    48: # Copied from transformers.models.xlm.modeling_xlm.create_sinusoidal_embeddings
    49  def create_sinusoidal_embeddings(n_pos, dim, out):

    57  
    58: # Copied from transformers.models.xlm.modeling_xlm.get_masks
    59  def get_masks(slen, lengths, causal, padding_mask=None):

    83  
    84: # Copied from transformers.models.xlm.modeling_xlm.MultiHeadAttention
    85  class MultiHeadAttention(nn.Module):

   165  
   166: # Copied from transformers.models.xlm.modeling_xlm.TransformerFFN
   167  class TransformerFFN(nn.Module):

   192  )
   193: # Copied from transformers.models.xlm.modeling_xlm.XLMPredLayer with XLM->Flaubert
   194  class FlaubertPredLayer(nn.Module):

   241  )
   242: # Copied from transformers.models.xlm.modeling_xlm.XLMSquadHeadOutput with XLM->Flaubert
   243  class FlaubertSquadHeadOutput(ModelOutput):

   268  
   269: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerStartLogits with XLM->Flaubert
   270  class FlaubertPoolerStartLogits(nn.Module):

   307  
   308: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerEndLogits with XLM->Flaubert
   309  class FlaubertPoolerEndLogits(nn.Module):

   377  
   378: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerAnswerClass with XLM->Flaubert
   379  class FlaubertPoolerAnswerClass(nn.Module):

   443  
   444: # Copied from transformers.models.xlm.modeling_xlm.XLMSQuADHead with XLM->Flaubert
   445  class FlaubertSQuADHead(nn.Module):

   556  
   557: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Flaubert
   558  class FlaubertSequenceSummary(nn.Module):

   657  @auto_docstring
   658: # Copied from transformers.models.xlm.modeling_xlm.XLMPreTrainedModel with XLM->Flaubert
   659  class FlaubertPreTrainedModel(PreTrainedModel):

   770  
   771:     # Copied from transformers.models.xlm.modeling_xlm.XLMModel.get_input_embeddings
   772      def get_input_embeddings(self):

   774  
   775:     # Copied from transformers.models.xlm.modeling_xlm.XLMModel.set_input_embeddings
   776      def set_input_embeddings(self, new_embeddings):

  1065  )
  1066: # Copied from transformers.models.xlm.modeling_xlm.XLMForSequenceClassification with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert
  1067  class FlaubertForSequenceClassification(FlaubertPreTrainedModel):

  1171  @auto_docstring
  1172: # Copied from transformers.models.xlm.modeling_xlm.XLMForTokenClassification with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert
  1173  class FlaubertForTokenClassification(FlaubertPreTrainedModel):

  1264  )
  1265: # Copied from transformers.models.xlm.modeling_xlm.XLMForQuestionAnsweringSimple with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert
  1266  class FlaubertForQuestionAnsweringSimple(FlaubertPreTrainedModel):

  1399  @auto_docstring
  1400: # Copied from transformers.models.xlm.modeling_xlm.XLMForQuestionAnswering with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert
  1401  class FlaubertForQuestionAnswering(FlaubertPreTrainedModel):

  1459          ```python
  1460:         >>> from transformers import AutoTokenizer, FlaubertForQuestionAnswering
  1461          >>> import torch

  1518  @auto_docstring
  1519: # Copied from transformers.models.xlm.modeling_xlm.XLMForMultipleChoice with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert
  1520  class FlaubertForMultipleChoice(FlaubertPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/flaubert/tokenization_flaubert.py:
   50  
   51: # Copied from transformers.models.xlm.tokenization_xlm.get_pairs
   52  def get_pairs(word):

   64  
   65: # Copied from transformers.models.xlm.tokenization_xlm.replace_unicode_punct
   66  def replace_unicode_punct(text):

  108  
  109: # Copied from transformers.models.xlm.tokenization_xlm.remove_non_printing_char
  110  def remove_non_printing_char(text):

  262      @property
  263:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.do_lower_case
  264      def do_lower_case(self):

  266  
  267:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_punct_norm
  268      def moses_punct_norm(self, text, lang):

  275  
  276:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_tokenize
  277      def moses_tokenize(self, text, lang):

  284  
  285:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_pipeline
  286      def moses_pipeline(self, text, lang):

  291  
  292:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.ja_tokenize
  293      def ja_tokenize(self, text):

  314      @property
  315:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.vocab_size
  316      def vocab_size(self):

  318  
  319:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_vocab
  320      def get_vocab(self):

  322  
  323:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.bpe
  324      def bpe(self, token):

  414  
  415:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_token_to_id
  416      def _convert_token_to_id(self, token):

  419  
  420:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_id_to_token
  421      def _convert_id_to_token(self, index):

  424  
  425:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.convert_tokens_to_string
  426      def convert_tokens_to_string(self, tokens):

  430  
  431:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.build_inputs_with_special_tokens
  432      def build_inputs_with_special_tokens(

  458  
  459:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_special_tokens_mask
  460      def get_special_tokens_mask(

  487  
  488:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.save_vocabulary
  489      def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:

  516  
  517:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.__getstate__
  518      def __getstate__(self):

  522  
  523:     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.__setstate__
  524      def __setstate__(self, d):

train_real_world/transformers_4573/src/transformers/models/flava/configuration_flava.py:
   74      ```python
   75:     >>> from transformers import FlavaImageConfig, FlavaImageModel
   76  

  181      ```python
  182:     >>> from transformers import FlavaTextConfig, FlavaTextModel
  183  

  273      ```python
  274:     >>> from transformers import FlavaMultimodalConfig, FlavaMultimodalModel
  275  

  353      ```python
  354:     >>> from transformers import FlavaImageCodebookConfig, FlavaImageCodebook
  355  

  441      ```python
  442:     >>> from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining
  443  

train_real_world/transformers_4573/src/transformers/models/flava/convert_dalle_to_flava_codebook.py:
  20  
  21: from transformers import FlavaImageCodebook, FlavaImageCodebookConfig
  22  

train_real_world/transformers_4573/src/transformers/models/flava/convert_flava_original_pytorch_to_hf.py:
  20  
  21: from transformers import FlavaConfig, FlavaForPreTraining
  22: from transformers.models.flava.convert_dalle_to_flava_codebook import convert_dalle_checkpoint
  23  

train_real_world/transformers_4573/src/transformers/models/flava/image_processing_flava.py:
  424  
  425:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC
  426      def resize(

train_real_world/transformers_4573/src/transformers/models/flava/modeling_flava.py:
   255  
   256:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   257      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   539  
   540:     # Copied from transformers.models.vit.modeling_vit.ViTIntermediate.forward
   541      def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:

   553  
   554:     # Copied from transformers.models.vit.modeling_vit.ViTOutput.forward
   555      def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:

  1027          >>> import torch
  1028:         >>> from transformers import AutoProcessor, FlavaModel
  1029  

  1071          >>> import torch
  1072:         >>> from transformers import AutoProcessor, FlavaModel
  1073:         >>> from transformers.image_utils import load_image
  1074  

  1138          >>> import requests
  1139:         >>> from transformers import AutoProcessor, FlavaModel
  1140  

  1368          >>> import requests
  1369:         >>> from transformers import AutoImageProcessor, FlavaImageCodebook
  1370  

  1401          >>> import requests
  1402:         >>> from transformers import AutoImageProcessor, FlavaImageCodebook
  1403  

  1636          >>> import requests
  1637:         >>> from transformers import FlavaForPreTraining, AutoProcessor
  1638  

train_real_world/transformers_4573/src/transformers/models/flex_olmo/configuration_flex_olmo.py:
  97      ```python
  98:     >>> from transformers import FlexOlmoModel, FlexOlmoConfig
  99  

train_real_world/transformers_4573/src/transformers/models/flex_olmo/modeling_flex_olmo.py:
  649          ```python
  650:         >>> from transformers import AutoTokenizer, FlexOlmoForCausalLM
  651  

train_real_world/transformers_4573/src/transformers/models/flex_olmo/modular_flex_olmo.py:
  108      ```python
  109:     >>> from transformers import FlexOlmoModel, FlexOlmoConfig
  110  

train_real_world/transformers_4573/src/transformers/models/florence2/configuration_florence2.py:
   78      ```python
   79:     >>> from transformers import Florence2VisionConfig, Florence2VisionModel
   80  

  160      ```python
  161:     >>> from transformers import Florence2ForConditionalGeneration, Florence2Config, CLIPVisionConfig, BartConfig
  162  

train_real_world/transformers_4573/src/transformers/models/florence2/convert_florence2_original_pytorch_to_hf.py:
  19  
  20: from transformers import (
  21      AddedToken,

train_real_world/transformers_4573/src/transformers/models/florence2/modeling_florence2.py:
  870          >>> import requests
  871:         >>> from transformers import AutoProcessor, Florence2ForConditionalGeneration
  872  

train_real_world/transformers_4573/src/transformers/models/florence2/modular_florence2.py:
    99      ```python
   100:     >>> from transformers import Florence2VisionConfig, Florence2VisionModel
   101  

   181      ```python
   182:     >>> from transformers import Florence2ForConditionalGeneration, Florence2Config, CLIPVisionConfig, BartConfig
   183  

  1676          >>> import requests
  1677:         >>> from transformers import AutoProcessor, Florence2ForConditionalGeneration
  1678  

train_real_world/transformers_4573/src/transformers/models/fnet/configuration_fnet.py:
  69      ```python
  70:     >>> from transformers import FNetConfig, FNetModel
  71  

train_real_world/transformers_4573/src/transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py:
  21  
  22: from transformers import FNetConfig, FNetForPreTraining
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/fnet/modeling_fnet.py:
  205  
  206: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->FNet
  207  class FNetIntermediate(nn.Module):

  221  
  222: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->FNet
  223  class FNetOutput(nn.Module):

  290  
  291: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->FNet
  292  class FNetPooler(nn.Module):

  306  
  307: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->FNet
  308  class FNetPredictionHeadTransform(nn.Module):

  347  
  348: # Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->FNet
  349  class FNetOnlyNSPHead(nn.Module):

  358  
  359: # Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->FNet
  360  class FNetPreTrainingHeads(nn.Module):

  567          ```python
  568:         >>> from transformers import AutoTokenizer, FNetForPreTraining
  569          >>> import torch

  715          ```python
  716:         >>> from transformers import AutoTokenizer, FNetForNextSentencePrediction
  717          >>> import torch

train_real_world/transformers_4573/src/transformers/models/focalnet/configuration_focalnet.py:
  93      ```python
  94:     >>> from transformers import FocalNetConfig, FocalNetModel
  95  

train_real_world/transformers_4573/src/transformers/models/focalnet/convert_focalnet_to_hf_format.py:
  25  
  26: from transformers import BitImageProcessor, FocalNetConfig, FocalNetForImageClassification
  27: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
  28  

train_real_world/transformers_4573/src/transformers/models/focalnet/modeling_focalnet.py:
  245  
  246: # Copied from transformers.models.beit.modeling_beit.drop_path
  247  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  261  
  262: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->FocalNet
  263  class FocalNetDropPath(nn.Module):

  721          ```python
  722:         >>> from transformers import AutoImageProcessor, FocalNetConfig, FocalNetForMaskedImageModeling
  723          >>> import torch

  794  class FocalNetForImageClassification(FocalNetPreTrainedModel):
  795:     # Copied from transformers.models.swin.modeling_swin.SwinForImageClassification.__init__ with Swin->FocalNet, swin->focalnet
  796      def __init__(self, config):

  882          ```python
  883:         >>> from transformers import AutoImageProcessor, AutoBackbone
  884          >>> import torch

train_real_world/transformers_4573/src/transformers/models/fsmt/configuration_fsmt.py:
  122      ```python
  123:     >>> from transformers import FSMTConfig, FSMTModel
  124  

train_real_world/transformers_4573/src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py:
  33  
  34: from transformers import FSMTConfig, FSMTForConditionalGeneration
  35: from transformers.models.fsmt.tokenization_fsmt import VOCAB_FILES_NAMES
  36: from transformers.tokenization_utils_base import TOKENIZER_CONFIG_FILE
  37: from transformers.utils import WEIGHTS_NAME, logging
  38  

train_real_world/transformers_4573/src/transformers/models/fsmt/modeling_fsmt.py:
  1007          ```python
  1008:         >>> from transformers import AutoTokenizer, FSMTForConditionalGeneration
  1009  

train_real_world/transformers_4573/src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import FunnelBaseModel, FunnelConfig, FunnelModel
  23: from transformers.models.funnel.modeling_funnel import FunnelPositionwiseFFN, FunnelRelMultiheadAttention
  24: from transformers.utils import logging
  25  

train_real_world/transformers_4573/src/transformers/models/funnel/modeling_funnel.py:
  940          ```python
  941:         >>> from transformers import AutoTokenizer, FunnelForPreTraining
  942          >>> import torch

train_real_world/transformers_4573/src/transformers/models/fuyu/configuration_fuyu.py:
  91      ```python
  92:     >>> from transformers import FuyuConfig
  93  

train_real_world/transformers_4573/src/transformers/models/fuyu/convert_fuyu_model_weights_to_hf.py:
  21  
  22: from transformers import FuyuConfig, FuyuForCausalLM, LlamaTokenizer
  23  

  25  try:
  26:     from transformers import LlamaTokenizerFast
  27  

  39  ```py
  40: from transformers import FuyuForCausalLM, FuyuTokenizer
  41  

train_real_world/transformers_4573/src/transformers/models/fuyu/modeling_fuyu.py:
  290          ```python
  291:         >>> from transformers import FuyuProcessor, FuyuForCausalLM
  292          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/gemma/configuration_gemma.py:
  89      ```python
  90:     >>> from transformers import GemmaModel, GemmaConfig
  91      >>> # Initializing a Gemma gemma-7b style configuration

train_real_world/transformers_4573/src/transformers/models/gemma/convert_gemma_weights_to_hf.py:
  19  
  20: from transformers import GemmaConfig, GemmaForCausalLM, GemmaTokenizer
  21  

  23  try:
  24:     from transformers import GemmaTokenizerFast
  25  except ImportError as e:

  42  ```py
  43: from transformers import GemmaForCausalLM, GemmaTokenizerFast
  44  

train_real_world/transformers_4573/src/transformers/models/gemma/modeling_gemma.py:
  483          ```python
  484:         >>> from transformers import AutoTokenizer, GemmaForCausalLM
  485  

train_real_world/transformers_4573/src/transformers/models/gemma/modular_gemma.py:
  114      ```python
  115:     >>> from transformers import GemmaModel, GemmaConfig
  116      >>> # Initializing a Gemma gemma-7b style configuration

  312          ```python
  313:         >>> from transformers import AutoTokenizer, GemmaForCausalLM
  314  

train_real_world/transformers_4573/src/transformers/models/gemma2/configuration_gemma2.py:
  100      ```python
  101:     >>> from transformers import Gemma2Model, Gemma2Config
  102      >>> # Initializing a Gemma2 gemma2-7b style configuration

train_real_world/transformers_4573/src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py:
  19  
  20: from transformers import Gemma2Config, Gemma2ForCausalLM, GemmaTokenizer
  21  

  23  try:
  24:     from transformers import GemmaTokenizerFast
  25  except ImportError as e:

  42  ```py
  43: from transformers import Gemma2ForCausalLM, GemmaTokenizerFast
  44  

train_real_world/transformers_4573/src/transformers/models/gemma2/modeling_gemma2.py:
  512          ```python
  513:         >>> from transformers import AutoTokenizer, Gemma2ForCausalLM
  514  

train_real_world/transformers_4573/src/transformers/models/gemma2/modular_gemma2.py:
  128      ```python
  129:     >>> from transformers import Gemma2Model, Gemma2Config
  130      >>> # Initializing a Gemma2 gemma2-7b style configuration

  516          ```python
  517:         >>> from transformers import AutoTokenizer, Gemma2ForCausalLM
  518  

train_real_world/transformers_4573/src/transformers/models/gemma3/configuration_gemma3.py:
  106      ```python
  107:     >>> from transformers import Gemma3TextModel, Gemma3TextConfig
  108      >>> # Initializing a Gemma3Text gemma3_text-7b style configuration

  262      ```python
  263:     >>> from transformers import Gemma3ForConditionalGeneration, Gemma3Config, SiglipVisionConfig, Gemma3TextConfig
  264  

train_real_world/transformers_4573/src/transformers/models/gemma3/convert_gemma3_weights.py:
  36  
  37: from transformers import (
  38      Gemma3Config,

  48  )
  49: from transformers.image_utils import PILImageResampling
  50  

train_real_world/transformers_4573/src/transformers/models/gemma3/modeling_gemma3.py:
   634          ```python
   635:         >>> from transformers import AutoTokenizer, Gemma3ForCausalLM
   636  

   906          >>> import requests
   907:         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration
   908  

  1045          >>> import requests
  1046:         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration
  1047  

train_real_world/transformers_4573/src/transformers/models/gemma3/modular_gemma3.py:
  135      ```python
  136:     >>> from transformers import Gemma3TextModel, Gemma3TextConfig
  137      >>> # Initializing a Gemma3Text gemma3_text-7b style configuration

  276      ```python
  277:     >>> from transformers import Gemma3ForConditionalGeneration, Gemma3Config, SiglipVisionConfig, Gemma3TextConfig
  278  

  927          >>> import requests
  928:         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration
  929  

train_real_world/transformers_4573/src/transformers/models/gemma3n/configuration_gemma3n.py:
  130      ```python
  131:     >>> from transformers import Gemma3nTextModel, Gemma3nTextConfig
  132  

  357      ```python
  358:     >>> from transformers import Gemma3nAudioConfig, Gemma3nAudioEncoder
  359  

  455      ```python
  456:     >>> from transformers import Gemma3nVisionConfig, TimmWrapper
  457  

  589      ```python
  590:     >>> from transformers import Gemma3nForConditionalGeneration, Gemma3nConfig, Gemma3nTextConfig
  591  

train_real_world/transformers_4573/src/transformers/models/gemma3n/convert_gemma3n_weights.py:
  38  
  39: from transformers import (
  40      Gemma3nAudioConfig,

  50  )
  51: from transformers.image_utils import PILImageResampling
  52  

train_real_world/transformers_4573/src/transformers/models/gemma3n/modeling_gemma3n.py:
  1809          ```python
  1810:         >>> from transformers import AutoTokenizer, Gemma3nForCausalLM
  1811  

  2035          >>> import requests
  2036:         >>> from transformers import AutoProcessor, Gemma3nForConditionalGeneration
  2037  

  2226          >>> import requests
  2227:         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration
  2228  

train_real_world/transformers_4573/src/transformers/models/gemma3n/modular_gemma3n.py:
   157      ```python
   158:     >>> from transformers import Gemma3nTextModel, Gemma3nTextConfig
   159  

   369      ```python
   370:     >>> from transformers import Gemma3nAudioConfig, Gemma3nAudioEncoder
   371  

   467      ```python
   468:     >>> from transformers import Gemma3nVisionConfig, TimmWrapper
   469  

   546      ```python
   547:     >>> from transformers import Gemma3nForConditionalGeneration, Gemma3nConfig, Gemma3nTextConfig
   548  

  2297          >>> import requests
  2298:         >>> from transformers import AutoProcessor, Gemma3nForConditionalGeneration
  2299  

  2472          >>> import requests
  2473:         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration
  2474  

train_real_world/transformers_4573/src/transformers/models/git/configuration_git.py:
   59      ```python
   60:     >>> from transformers import GitVisionConfig, GitVisionModel
   61  

  150      ```python
  151:     >>> from transformers import GitConfig, GitModel
  152  

train_real_world/transformers_4573/src/transformers/models/git/convert_git_to_pytorch.py:
  29  
  30: from transformers import (
  31      AutoTokenizer,

  38  )
  39: from transformers.utils import logging
  40  

train_real_world/transformers_4573/src/transformers/models/git/modeling_git.py:
    59  )
    60: # Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Git
    61  class GitVisionModelOutput(ModelOutput):

    72  
    73: # Copied from transformers.models.gemma3.modeling_gemma3.token_type_ids_mask_function
    74  def token_type_ids_mask_function(

   113  
   114: # Copied from transformers.models.gemma3.modeling_gemma3.create_causal_mask_mapping
   115  def create_causal_mask_mapping(

   297  
   298: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
   299  class GitSelfOutput(nn.Module):

   341  
   342: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
   343  class GitIntermediate(nn.Module):

   357  
   358: # Copied from transformers.models.bert.modeling_bert.BertOutput
   359  class GitOutput(nn.Module):

   508  
   509: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Git
   510  class GitVisionEmbeddings(nn.Module):

   607  
   608: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   609  def eager_attention_forward(

   704  
   705: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->GitVision
   706  class GitVisionEncoderLayer(GradientCheckpointingLayer):

   755  
   756: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->GitVision, CLIPConfig
   757  class GitVisionEncoder(nn.Module):

   844  class GitVisionTransformer(nn.Module):
   845:     # Copied from transformers.models.altclip.modeling_altclip.AltCLIPVisionTransformer.__init__ with AltCLIPEncoder->GitVisionEncoder, AltCLIP->Git
   846      def __init__(self, config: GitVisionConfig):

   907  
   908:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP->Git
   909      def __init__(self, config: GitVisionConfig):

   933          >>> import requests
   934:         >>> from transformers import AutoProcessor, GitVisionModel
   935  

  1022          ```python
  1023:         >>> from transformers import AutoProcessor, AutoModel
  1024          >>> import requests

  1215          ```python
  1216:         >>> from transformers import AutoProcessor, AutoModelForCausalLM
  1217          >>> import requests

  1236          ```python
  1237:         >>> from transformers import AutoProcessor, AutoModelForCausalLM
  1238          >>> from huggingface_hub import hf_hub_download

  1266          >>> from huggingface_hub import hf_hub_download
  1267:         >>> from transformers import AutoProcessor, AutoModelForCausalLM
  1268  

train_real_world/transformers_4573/src/transformers/models/glm/configuration_glm.py:
  80      ```python
  81:     >>> from transformers import GlmModel, GlmConfig
  82      >>> # Initializing a Glm glm-4-9b-chat style configuration

train_real_world/transformers_4573/src/transformers/models/glm/convert_glm_weights_to_hf.py:
   9  
  10: from transformers import GlmConfig, GlmForCausalLM, PreTrainedTokenizerFast
  11  

train_real_world/transformers_4573/src/transformers/models/glm/modeling_glm.py:
  486          ```python
  487:         >>> from transformers import AutoTokenizer, GlmForCausalLM
  488  

train_real_world/transformers_4573/src/transformers/models/glm4/configuration_glm4.py:
  80      ```python
  81:     >>> from transformers import Glm4Model, Glm4Config
  82      >>> # Initializing a Glm4 glm4-4-9b-chat style configuration

train_real_world/transformers_4573/src/transformers/models/glm4/convert_glm4_weights_to_hf.py:
   9  
  10: from transformers import Glm4Config, Glm4ForCausalLM, PreTrainedTokenizerFast
  11  

train_real_world/transformers_4573/src/transformers/models/glm4/modeling_glm4.py:
  495          ```python
  496:         >>> from transformers import AutoTokenizer, Glm4ForCausalLM
  497  

train_real_world/transformers_4573/src/transformers/models/glm4/modular_glm4.py:
  106          ```python
  107:         >>> from transformers import AutoTokenizer, Glm4ForCausalLM
  108  

train_real_world/transformers_4573/src/transformers/models/glm46v/configuration_glm46v.py:
  55      ```python
  56:     >>> from transformers import Glm46VForConditionalGeneration, Glm46VConfig
  57  

train_real_world/transformers_4573/src/transformers/models/glm46v/modeling_glm46v.py:
  570          >>> import requests
  571:         >>> from transformers import AutoProcessor, Glm46VForConditionalGeneration
  572  

train_real_world/transformers_4573/src/transformers/models/glm46v/modular_glm46v.py:
  58      ```python
  59:     >>> from transformers import Glm46VForConditionalGeneration, Glm46VConfig
  60  

train_real_world/transformers_4573/src/transformers/models/glm4_moe/configuration_glm4_moe.py:
  100      ```python
  101:     >>> from transformers import Glm4MoeModel, Glm4MoeConfig
  102  

train_real_world/transformers_4573/src/transformers/models/glm4_moe/modeling_glm4_moe.py:
  623          ```python
  624:         >>> from transformers import AutoTokenizer, Glm4MoeForCausalLM
  625  

train_real_world/transformers_4573/src/transformers/models/glm4_moe/modular_glm4_moe.py:
  114      ```python
  115:     >>> from transformers import Glm4MoeModel, Glm4MoeConfig
  116  

train_real_world/transformers_4573/src/transformers/models/glm4v/configuration_glm4v.py:
   66      ```python
   67:     >>> from transformers import Glm4vVisionConfig, Glm4vVisionModel
   68  

  169      ```python
  170:     >>> from transformers import Glm4vTextModel, Glm4vConfig
  171  

  271      ```python
  272:     >>> from transformers import Glm4vForConditionalGeneration, Glm4vConfig
  273  

train_real_world/transformers_4573/src/transformers/models/glm4v/modeling_glm4v.py:
  1426          >>> import requests
  1427:         >>> from transformers import AutoProcessor, Glm4vForConditionalGeneration
  1428  

train_real_world/transformers_4573/src/transformers/models/glm4v/modular_glm4v.py:
   104      ```python
   105:     >>> from transformers import Glm4vVisionConfig, Glm4vVisionModel
   106  

   207      ```python
   208:     >>> from transformers import Glm4vTextModel, Glm4vConfig
   209  

   309      ```python
   310:     >>> from transformers import Glm4vForConditionalGeneration, Glm4vConfig
   311  

  1351          >>> import requests
  1352:         >>> from transformers import AutoProcessor, Glm4vForConditionalGeneration
  1353  

train_real_world/transformers_4573/src/transformers/models/glm4v_moe/configuration_glm4v_moe.py:
   66      ```python
   67:     >>> from transformers import Glm4vMoeVisionConfig, Glm4vMoeVisionModel
   68  

  191      ```python
  192:     >>> from transformers import Glm4vMoeTextModel, Glm4vMoeConfig
  193  

  317      ```python
  318:     >>> from transformers import Glm4vMoeForConditionalGeneration, Glm4vMoeConfig
  319  

train_real_world/transformers_4573/src/transformers/models/glm4v_moe/modeling_glm4v_moe.py:
  1686          >>> import requests
  1687:         >>> from transformers import AutoProcessor, Glm4vMoeForConditionalGeneration
  1688  

train_real_world/transformers_4573/src/transformers/models/glm4v_moe/modular_glm4v_moe.py:
  142      ```python
  143:     >>> from transformers import Glm4vMoeTextModel, Glm4vMoeConfig
  144  

  265      ```python
  266:     >>> from transformers import Glm4vMoeForConditionalGeneration, Glm4vMoeConfig
  267  

train_real_world/transformers_4573/src/transformers/models/glpn/configuration_glpn.py:
  76      ```python
  77:     >>> from transformers import GLPNModel, GLPNConfig
  78  

train_real_world/transformers_4573/src/transformers/models/glpn/convert_glpn_to_pytorch.py:
  23  
  24: from transformers import GLPNConfig, GLPNForDepthEstimation, GLPNImageProcessor
  25: from transformers.utils import logging
  26  

train_real_world/transformers_4573/src/transformers/models/glpn/modeling_glpn.py:
   32  
   33: # Copied from transformers.models.beit.modeling_beit.drop_path
   34  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   48  
   49: # Copied from transformers.models.segformer.modeling_segformer.SegformerDropPath
   50  class GLPNDropPath(nn.Module):

   63  
   64: # Copied from transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings
   65  class GLPNOverlapPatchEmbeddings(nn.Module):

   89  
   90: # Copied from transformers.models.segformer.modeling_segformer.SegformerEfficientSelfAttention
   91  class GLPNEfficientSelfAttention(nn.Module):

  179  
  180: # Copied from transformers.models.segformer.modeling_segformer.SegformerSelfOutput
  181  class GLPNSelfOutput(nn.Module):

  192  
  193: # Copied from transformers.models.segformer.modeling_segformer.SegformerAttention with Segformer->GLPN
  194  class GLPNAttention(nn.Module):

  212  
  213: # Copied from transformers.models.segformer.modeling_segformer.SegformerDWConv
  214  class GLPNDWConv(nn.Module):

  227  
  228: # Copied from transformers.models.segformer.modeling_segformer.SegformerMixFFN with Segformer->GLPN
  229  class GLPNMixFFN(nn.Module):

  251  
  252: # Copied from transformers.models.segformer.modeling_segformer.SegformerLayer with Segformer->GLPN
  253  class GLPNLayer(nn.Module):

  394  class GLPNModel(GLPNPreTrainedModel):
  395:     # Copied from transformers.models.segformer.modeling_segformer.SegformerModel.__init__ with Segformer->GLPN
  396      def __init__(self, config):

  406      @auto_docstring
  407:     # Copied from transformers.models.segformer.modeling_segformer.SegformerModel.forward
  408      def forward(

  609          ```python
  610:         >>> from transformers import AutoImageProcessor, GLPNForDepthEstimation
  611          >>> import torch

train_real_world/transformers_4573/src/transformers/models/got_ocr2/configuration_got_ocr2.py:
  144      ```python
  145:     >>> from transformers import GotOcr2ForConditionalGeneration, GotOcr2Config
  146  

train_real_world/transformers_4573/src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py:
  25  
  26: from transformers import (
  27      GotOcr2Config,

  33  )
  34: from transformers.convert_slow_tokenizer import TikTokenConverter
  35: from transformers.tokenization_python import AddedToken
  36  

  38  if is_vision_available():
  39:     from transformers.image_utils import load_image
  40  

train_real_world/transformers_4573/src/transformers/models/got_ocr2/modeling_got_ocr2.py:
   30  
   31: from transformers.utils.generic import check_model_inputs
   32  

  725          >>> import requests
  726:         >>> from transformers import AutoProcessor, GotOcr2ForConditionalGeneration, TextStreamer
  727  

train_real_world/transformers_4573/src/transformers/models/got_ocr2/modular_got_ocr2.py:
  164      ```python
  165:     >>> from transformers import GotOcr2ForConditionalGeneration, GotOcr2Config
  166  

  412          >>> import requests
  413:         >>> from transformers import AutoProcessor, GotOcr2ForConditionalGeneration, TextStreamer
  414  

train_real_world/transformers_4573/src/transformers/models/gpt2/configuration_gpt2.py:
  106      ```python
  107:     >>> from transformers import GPT2Config, GPT2Model
  108  

train_real_world/transformers_4573/src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import GPT2Config, GPT2Model
  23: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, logging
  24  

train_real_world/transformers_4573/src/transformers/models/gpt2/modeling_gpt2.py:
  367  
  368: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->GPT2
  369  class GPT2SequenceSummary(nn.Module):

  918          >>> import torch
  919:         >>> from transformers import AutoTokenizer, GPT2DoubleHeadsModel
  920  

train_real_world/transformers_4573/src/transformers/models/gpt2/tokenization_gpt2.py:
  41      ```python
  42:     >>> from transformers import GPT2Tokenizer
  43  

train_real_world/transformers_4573/src/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py:
  75      ```python
  76:     >>> from transformers import GPTBigCodeConfig, GPTBigCodeModel
  77  

train_real_world/transformers_4573/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:
  256  
  257:     # Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP.forward
  258      def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.FloatTensor:

train_real_world/transformers_4573/src/transformers/models/gpt_neo/configuration_gpt_neo.py:
  83      ```python
  84:     >>> from transformers import GPTNeoConfig, GPTNeoModel
  85  

train_real_world/transformers_4573/src/transformers/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py:
  23  
  24: from transformers import GPTNeoConfig, GPTNeoForCausalLM
  25: from transformers.utils import logging
  26  

train_real_world/transformers_4573/src/transformers/models/gpt_neo/modeling_gpt_neo.py:
  533  
  534:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  535      def _update_causal_mask(

  603      @staticmethod
  604:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  605      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/gpt_neox/configuration_gpt_neox.py:
  83      ```python
  84:     >>> from transformers import GPTNeoXConfig, GPTNeoXModel
  85  

train_real_world/transformers_4573/src/transformers/models/gpt_neox/modeling_gpt_neox.py:
  565          ```python
  566:         >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig
  567          >>> import torch

train_real_world/transformers_4573/src/transformers/models/gpt_neox/modular_gpt_neox.py:
  438          ```python
  439:         >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig
  440          >>> import torch

train_real_world/transformers_4573/src/transformers/models/gpt_neox/tokenization_gpt_neox.py:
  39      ```python
  40:     >>> from transformers import GPTNeoXTokenizer
  41  

train_real_world/transformers_4573/src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py:
  71      ```python
  72:     >>> from transformers import GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel
  73  

train_real_world/transformers_4573/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py:
   62  
   63: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoXJapanese
   64  class GPTNeoXJapaneseRotaryEmbedding(nn.Module):

  135  
  136: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
  137  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  440          ```python
  441:         >>> from transformers import AutoTokenizer, GPTNeoXJapaneseModel
  442          >>> import torch

  520  
  521:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  522      def _update_causal_mask(

  590      @staticmethod
  591:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  592      def _prepare_4d_causal_attention_mask_with_cache_position(

  696          ```python
  697:         >>> from transformers import AutoTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
  698          >>> import torch

train_real_world/transformers_4573/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py:
  72      ```python
  73:     >>> from transformers import GPTNeoXJapaneseTokenizer
  74  

train_real_world/transformers_4573/src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py:
  26  
  27: from transformers import (
  28      GenerationConfig,

  32  )
  33: from transformers.convert_slow_tokenizer import TikTokenConverter
  34  

train_real_world/transformers_4573/src/transformers/models/gpt_oss/modeling_gpt_oss.py:
  676          ```python
  677:         >>> from transformers import AutoTokenizer, GptOssForCausalLM
  678  

train_real_world/transformers_4573/src/transformers/models/gpt_sw3/convert_megatron_to_pytorch.py:
  21  
  22: from transformers import GPT2Config
  23  

train_real_world/transformers_4573/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py:
  29      ```python
  30:     >>> from transformers import GPTSw3Tokenizer
  31  

train_real_world/transformers_4573/src/transformers/models/gptj/configuration_gptj.py:
  67      ```python
  68:     >>> from transformers import GPTJModel, GPTJConfig
  69  

train_real_world/transformers_4573/src/transformers/models/granite/configuration_granite.py:
  96      ```python
  97:     >>> from transformers import GraniteModel, GraniteConfig
  98  

train_real_world/transformers_4573/src/transformers/models/granite/modeling_granite.py:
  540          ```python
  541:         >>> from transformers import AutoTokenizer, GraniteForCausalLM
  542  

train_real_world/transformers_4573/src/transformers/models/granite_speech/configuration_granite_speech.py:
   61      ```python
   62:     >>> from transformers import GraniteSpeechEncoderConfig, GraniteSpeechCTCEncoder
   63  

  136      ```python
  137:     >>> from transformers import GraniteSpeechConfig, GraniteSpeechForConditionalGeneration
  138  

train_real_world/transformers_4573/src/transformers/models/granitemoe/configuration_granitemoe.py:
  100      ```python
  101:     >>> from transformers import GraniteMoeModel, GraniteMoeConfig
  102  

train_real_world/transformers_4573/src/transformers/models/granitemoe/modeling_granitemoe.py:
  681          ```python
  682:         >>> from transformers import AutoTokenizer, GraniteMoeForCausalLM
  683  

train_real_world/transformers_4573/src/transformers/models/granitemoe/modular_granitemoe.py:
  262          ```python
  263:         >>> from transformers import AutoTokenizer, GraniteMoeForCausalLM
  264  

train_real_world/transformers_4573/src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py:
  118      ```python
  119:     >>> from transformers import GraniteMoeHybridModel, GraniteMoeHybridConfig
  120  

train_real_world/transformers_4573/src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py:
    28  
    29: from transformers.activations import ACT2FN
    30  

   366  
   367: # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer
   368  class GraniteMoeHybridMambaLayer(nn.Module):

  1483          ```python
  1484:         >>> from transformers import AutoTokenizer, GraniteMoeHybridForCausalLM
  1485  

train_real_world/transformers_4573/src/transformers/models/granitemoeshared/configuration_granitemoeshared.py:
  102      ```python
  103:     >>> from transformers import GraniteMoeSharedModel, GraniteMoeSharedConfig
  104  

train_real_world/transformers_4573/src/transformers/models/granitemoeshared/modeling_granitemoeshared.py:
  752          ```python
  753:         >>> from transformers import AutoTokenizer, GraniteMoeSharedForCausalLM
  754  

train_real_world/transformers_4573/src/transformers/models/grounding_dino/configuration_grounding_dino.py:
  135      ```python
  136:     >>> from transformers import GroundingDinoConfig, GroundingDinoModel
  137  

train_real_world/transformers_4573/src/transformers/models/grounding_dino/convert_grounding_dino_to_hf.py:
  25  
  26: from transformers import (
  27      AutoTokenizer,

train_real_world/transformers_4573/src/transformers/models/grounding_dino/image_processing_grounding_dino.py:
   146  
   147: # Copied from transformers.models.detr.image_processing_detr.get_image_size_for_max_height_width
   148  def get_image_size_for_max_height_width(

   182  
   183: # Copied from transformers.models.detr.image_processing_detr.safe_squeeze
   184  def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:

   196  
   197: # Copied from transformers.models.detr.image_processing_detr.normalize_annotation
   198  def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:

   211  
   212: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
   213  def max_across_indices(values: Iterable[Any]) -> list[Any]:

   219  
   220: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   221  def get_max_height_width(

   238  
   239: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
   240  def make_pixel_mask(

   257  
   258: # Copied from transformers.models.detr.image_processing_detr.convert_coco_poly_to_mask
   259  def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndarray:

   292  
   293: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_detection_annotation with DETR->GroundingDino
   294  def prepare_coco_detection_annotation(

   353  
   354: # Copied from transformers.models.detr.image_processing_detr.masks_to_boxes
   355  def masks_to_boxes(masks: np.ndarray) -> np.ndarray:

   388  
   389: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_panoptic_annotation with DETR->GroundingDino
   390  def prepare_coco_panoptic_annotation(

   430  
   431: # Copied from transformers.models.detr.image_processing_detr.get_segmentation_image
   432  def get_segmentation_image(

   456  
   457: # Copied from transformers.models.detr.image_processing_detr.get_mask_area
   458  def get_mask_area(seg_img: np.ndarray, target_size: tuple[int, int], n_classes: int) -> np.ndarray:

   466  
   467: # Copied from transformers.models.detr.image_processing_detr.score_labels_from_class_probabilities
   468  def score_labels_from_class_probabilities(logits: np.ndarray) -> tuple[np.ndarray, np.ndarray]:

   475  
   476: # Copied from transformers.models.detr.image_processing_detr.post_process_panoptic_sample
   477  def post_process_panoptic_sample(

   561  
   562: # Copied from transformers.models.detr.image_processing_detr.resize_annotation
   563  def resize_annotation(

   613  
   614: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
   615  def binary_mask_to_rle(mask):

   636  
   637: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
   638  def convert_segmentation_to_rle(segmentation):

   658  
   659: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
   660  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

   687  
   688: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
   689  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

   706  
   707: # Copied from transformers.models.detr.image_processing_detr.compute_segments
   708  def compute_segments(

   767  
   768: # Copied from transformers.models.owlvit.image_processing_owlvit._scale_boxes
   769  def _scale_boxes(boxes, target_sizes):

   853  
   854:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__
   855      def __init__(

   913  
   914:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->GroundingDino
   915      def prepare_annotation(

   946  
   947:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize
   948      def resize(

  1007  
  1008:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize_annotation
  1009      def resize_annotation(

  1021  
  1022:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
  1023      def rescale(

  1050  
  1051:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation
  1052      def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:

  1058  
  1059:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image
  1060      def _update_annotation_for_padded_image(

  1102  
  1103:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._pad_image
  1104      def _pad_image(

  1136  
  1137:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.pad
  1138      def pad(

  1220  
  1221:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.preprocess
  1222      def preprocess(

  1469  
  1470:     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection with OwlViT->GroundingDino
  1471      def post_process_object_detection(

train_real_world/transformers_4573/src/transformers/models/grounding_dino/modeling_grounding_dino.py:
    45  @use_kernel_forward_from_hub("MultiScaleDeformableAttention")
    46: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention
    47  class MultiScaleDeformableAttention(nn.Module):

   300  
   301: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->GroundingDino
   302  class GroundingDinoFrozenBatchNorm2d(nn.Module):

   340  
   341: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->GroundingDino
   342  def replace_batch_norm(model):

   414  
   415:     # Copied from transformers.models.detr.modeling_detr.DetrConvEncoder.forward with Detr->GroundingDino
   416      def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):

   427  
   428: # Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->GroundingDino
   429  class GroundingDinoConvModel(nn.Module):

   515  
   516: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->GroundingDino, Deformable DETR->Grounding DINO
   517  class GroundingDinoMultiscaleDeformableAttention(nn.Module):

   859  
   860: # Copied from transformers.models.beit.modeling_beit.drop_path
   861  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   875  
   876: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->GroundingDino
   877  class GroundingDinoDropPath(nn.Module):

  2077          ```python
  2078:         >>> from transformers import AutoProcessor, AutoModel
  2079          >>> from PIL import Image

  2322  
  2323: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead
  2324  class GroundingDinoMLPPredictionHead(nn.Module):

  2491          >>> from PIL import Image
  2492:         >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  2493  

train_real_world/transformers_4573/src/transformers/models/grounding_dino/modular_grounding_dino.py:
  24  
  25: from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast
  26  

train_real_world/transformers_4573/src/transformers/models/groupvit/configuration_groupvit.py:
   66      ```python
   67:     >>> from transformers import GroupViTTextConfig, GroupViTTextModel
   68  

  160      ```python
  161:     >>> from transformers import GroupViTVisionConfig, GroupViTVisionModel
  162  

train_real_world/transformers_4573/src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py:
  27  
  28: from transformers import CLIPProcessor, GroupViTConfig, GroupViTModel
  29  

train_real_world/transformers_4573/src/transformers/models/groupvit/modeling_groupvit.py:
    43  
    44: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit
    45  def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:

   418  
   419: # Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT
   420  class GroupViTTextEmbeddings(nn.Module):

   693  
   694: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->GroupViT
   695  class GroupViTEncoderLayer(GradientCheckpointingLayer):

  1058          ```python
  1059:         >>> from transformers import CLIPTokenizer, GroupViTTextModel
  1060  

  1161          >>> import requests
  1162:         >>> from transformers import AutoProcessor, GroupViTVisionModel
  1163  

  1247          >>> import torch
  1248:         >>> from transformers import CLIPTokenizer, GroupViTModel
  1249  

  1276          >>> import torch
  1277:         >>> from transformers import AutoProcessor, GroupViTModel
  1278:         >>> from transformers.image_utils import load_image
  1279  

  1319          >>> import requests
  1320:         >>> from transformers import AutoProcessor, GroupViTModel
  1321  

train_real_world/transformers_4573/src/transformers/models/helium/configuration_helium.py:
  84      ```python
  85:     >>> from transformers import HeliumModel, HeliumConfig
  86      >>> # Initializing a Helium 2b style configuration

train_real_world/transformers_4573/src/transformers/models/helium/modeling_helium.py:
  470          ```python
  471:         >>> from transformers import AutoTokenizer, HeliumForCausalLM
  472  

train_real_world/transformers_4573/src/transformers/models/hgnet_v2/modeling_hgnet_v2.py:
  369          ```python
  370:         >>> from transformers import HGNetV2Config, HGNetV2Backbone
  371          >>> import torch

  454          >>> import requests
  455:         >>> from transformers import HGNetV2ForImageClassification, AutoImageProcessor
  456          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/hgnet_v2/modular_hgnet_v2.py:
  492          ```python
  493:         >>> from transformers import HGNetV2Config, HGNetV2Backbone
  494          >>> import torch

  577          >>> import requests
  578:         >>> from transformers import HGNetV2ForImageClassification, AutoImageProcessor
  579          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/hiera/configuration_hiera.py:
  101      ```python
  102:     >>> from transformers import HieraConfig, HieraModel
  103  

train_real_world/transformers_4573/src/transformers/models/hiera/convert_hiera_to_hf.py:
  29  
  30: from transformers import BitImageProcessor, HieraConfig, HieraForImageClassification, HieraForPreTraining, HieraModel
  31: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
  32: from transformers.utils import logging
  33  

train_real_world/transformers_4573/src/transformers/models/hiera/modeling_hiera.py:
   397  
   398: # Copied from transformers.models.beit.modeling_beit.drop_path
   399  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   413  
   414: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Hiera
   415  class HieraDropPath(nn.Module):

  1143          ```python
  1144:         >>> from transformers import AutoImageProcessor, HieraForPreTraining
  1145          >>> import torch

  1338          ```python
  1339:         >>> from transformers import AutoImageProcessor, AutoBackbone
  1340          >>> import torch

train_real_world/transformers_4573/src/transformers/models/hubert/configuration_hubert.py:
  147      ```python
  148:     >>> from transformers import HubertModel, HubertConfig
  149  

train_real_world/transformers_4573/src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py:
  21  
  22: from transformers import HubertConfig, HubertModel, Wav2Vec2FeatureExtractor, logging
  23  

train_real_world/transformers_4573/src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import (
  26      HubertConfig,

train_real_world/transformers_4573/src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py:
  20  
  21: from transformers import HubertConfig, HubertForSequenceClassification, Wav2Vec2FeatureExtractor, logging
  22  

train_real_world/transformers_4573/src/transformers/models/hubert/modeling_hubert.py:
  908          ```python
  909:         >>> from transformers import AutoProcessor, HubertModel
  910          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/hubert/modular_hubert.py:
  242          ```python
  243:         >>> from transformers import AutoProcessor, HubertModel
  244          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py:
   27  
   28: from transformers.cache_utils import Cache
   29  

  484          ```python
  485:         >>> from transformers import AutoTokenizer, HunYuanDenseV1ForCausalLM
  486  

train_real_world/transformers_4573/src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py:
  22  
  23: from transformers.cache_utils import Cache
  24: from transformers.utils import (
  25      logging,

train_real_world/transformers_4573/src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py:
  577          ```python
  578:         >>> from transformers import AutoTokenizer, HunYuanMoEV1ForCausalLM
  579  

train_real_world/transformers_4573/src/transformers/models/idefics/configuration_idefics.py:
  222      ```python
  223:     >>> from transformers import IdeficsModel, IdeficsConfig
  224  

train_real_world/transformers_4573/src/transformers/models/idefics/modeling_idefics.py:
   454  
   455: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   456  def eager_attention_forward(

  1163          ```python
  1164:         >>> from transformers import AutoProcessor, IdeficsForVisionText2Text
  1165  

train_real_world/transformers_4573/src/transformers/models/idefics/vision.py:
   68  
   69: # Adapted from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings
   70  class IdeficsVisionEmbeddings(nn.Module):

  168  
  169: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
  170  def eager_attention_forward(

  265  
  266: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->IdeficsVision
  267  class IdeficsVisionMLP(nn.Module):

  281  
  282: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->IdeficsVision
  283  class IdeficsVisionEncoderLayer(GradientCheckpointingLayer):

  332  
  333: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->IdeficsVision
  334  class IdeficsVisionEncoder(nn.Module):

  420  
  421: # Adapted from transformers.models.clip.modeling_clip.CLIPVisionTransformer
  422  class IdeficsVisionTransformer(nn.Module):

  432  
  433:     # Adapted from transformers.models.clip.modeling_clip.CLIPVisionTransformer.forward
  434      def forward(

train_real_world/transformers_4573/src/transformers/models/idefics2/configuration_idefics2.py:
   62      ```python
   63:     >>> from transformers.models.idefics2.modeling_idefics2 import Idefics2VisionTransformer
   64:     >>> from transformers.models.idefics2.configuration_idefics2 import Idefics2VisionConfig
   65  

  196      ```python
  197:     >>> from transformers import Idefics2Model, Idefics2Config
  198      >>> # Initializing configuration

train_real_world/transformers_4573/src/transformers/models/idefics2/convert_idefics2_weights_to_hf.py:
  19  
  20: from transformers import (
  21      AutoConfig,

train_real_world/transformers_4573/src/transformers/models/idefics2/image_processing_idefics2.py:
   89  
   90: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
   91  def max_across_indices(values: Iterable[Any]) -> list[Any]:

  115  
  116: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
  117  def make_pixel_mask(

  264  
  265:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor._pad_image
  266      def _pad_image(

train_real_world/transformers_4573/src/transformers/models/idefics2/modeling_idefics2.py:
    79  )
    80: # Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->Idefics2
    81  class Idefics2CausalLMOutputWithPast(ModelOutput):

   205  
   206: # Copied from transformers.models.siglip.modeling_siglip.SiglipAttention with Siglip->Idefics2Vision
   207  class Idefics2VisionAttention(nn.Module):

   209  
   210:     # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__
   211      def __init__(self, config):

   271  
   272: # Copied from transformers.models.siglip.modeling_siglip.SiglipMLP with Siglip->Idefics2Vision
   273  class Idefics2VisionMLP(nn.Module):

   305  
   306: # Copied from transformers.models.siglip.modeling_siglip.SiglipMultiheadAttentionPoolingHead with Siglip->Idefics2
   307  class Idefics2MultiheadAttentionPoolingHead(nn.Module):

   346      @auto_docstring
   347:     # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoderLayer.forward
   348      def forward(

   371  
   372: # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoder with Siglip->Idefics2
   373  class Idefics2Encoder(nn.Module):

   509  
   510: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   511  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   522  
   523: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Idefics2
   524  class Idefics2RMSNorm(nn.Module):

  1036  
  1037:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  1038:         >>> from transformers.image_utils import load_image
  1039  

train_real_world/transformers_4573/src/transformers/models/idefics2/processing_idefics2.py:
  123          >>> import requests
  124:         >>> from transformers import Idefics2Processor
  125:         >>> from transformers.image_utils import load_image
  126  

train_real_world/transformers_4573/src/transformers/models/idefics3/configuration_idefics3.py:
   62      ```python
   63:     >>> from transformers.models.idefics3.modeling_idefics3 import Idefics3VisionTransformer
   64:     >>> from transformers.models.idefics3.configuration_idefics3 import Idefics3VisionConfig
   65  

  137      ```python
  138:     >>> from transformers import Idefics3Model, Idefics3Config
  139      >>> # Initializing configuration

train_real_world/transformers_4573/src/transformers/models/idefics3/convert_idefics3_weights_to_hf.py:
  20  
  21: from transformers import (
  22      AutoModelForCausalLM,

train_real_world/transformers_4573/src/transformers/models/idefics3/image_processing_idefics3.py:
  161  
  162: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
  163  def max_across_indices(values: Iterable[Any]) -> list[Any]:

  187  
  188: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
  189  def make_pixel_mask(

train_real_world/transformers_4573/src/transformers/models/idefics3/modeling_idefics3.py:
  103  
  104: # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionEmbeddings with Idefics2->Idefics3
  105  class Idefics3VisionEmbeddings(nn.Module):

  176  
  177: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
  178  def eager_attention_forward(

  200  
  201: # Copied from transformers.models.siglip.modeling_siglip.SiglipAttention with Siglip->Idefics3Vision
  202  class Idefics3VisionAttention(nn.Module):

  204  
  205:     # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__
  206      def __init__(self, config):

  266  
  267: # Copied from transformers.models.siglip.modeling_siglip.SiglipMLP with Siglip->Idefics3Vision
  268  class Idefics3VisionMLP(nn.Module):

  293  
  294: # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2EncoderLayer with Idefics2->Idefics3
  295  class Idefics3EncoderLayer(GradientCheckpointingLayer):

  304      @auto_docstring
  305:     # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoderLayer.forward
  306      def forward(

  329  
  330: # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoder with Siglip->Idefics3
  331  class Idefics3Encoder(nn.Module):

  364  
  365: # Copied from transformers.models.llama.modeling_llama.repeat_kv
  366  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

  377  
  378: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Idefics3
  379  class Idefics3RMSNorm(nn.Module):

  462  
  463:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionTransformer.get_input_embeddings
  464      def get_input_embeddings(self):

  466  
  467:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionTransformer.set_input_embeddings
  468      def set_input_embeddings(self, value):

  534  
  535:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.get_input_embeddings
  536      def get_input_embeddings(self):

  538  
  539:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.set_input_embeddings
  540      def set_input_embeddings(self, value):

  725  
  726:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.__init__ with Idefics2->Idefics3
  727      def __init__(self, config):

  737  
  738:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.get_input_embeddings
  739      def get_input_embeddings(self):

  741  
  742:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.set_input_embeddings
  743      def set_input_embeddings(self, value):

  790  
  791:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  792:         >>> from transformers.image_utils import load_image
  793  

  879  
  880:     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.prepare_inputs_for_generation
  881      def prepare_inputs_for_generation(

train_real_world/transformers_4573/src/transformers/models/idefics3/processing_idefics3.py:
  180          >>> import requests
  181:         >>> from transformers import Idefics3Processor
  182:         >>> from transformers.image_utils import load_image
  183  

train_real_world/transformers_4573/src/transformers/models/ijepa/configuration_ijepa.py:
  66      ```python
  67:     >>> from transformers import IJepaConfig, IJepaModel
  68  

train_real_world/transformers_4573/src/transformers/models/ijepa/convert_ijepa_to_hf.py:
  29  
  30: from transformers import (
  31      IJepaConfig,

  34  )
  35: from transformers.utils import logging
  36  

train_real_world/transformers_4573/src/transformers/models/ijepa/modular_ijepa.py:
  5  
  6: from transformers.models.ijepa.configuration_ijepa import IJepaConfig
  7  

train_real_world/transformers_4573/src/transformers/models/imagegpt/configuration_imagegpt.py:
  75      ```python
  76:     >>> from transformers import ImageGPTConfig, ImageGPTModel
  77  

train_real_world/transformers_4573/src/transformers/models/imagegpt/convert_imagegpt_original_tf2_to_pytorch.py:
  21  
  22: from transformers import ImageGPTConfig, ImageGPTForCausalLM
  23: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, logging
  24  

train_real_world/transformers_4573/src/transformers/models/imagegpt/image_processing_imagegpt.py:
  125  
  126:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize
  127      def resize(

train_real_world/transformers_4573/src/transformers/models/imagegpt/modeling_imagegpt.py:
  451          ```python
  452:         >>> from transformers import AutoImageProcessor, ImageGPTModel
  453          >>> from PIL import Image

  652          ```python
  653:         >>> from transformers import AutoImageProcessor, ImageGPTForCausalImageModeling
  654          >>> import torch

  779          ```python
  780:         >>> from transformers import AutoImageProcessor, ImageGPTForImageClassification
  781          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/informer/configuration_informer.py:
  117      ```python
  118:     >>> from transformers import InformerConfig, InformerModel
  119  

train_real_world/transformers_4573/src/transformers/models/informer/modeling_informer.py:
  1396          >>> import torch
  1397:         >>> from transformers import InformerModel
  1398  

  1682          >>> import torch
  1683:         >>> from transformers import InformerForPrediction
  1684  

train_real_world/transformers_4573/src/transformers/models/informer/modular_informer.py:
  637          >>> import torch
  638:         >>> from transformers import InformerModel
  639  

  792          >>> import torch
  793:         >>> from transformers import InformerForPrediction
  794  

train_real_world/transformers_4573/src/transformers/models/instructblip/configuration_instructblip.py:
   64      ```python
   65:     >>> from transformers import InstructBlipVisionConfig, InstructBlipVisionModel
   66  

  156      ```python
  157:     >>> from transformers import InstructBlipQFormerConfig, InstructBlipQFormerModel
  158  

  234      ```python
  235:     >>> from transformers import (
  236      ...     InstructBlipVisionConfig,

train_real_world/transformers_4573/src/transformers/models/instructblip/convert_instructblip_original_to_pytorch.py:
  32  
  33: from transformers import (
  34      AutoTokenizer,

  45  )
  46: from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  47  

train_real_world/transformers_4573/src/transformers/models/instructblip/modeling_instructblip.py:
    53  )
    54: # Copied from transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput with Blip2->InstructBlip
    55  class InstructBlipForConditionalGenerationModelOutput(ModelOutput):

    83  
    84: # Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->InstructBlip
    85  class InstructBlipVisionEmbeddings(nn.Module):

   158  
   159: # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBLIP doesn't cast attn weights to fp32
   160  def eager_attention_forward(

   182  
   183: # Copied from transformers.models.blip_2.modeling_blip_2.Blip2Attention with Blip2->InstructBlip
   184  class InstructBlipAttention(nn.Module):

   258  
   259: # Copied from transformers.models.blip.modeling_blip.BlipMLP
   260  class InstructBlipMLP(nn.Module):

   274  
   275: # Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->InstructBlip
   276  class InstructBlipEncoderLayer(GradientCheckpointingLayer):

   341  
   342: # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip
   343  class InstructBlipEncoder(nn.Module):

   374  
   375: # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP
   376  class InstructBlipVisionModel(InstructBlipPreTrainedModel):

   525  
   526: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->InstructBlipQFormer
   527  class InstructBlipQFormerSelfOutput(nn.Module):

   540  
   541: # Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerAttention with Blip2->InstructBlip
   542  class InstructBlipQFormerAttention(nn.Module):

   566  
   567: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->InstructBlipQFormer
   568  class InstructBlipQFormerIntermediate(nn.Module):

   582  
   583: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->InstructBlipQFormer
   584  class InstructBlipQFormerOutput(nn.Module):

   682  
   683: # Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerEncoder with Blip2->InstructBlip
   684  class InstructBlipQFormerEncoder(nn.Module):

  1155  
  1156:     # Copied from transformers.models.instructblip.modeling_instructblip.InstructBlipModel._preprocess_accelerate
  1157      def _preprocess_accelerate(self):

  1285          ```python
  1286:         >>> from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration
  1287          >>> import torch

train_real_world/transformers_4573/src/transformers/models/instructblipvideo/configuration_instructblipvideo.py:
   70      ```python
   71:     >>> from transformers import InstructBlipVideoVisionConfig, InstructBlipVideoVisionModel
   72  

  162      ```python
  163:     >>> from transformers import InstructBlipVideoQFormerConfig, InstructBlipVideoQFormerModel
  164  

  240      ```python
  241:     >>> from transformers import (
  242      ...     InstructBlipVideoVisionConfig,

train_real_world/transformers_4573/src/transformers/models/instructblipvideo/convert_instructblipvideo_original_to_pytorch.py:
  32  
  33: from transformers import (
  34      AutoTokenizer,

  45  )
  46: from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  47  

train_real_world/transformers_4573/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:
   214  
   215: # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32
   216  def eager_attention_forward(

  1266          ```python
  1267:         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration
  1268          >>> import torch

train_real_world/transformers_4573/src/transformers/models/instructblipvideo/modular_instructblipvideo.py:
   19  
   20: from transformers.models.instructblip.configuration_instructblip import (
   21      InstructBlipQFormerConfig,

   23  )
   24: from transformers.models.instructblip.modeling_instructblip import (
   25      InstructBlipForConditionalGeneration,

   81      ```python
   82:     >>> from transformers import (
   83      ...     InstructBlipVideoVisionConfig,

  399          ```python
  400:         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration
  401          >>> import torch

train_real_world/transformers_4573/src/transformers/models/internvl/configuration_internvl.py:
   74      ```python
   75:     >>> from transformers import InternVLVisionConfig, InternVLVisionModel
   76  

  172      ```python
  173:     >>> from transformers import InternVLForConditionalGeneration, InternVLConfig
  174  

train_real_world/transformers_4573/src/transformers/models/internvl/convert_internvl_weights_to_hf.py:
  22  
  23: from transformers import (
  24      AutoModel,

train_real_world/transformers_4573/src/transformers/models/internvl/modeling_internvl.py:
  817          >>> import torch
  818:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  819  

train_real_world/transformers_4573/src/transformers/models/internvl/modular_internvl.py:
  613          >>> import torch
  614:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  615  

train_real_world/transformers_4573/src/transformers/models/jais2/modeling_jais2.py:
  443          ```python
  444:         >>> from transformers import AutoTokenizer, Jais2ForCausalLM
  445  

train_real_world/transformers_4573/src/transformers/models/jais2/modular_jais2.py:
  174          ```python
  175:         >>> from transformers import AutoTokenizer, Jais2ForCausalLM
  176  

train_real_world/transformers_4573/src/transformers/models/jamba/modeling_jamba.py:
  1003          ```python
  1004:         >>> from transformers import AutoTokenizer, JambaForCausalLM
  1005  

train_real_world/transformers_4573/src/transformers/models/jamba/modular_jamba.py:
  754          ```python
  755:         >>> from transformers import AutoTokenizer, JambaForCausalLM
  756  

train_real_world/transformers_4573/src/transformers/models/janus/configuration_janus.py:
  236      ```python
  237:     >>> from transformers import JanusForConditionalGeneration, JanusConfig, JanusVisionConfig, JanusVQVAEConfig, LlamaConfig
  238  

train_real_world/transformers_4573/src/transformers/models/janus/convert_janus_weights_to_hf.py:
  32  
  33: from transformers import (
  34      AutoTokenizer,

  40  )
  41: from transformers.models.janus.image_processing_janus import JanusImageProcessor
  42: from transformers.models.janus.processing_janus import JanusProcessor
  43  

train_real_world/transformers_4573/src/transformers/models/janus/modular_janus.py:
  295      ```python
  296:     >>> from transformers import JanusForConditionalGeneration, JanusConfig, JanusVisionConfig, JanusVQVAEConfig, LlamaConfig
  297  

train_real_world/transformers_4573/src/transformers/models/jetmoe/configuration_jetmoe.py:
  87      ```python
  88:     >>> from transformers import JetMoeModel, JetMoeConfig
  89  

train_real_world/transformers_4573/src/transformers/models/kosmos2/configuration_kosmos2.py:
  225      ```python
  226:     >>> from transformers import Kosmos2Config, Kosmos2Model
  227  

train_real_world/transformers_4573/src/transformers/models/kosmos2/convert_kosmos2_original_pytorch_checkpoint_to_pytorch.py:
  4  
  5: from transformers import Kosmos2Config, Kosmos2ForConditionalGeneration
  6  

train_real_world/transformers_4573/src/transformers/models/kosmos2/modeling_kosmos2.py:
   163  
   164: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Kosmos2
   165  class Kosmos2VisionEmbeddings(nn.Module):

   247  
   248: # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> Kosmos2 doesn't cast attn weights to fp32
   249  def eager_attention_forward(

   343  
   344: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Kosmos2Vision
   345  class Kosmos2VisionMLP(nn.Module):

   359  
   360: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->Kosmos2Vision
   361  class Kosmos2VisionEncoderLayer(GradientCheckpointingLayer):

   410  
   411: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->Kosmos2Vision
   412  class Kosmos2VisionEncoder(nn.Module):

   500  class Kosmos2VisionTransformer(nn.Module):
   501:     # Copied from transformers.models.altclip.modeling_altclip.AltCLIPVisionTransformer.__init__ with AltCLIPVision->Kosmos2Vision,ALTCLIP_VISION->KOSMOS2_VISION,AltCLIP->Kosmos2Vision
   502      def __init__(self, config: Kosmos2VisionConfig):

   557  
   558:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.__init__
   559      def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):

   566  
   567:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.make_weights
   568      def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):

   576      @staticmethod
   577:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.get_embedding
   578      def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):

   627      @staticmethod
   628:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds
   629      def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):

   646      @staticmethod
   647:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
   648      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

  1189  
  1190:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP_VISION->KOSMOS2_VISION,CLIP->Kosmos2,self.vision_model->self.model
  1191      def __init__(self, config: Kosmos2VisionConfig):

  1196  
  1197:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.get_input_embeddings with CLIP_VISION->KOSMOS2_VISION,CLIP->Kosmos2,self.vision_model->self.model
  1198      def get_input_embeddings(self) -> nn.Module:

  1559          >>> import requests
  1560:         >>> from transformers import AutoProcessor, Kosmos2Model
  1561  

  1697          >>> import requests
  1698:         >>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration
  1699  

train_real_world/transformers_4573/src/transformers/models/kosmos2_5/configuration_kosmos2_5.py:
  162      ```python
  163:     >>> from transformers import Kosmos2_5VisionConfig, Kosmos2_5VisionModel
  164  

train_real_world/transformers_4573/src/transformers/models/kosmos2_5/convert_kosmos2_5.py:
  18  
  19: from transformers import Kosmos2_5Config, Kosmos2_5ForConditionalGeneration
  20  

train_real_world/transformers_4573/src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py:
   61  
   62: # Copied from transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches
   63  def torch_extract_patches(image_tensor, patch_height, patch_width):

  215  
  216:     # Copied from transformers.models.pix2struct.image_processing_pix2struct.Pix2StructImageProcessor.normalize
  217      def normalize(

train_real_world/transformers_4573/src/transformers/models/kosmos2_5/modeling_kosmos2_5.py:
    67  
    68: # Copied from transformers.models.kosmos2.modeling_kosmos2._expand_mask
    69  def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):

   338  
   339: # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructLayerNorm with Pix2Struct->Kosmos2_5
   340  class Kosmos2_5LayerNorm(nn.Module):

   397  
   398: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5DenseGatedActDense->Pix2StructVisionMlp,T5Config->Pix2StructVisionConfig,config.d_model->config.hidden_size,dropout_rate->dropout_rate
   399  class Kosmos2_5VisionMlp(nn.Module):

   560  
   561: # Adapted from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionEncoder with Pix2Struct->Kosmos2_5
   562  class Kosmos2_5VisionEncoder(nn.Module):

   613  
   614: # Copied from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextSinusoidalPositionalEmbedding with Kosmos2->Kosmos2_5
   615  class Kosmos2_5TextSinusoidalPositionalEmbedding(nn.Module):

   617  
   618:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.__init__
   619      def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):

   626  
   627:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.make_weights
   628      def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):

   636      @staticmethod
   637:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.get_embedding
   638      def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):

   687      @staticmethod
   688:     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds
   689      def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):

   706      @staticmethod
   707:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
   708      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

   723  
   724: # Copied from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextFFN with Kosmos2->Kosmos2_5
   725  class Kosmos2_5TextFFN(nn.Module):

   859  
   860:     # Adapted from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextBlock.forward
   861      def forward(

   900  
   901: # Adapted from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextTransformer with Kosmos2->Kosmos2_5
   902  class Kosmos2_5TextTransformer(nn.Module):

  1267  
  1268:     # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel.__init__ with Pix2Struct->Kosmos2_5
  1269      def __init__(self, config: Kosmos2_5VisionConfig):

  1280  
  1281:     # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel.get_input_embeddings
  1282      def get_input_embeddings(self):

  1324  
  1325: # Adapted from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextModel with KOSMOS2->KOSMOS2_5
  1326  class Kosmos2_5TextModel(Kosmos2_5PreTrainedModel):

  1432          >>> import requests
  1433:         >>> from transformers import AutoProcessor, Kosmos2_5Model
  1434  

  1732          >>> import torch
  1733:         >>> from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration
  1734  

train_real_world/transformers_4573/src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py:
  106      ```python
  107:     >>> from transformers import KyutaiSpeechToTextConfig, KyutaiSpeechToTextForConditionalGeneration
  108  

train_real_world/transformers_4573/src/transformers/models/kyutai_speech_to_text/convert_kyutai_speech_to_text_to_hf.py:
  24  
  25: from transformers import (
  26      KyutaiSpeechToTextConfig,

  31  )
  32: from transformers.convert_slow_tokenizer import MoshiConverter
  33: from transformers.utils.hub import cached_file
  34  

train_real_world/transformers_4573/src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:
   522  
   523: # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->KyutaiSpeechToText
   524  # TODO cyril: modular

  1126          >>> from datasets import load_dataset, Audio
  1127:         >>> from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration
  1128  

train_real_world/transformers_4573/src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:
  286          >>> from datasets import load_dataset, Audio
  287:         >>> from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration
  288  

train_real_world/transformers_4573/src/transformers/models/lasr/configuration_lasr.py:
   88          ```python
   89:         >>> from transformers import LasrEncoderModel, LasrEncoderConfig
   90  

  190          ```python
  191:         >>> from transformers import LasrForCTC, LasrCTCConfig
  192          >>> # Initializing a Lasr configuration

train_real_world/transformers_4573/src/transformers/models/lasr/modeling_lasr.py:
  502          ```python
  503:         >>> from transformers import AutoProcessor, LasrEncoder
  504          >>> from datasets import load_dataset, Audio

  614          ```python
  615:         >>> from transformers import AutoProcessor, LasrForCTC
  616          >>> from datasets import load_dataset, Audio

  686          ```python
  687:         >>> from transformers import AutoProcessor, LasrForCTC
  688          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/lasr/modular_lasr.py:
  164          ```python
  165:         >>> from transformers import LasrEncoderModel, LasrEncoderConfig
  166  

  263          ```python
  264:         >>> from transformers import LasrForCTC, LasrCTCConfig
  265          >>> # Initializing a Lasr configuration

  485          ```python
  486:         >>> from transformers import AutoProcessor, LasrEncoder
  487          >>> from datasets import load_dataset, Audio

  547          ```python
  548:         >>> from transformers import AutoProcessor, LasrForCTC
  549          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/layoutlm/configuration_layoutlm.py:
  74      ```python
  75:     >>> from transformers import LayoutLMConfig, LayoutLMModel
  76  

train_real_world/transformers_4573/src/transformers/models/layoutlm/modeling_layoutlm.py:
  122  
  123: # Copied from transformers.models.align.modeling_align.eager_attention_forward
  124  def eager_attention_forward(

  146  
  147: # Copied from transformers.models.align.modeling_align.AlignTextSelfAttention with AlignText->LayoutLM
  148  class LayoutLMSelfAttention(nn.Module):

  203  
  204: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->LayoutLM
  205  class LayoutLMSelfOutput(nn.Module):

  218  
  219: # Copied from transformers.models.align.modeling_align.AlignTextAttention with AlignText->LayoutLM
  220  class LayoutLMAttention(nn.Module):

  243  
  244: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  245  class LayoutLMIntermediate(nn.Module):

  259  
  260: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->LayoutLM
  261  class LayoutLMOutput(nn.Module):

  274  
  275: # Copied from transformers.models.align.modeling_align.AlignTextLayer with AlignText->LayoutLM
  276  class LayoutLMLayer(GradientCheckpointingLayer):

  313  
  314: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with AlignText->LayoutLM
  315  class LayoutLMEncoder(nn.Module):

  359  
  360: # Copied from transformers.models.bert.modeling_bert.BertPooler
  361  class LayoutLMPooler(nn.Module):

  375  
  376: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->LayoutLM
  377  class LayoutLMPredictionHeadTransform(nn.Module):

  393  
  394: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->LayoutLM
  395  class LayoutLMLMPredictionHead(nn.Module):

  410  
  411: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->LayoutLM
  412  class LayoutLMOnlyMLMHead(nn.Module):

  481          ```python
  482:         >>> from transformers import AutoTokenizer, LayoutLMModel
  483          >>> import torch

  621          ```python
  622:         >>> from transformers import AutoTokenizer, LayoutLMForMaskedLM
  623          >>> import torch

  738          ```python
  739:         >>> from transformers import AutoTokenizer, LayoutLMForSequenceClassification
  740          >>> import torch

  871          ```python
  872:         >>> from transformers import AutoTokenizer, LayoutLMForTokenClassification
  873          >>> import torch

  986          ```python
  987:         >>> from transformers import AutoTokenizer, LayoutLMForQuestionAnswering
  988          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py:
  104      ```python
  105:     >>> from transformers import LayoutLMv2Config, LayoutLMv2Model
  106  

train_real_world/transformers_4573/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py:
  170  
  171:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize
  172      def resize(

train_real_world/transformers_4573/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py:
   222  
   223: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->LayoutLMv2
   224  class LayoutLMv2Intermediate(nn.Module):

   238  
   239: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->LayoutLM
   240  class LayoutLMv2Output(nn.Module):

   729          ```python
   730:         >>> from transformers import AutoProcessor, LayoutLMv2Model, set_seed
   731          >>> from PIL import Image

   911          ```python
   912:         >>> from transformers import AutoProcessor, LayoutLMv2ForSequenceClassification, set_seed
   913          >>> from PIL import Image

  1113          ```python
  1114:         >>> from transformers import AutoProcessor, LayoutLMv2ForTokenClassification, set_seed
  1115          >>> from PIL import Image

  1266          ```python
  1267:         >>> from transformers import AutoProcessor, LayoutLMv2ForQuestionAnswering, set_seed
  1268          >>> import torch

train_real_world/transformers_4573/src/transformers/models/layoutlmv3/configuration_layoutlmv3.py:
  94      ```python
  95:     >>> from transformers import LayoutLMv3Config, LayoutLMv3Model
  96  

train_real_world/transformers_4573/src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py:
  200  
  201:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize
  202      def resize(

train_real_world/transformers_4573/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py:
   311  
   312: # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput
   313  class LayoutLMv3SelfOutput(nn.Module):

   326  
   327: # Copied from transformers.models.layoutlmv2.modeling_layoutlmv2.LayoutLMv2Attention with LayoutLMv2->LayoutLMv3
   328  class LayoutLMv3Attention(nn.Module):

   353  
   354: # Copied from transformers.models.layoutlmv2.modeling_layoutlmv2.LayoutLMv2Layer with LayoutLMv2->LayoutLMv3
   355  class LayoutLMv3Layer(GradientCheckpointingLayer):

   537  
   538: # Copied from transformers.models.roberta.modeling_roberta.RobertaIntermediate
   539  class LayoutLMv3Intermediate(nn.Module):

   553  
   554: # Copied from transformers.models.roberta.modeling_roberta.RobertaOutput
   555  class LayoutLMv3Output(nn.Module):

   713          ```python
   714:         >>> from transformers import AutoProcessor, AutoModel
   715          >>> from datasets import load_dataset

   926          ```python
   927:         >>> from transformers import AutoProcessor, AutoModelForTokenClassification
   928          >>> from datasets import load_dataset

  1031          ```python
  1032:         >>> from transformers import AutoProcessor, AutoModelForQuestionAnswering
  1033          >>> from datasets import load_dataset

  1156          ```python
  1157:         >>> from transformers import AutoProcessor, AutoModelForSequenceClassification
  1158          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/layoutxlm/configuration_layoutxlm.py:
  106      ```python
  107:     >>> from transformers import LayoutXLMConfig, LayoutXLMModel
  108  

train_real_world/transformers_4573/src/transformers/models/layoutxlm/modular_layoutxlm.py:
  93      ```python
  94:     >>> from transformers import LayoutXLMConfig, LayoutXLMModel
  95  

train_real_world/transformers_4573/src/transformers/models/led/configuration_led.py:
  83      ```python
  84:     >>> from transformers import LEDModel, LEDConfig
  85  

train_real_world/transformers_4573/src/transformers/models/led/modeling_led.py:
    91  
    92: # Copied from transformers.models.longformer.modeling_longformer.LongformerSelfAttention with Longformer->LEDEncoder
    93  class LEDEncoderSelfAttention(nn.Module):

  1092  )
  1093: # Copied from transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput with Longformer->LEDEncoder
  1094  class LEDEncoderBaseModelOutput(ModelOutput):

  1988          >>> import torch
  1989:         >>> from transformers import AutoTokenizer, LEDForConditionalGeneration
  1990  

  2023          ```python
  2024:         >>> from transformers import AutoTokenizer, LEDForConditionalGeneration
  2025  

train_real_world/transformers_4573/src/transformers/models/levit/configuration_levit.py:
  67      ```python
  68:     >>> from transformers import LevitConfig, LevitModel
  69  

train_real_world/transformers_4573/src/transformers/models/levit/convert_levit_timm_to_pytorch.py:
  27  
  28: from transformers import LevitConfig, LevitForImageClassificationWithTeacher, LevitImageProcessor
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/lfm2/configuration_lfm2.py:
  88      ```python
  89:     >>> from transformers import Lfm2Model, Lfm2Config
  90  

train_real_world/transformers_4573/src/transformers/models/lfm2/modeling_lfm2.py:
  730          ```python
  731:         >>> from transformers import AutoTokenizer, Lfm2ForCausalLM
  732  

train_real_world/transformers_4573/src/transformers/models/lfm2_moe/configuration_lfm2_moe.py:
  93      ```python
  94:     >>> from transformers import Lfm2MoeModel, Lfm2MoeConfig
  95  

train_real_world/transformers_4573/src/transformers/models/lfm2_moe/modeling_lfm2_moe.py:
  812          ```python
  813:         >>> from transformers import AutoTokenizer, Lfm2MoeForCausalLM
  814  

train_real_world/transformers_4573/src/transformers/models/lfm2_vl/modeling_lfm2_vl.py:
  368          >>> import requests
  369:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  370:         >>> from transformers.image_utils import load_image
  371  

train_real_world/transformers_4573/src/transformers/models/lfm2_vl/modular_lfm2_vl.py:
  278          >>> import requests
  279:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  280:         >>> from transformers.image_utils import load_image
  281  

train_real_world/transformers_4573/src/transformers/models/lightglue/configuration_lightglue.py:
  72          ```python
  73:         >>> from transformers import LightGlueConfig, LightGlueForKeypointMatching
  74  

train_real_world/transformers_4573/src/transformers/models/lightglue/convert_lightglue_to_hf.py:
  21  
  22: from transformers import (
  23      AutoModelForKeypointDetection,

  26  )
  27: from transformers.models.lightglue.configuration_lightglue import LightGlueConfig
  28  

train_real_world/transformers_4573/src/transformers/models/lightglue/modular_lightglue.py:
  90          ```python
  91:         >>> from transformers import LightGlueConfig, LightGlueForKeypointMatching
  92  

train_real_world/transformers_4573/src/transformers/models/lilt/configuration_lilt.py:
  71      ```python
  72:     >>> from transformers import LiltConfig, LiltModel
  73  

train_real_world/transformers_4573/src/transformers/models/lilt/modeling_lilt.py:
  289  
  290: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  291  class LiltSelfOutput(nn.Module):

  334  
  335: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  336  class LiltIntermediate(nn.Module):

  350  
  351: # Copied from transformers.models.bert.modeling_bert.BertOutput
  352  class LiltOutput(nn.Module):

  411  
  412:     # Copied from transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk
  413      def feed_forward_chunk(self, attention_output):

  478  
  479: # Copied from transformers.models.bert.modeling_bert.BertPooler
  480  class LiltPooler(nn.Module):

  556          ```python
  557:         >>> from transformers import AutoTokenizer, AutoModel
  558          >>> from datasets import load_dataset

  647  class LiltForSequenceClassification(LiltPreTrainedModel):
  648:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.__init__ with Roberta->Lilt, roberta->lilt
  649      def __init__(self, config):

  688          ```python
  689:         >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
  690          >>> from datasets import load_dataset

  760  class LiltForTokenClassification(LiltPreTrainedModel):
  761:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.__init__ with Roberta->Lilt, roberta->lilt
  762      def __init__(self, config):

  802          ```python
  803:         >>> from transformers import AutoTokenizer, AutoModelForTokenClassification
  804          >>> from datasets import load_dataset

  856  
  857: # Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->Lilt
  858  class LiltClassificationHead(nn.Module):

  881  class LiltForQuestionAnswering(LiltPreTrainedModel):
  882:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.__init__ with Roberta->Lilt, roberta->lilt
  883      def __init__(self, config):

  918          ```python
  919:         >>> from transformers import AutoTokenizer, AutoModelForQuestionAnswering
  920          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/llama/configuration_llama.py:
  97      ```python
  98:     >>> from transformers import LlamaModel, LlamaConfig
  99  

train_real_world/transformers_4573/src/transformers/models/llama/convert_llama_weights_to_hf.py:
   23  
   24: from transformers import GenerationConfig, LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast
   25: from transformers.convert_slow_tokenizer import TikTokenConverter
   26  

   28  try:
   29:     from transformers import LlamaTokenizerFast
   30  except ImportError as e:

   47  ```py
   48: from transformers import LlamaForCausalLM, LlamaTokenizer
   49  

  456              if model_id is not None:
  457:                 from transformers import AutoTokenizer
  458  

train_real_world/transformers_4573/src/transformers/models/llama/modeling_llama.py:
  475          ```python
  476:         >>> from transformers import AutoTokenizer, LlamaForCausalLM
  477  

train_real_world/transformers_4573/src/transformers/models/llama/tokenization_llama.py:
  47      ```python
  48:     >>> from transformers import LlamaTokenizer
  49  

train_real_world/transformers_4573/src/transformers/models/llama4/configuration_llama4.py:
  376      ```python
  377:     >>> from transformers import Llama4Model, Llama4Config
  378  

train_real_world/transformers_4573/src/transformers/models/llama4/convert_llama4_weights_to_hf.py:
   12  
   13: from transformers import (
   14      GenerationConfig,

   22  )
   23: from transformers.integrations.tiktoken import TikTokenConverter
   24  

  535  
  536:         from transformers import AutoTokenizer
  537  

train_real_world/transformers_4573/src/transformers/models/llama4/modeling_llama4.py:
    24  
    25: from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig
    26  

   171  
   172: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Llama4Text
   173  class Llama4TextRotaryEmbedding(nn.Module):

   262  
   263: # Adapted from transformers.models.llama.modeling_llama.eager_attention_forward -> llama4 doesn't cast attn weights to fp32
   264  def eager_attention_forward(

   289  
   290: # Adapted from transformers.models.llama.modeling_llama.eager_attention_forward -> llama4 doesn't cast attn weights to fp32
   291  def vision_eager_attention_forward(

   632          ```python
   633:         >>> from transformers import AutoTokenizer, Llama4ForCausalLM
   634  

  1083          >>> import requests
  1084:         >>> from transformers import AutoProcessor, MllamaVisionModel
  1085  

  1279          >>> import requests
  1280:         >>> from transformers import AutoProcessor, LlavaForConditionalGeneration
  1281  

train_real_world/transformers_4573/src/transformers/models/llama4/processing_llama4.py:
  18  
  19: from transformers.processing_utils import ProcessingKwargs, ProcessorMixin, Unpack
  20: from transformers.tokenization_utils_base import PreTokenizedInput, TextInput
  21  

train_real_world/transformers_4573/src/transformers/models/llava/configuration_llava.py:
  58      ```python
  59:     >>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig
  60  

train_real_world/transformers_4573/src/transformers/models/llava/convert_llava_weights_to_hf.py:
  20  
  21: from transformers import (
  22      AddedToken,

train_real_world/transformers_4573/src/transformers/models/llava/image_processing_llava.py:
  228  
  229:     # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize
  230      def resize(

train_real_world/transformers_4573/src/transformers/models/llava/modeling_llava.py:
  372          >>> import requests
  373:         >>> from transformers import AutoProcessor, LlavaForConditionalGeneration
  374  

train_real_world/transformers_4573/src/transformers/models/llava_next/configuration_llava_next.py:
  63      ```python
  64:     >>> from transformers import LlavaNextForConditionalGeneration, LlavaNextConfig, CLIPVisionConfig, LlamaConfig
  65  

train_real_world/transformers_4573/src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py:
  37  
  38: from transformers import (
  39      AddedToken,

train_real_world/transformers_4573/src/transformers/models/llava_next/image_processing_llava_next.py:
  210  
  211:     # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize with CLIP->LLaVa
  212      def resize(

train_real_world/transformers_4573/src/transformers/models/llava_next/modeling_llava_next.py:
  199  
  200: # Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaNext
  201  class LlavaNextMultiModalProjector(nn.Module):

  616          >>> import requests
  617:         >>> from transformers import AutoProcessor, LlavaNextForConditionalGeneration
  618  

  721      @staticmethod
  722:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  723      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/llava_next_video/configuration_llava_next_video.py:
  70      ```python
  71:     >>> from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoConfig, CLIPVisionConfig, LlamaConfig
  72  

train_real_world/transformers_4573/src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py:
  28  
  29: from transformers import (
  30      AddedToken,

train_real_world/transformers_4573/src/transformers/models/llava_next_video/modeling_llava_next_video.py:
  754          >>> import av
  755:         >>> from transformers import AutoProcessor, LlavaNextVideoForConditionalGeneration
  756  

train_real_world/transformers_4573/src/transformers/models/llava_next_video/modular_llava_next_video.py:
   21  
   22: from transformers.models.llava_next.modeling_llava_next import (
   23      LlavaNextCausalLMOutputWithPast,

   88      ```python
   89:     >>> from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoConfig, CLIPVisionConfig, LlamaConfig
   90  

  579          >>> import av
  580:         >>> from transformers import AutoProcessor, LlavaNextVideoForConditionalGeneration
  581  

train_real_world/transformers_4573/src/transformers/models/llava_next_video/processing_llava_next_video.py:
  215  
  216:     # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_number_of_features
  217      def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:

  234  
  235:     # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_unpadded_features
  236      def _get_unpadded_features(self, height, width, patches_height, patches_width, scale_height, scale_width):

train_real_world/transformers_4573/src/transformers/models/llava_onevision/configuration_llava_onevision.py:
  66      ```python
  67:     >>> from transformers import LlavaOnevisionForConditionalGeneration, LlavaOnevisionConfig, SiglipVisionConfig, Qwen2Config
  68  

train_real_world/transformers_4573/src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py:
  32  
  33: from transformers import (
  34      AddedToken,

train_real_world/transformers_4573/src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py:
  286  
  287:     # Copied from transformers.models.llava.image_processing_llava_fast.LlavaImageProcessorFast.pad_to_square
  288      def pad_to_square(

train_real_world/transformers_4573/src/transformers/models/llava_onevision/image_processing_llava_onevision.py:
   71  
   72: # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches
   73  def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:

  100  
  101: # Copied from transformers.models.llava_next.image_processing_llava_next.expand_to_square
  102  def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.ndarray:

  235  
  236:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor.pad
  237      def pad(

  304  
  305:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._resize_for_patching
  306      def _resize_for_patching(

  331  
  332:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._get_padding_size
  333      def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):

  339  
  340:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_patching
  341      def _pad_for_patching(

  353  
  354:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor.get_image_patches
  355      def get_image_patches(

  418  
  419:     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_batching
  420      def _pad_for_batching(

  458  
  459:     # Copied from transformers.models.llava.image_processing_llava.LlavaImageProcessor.pad_to_square
  460      def pad_to_square(

train_real_world/transformers_4573/src/transformers/models/llava_onevision/modeling_llava_onevision.py:
  751          >>> import torch
  752:         >>> from transformers import LlavaOnevisionProcessor, LlavaOnevisionForConditionalGeneration
  753  

train_real_world/transformers_4573/src/transformers/models/llava_onevision/modular_llava_onevision.py:
   22  
   23: from transformers.models.llava_next.image_processing_llava_next_fast import LlavaNextImageProcessorFast
   24: from transformers.models.llava_next_video.modeling_llava_next_video import (
   25      LlavaNextVideoCausalLMOutputWithPast,

   77  
   78:     # Copied from transformers.models.llava.image_processing_llava_fast.LlavaImageProcessorFast.pad_to_square
   79      def pad_to_square(

  602          >>> import torch
  603:         >>> from transformers import LlavaOnevisionProcessor, LlavaOnevisionForConditionalGeneration
  604  

train_real_world/transformers_4573/src/transformers/models/llava_onevision/processing_llava_onevision.py:
  250  
  251:     # Adapted from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_unpadded_features
  252      def _get_unpadded_features(self, height, width, patches_height, patches_width, scale_height, scale_width):

train_real_world/transformers_4573/src/transformers/models/longcat_flash/configuration_longcat_flash.py:
  110      ```python
  111:     >>> from transformers import LongcatFlashModel, LongcatFlashConfig
  112  

train_real_world/transformers_4573/src/transformers/models/longcat_flash/modeling_longcat_flash.py:
  694          ```python
  695:         >>> from transformers import AutoTokenizer, LongcatFlashForCausalLM
  696  

train_real_world/transformers_4573/src/transformers/models/longformer/configuration_longformer.py:
  74      ```python
  75:     >>> from transformers import LongformerConfig, LongformerModel
  76  

train_real_world/transformers_4573/src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py:
  22  
  23: from transformers import LongformerForQuestionAnswering, LongformerModel
  24  

train_real_world/transformers_4573/src/transformers/models/longformer/modeling_longformer.py:
  1060  
  1061: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  1062  class LongformerSelfOutput(nn.Module):

  1103  
  1104: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  1105  class LongformerIntermediate(nn.Module):

  1119  
  1120: # Copied from transformers.models.bert.modeling_bert.BertOutput
  1121  class LongformerOutput(nn.Module):

  1248  
  1249: # Copied from transformers.models.bert.modeling_bert.BertPooler
  1250  class LongformerPooler(nn.Module):

  1264  
  1265: # Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead with Roberta->Longformer
  1266  class LongformerLMHead(nn.Module):

  1434          >>> import torch
  1435:         >>> from transformers import LongformerModel, AutoTokenizer
  1436  

  1591          ```python
  1592:         >>> from transformers import AutoTokenizer, LongformerForMaskedLM
  1593  

  1822          ```python
  1823:         >>> from transformers import AutoTokenizer, LongformerForQuestionAnswering
  1824          >>> import torch

train_real_world/transformers_4573/src/transformers/models/longt5/modeling_longt5.py:
   220  
   221: # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->LongT5
   222  class LongT5LayerNorm(nn.Module):

   259  
   260: # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->LongT5
   261  class LongT5DenseActDense(nn.Module):

   300  
   301: # Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->LongT5
   302  class LongT5LayerFF(nn.Module):

   319  
   320: # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->LongT5
   321  class LongT5Attention(nn.Module):

   550      @staticmethod
   551:     # Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket
   552      def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):

   726      @staticmethod
   727:     # Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket
   728      def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):

   935  
   936: # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->LongT5
   937  class LongT5LayerSelfAttention(nn.Module):

  1030  
  1031: # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->LongT5
  1032  class LongT5LayerCrossAttention(nn.Module):

  1168      @property
  1169:     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel.dummy_inputs
  1170      def dummy_inputs(self):

  1221  
  1222:     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->LongT5
  1223      def _shift_right(self, input_ids):

  1268  
  1269:     # Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings
  1270      def set_input_embeddings(self, new_embeddings):

  1442  
  1443:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  1444      def _update_causal_mask(

  1512      @staticmethod
  1513:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  1514      def _prepare_4d_causal_attention_mask_with_cache_position(

  1654          ```python
  1655:         >>> from transformers import AutoTokenizer, LongT5Model
  1656  

  1822          ```python
  1823:         >>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration
  1824  

  1963          ```python
  1964:         >>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration
  1965  

train_real_world/transformers_4573/src/transformers/models/luke/configuration_luke.py:
  83      ```python
  84:     >>> from transformers import LukeConfig, LukeModel
  85  

train_real_world/transformers_4573/src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py:
  22  
  23: from transformers import LukeConfig, LukeModel, LukeTokenizer, RobertaTokenizer
  24: from transformers.tokenization_utils_base import AddedToken
  25  

train_real_world/transformers_4573/src/transformers/models/luke/modeling_luke.py:
   512  
   513: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
   514  class LukeSelfOutput(nn.Module):

   568  
   569: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
   570  class LukeIntermediate(nn.Module):

   584  
   585: # Copied from transformers.models.bert.modeling_bert.BertOutput
   586  class LukeOutput(nn.Module):

   714  
   715: # Copied from transformers.models.bert.modeling_bert.BertPooler
   716  class LukePooler(nn.Module):

   865          ```python
   866:         >>> from transformers import AutoTokenizer, LukeModel
   867  

  1019  
  1020: # Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead
  1021  class LukeLMHead(nn.Module):

  1256          ```python
  1257:         >>> from transformers import AutoTokenizer, LukeForEntityClassification
  1258  

  1385          ```python
  1386:         >>> from transformers import AutoTokenizer, LukeForEntityPairClassification
  1387  

  1525          ```python
  1526:         >>> from transformers import AutoTokenizer, LukeForEntitySpanClassification
  1527  

train_real_world/transformers_4573/src/transformers/models/luke/tokenization_luke.py:
  140      ```python
  141:     >>> from transformers import LukeTokenizer
  142  

train_real_world/transformers_4573/src/transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import LxmertConfig, LxmertForPreTraining
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/m2m_100/configuration_m2m_100.py:
  80      ```python
  81:     >>> from transformers import M2M100Config, M2M100Model
  82  

train_real_world/transformers_4573/src/transformers/models/m2m_100/convert_m2m100_original_checkpoint_to_pytorch.py:
  19  
  20: from transformers import M2M100Config, M2M100ForConditionalGeneration
  21  

train_real_world/transformers_4573/src/transformers/models/m2m_100/modeling_m2m_100.py:
    50  
    51: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    52  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    67  
    68: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->M2M100
    69  class M2M100ScaledWordEmbedding(nn.Embedding):

   166      @staticmethod
   167:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
   168      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

   183  
   184: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   185  def eager_attention_forward(

   213  
   214: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->M2M100
   215  class M2M100Attention(nn.Module):

   339  
   340: # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->M2M100, MBART->M2M100
   341  class M2M100EncoderLayer(GradientCheckpointingLayer):

   399  
   400: # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->M2M100, MBART->M2M100
   401  class M2M100DecoderLayer(GradientCheckpointingLayer):

  1086          ```python
  1087:         >>> from transformers import AutoTokenizer, M2M100ForConditionalGeneration
  1088  

train_real_world/transformers_4573/src/transformers/models/m2m_100/tokenization_m2m_100.py:
  97      ```python
  98:     >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
  99  

train_real_world/transformers_4573/src/transformers/models/mamba/configuration_mamba.py:
  88      ```python
  89:     >>> from transformers import MambaConfig, MambaModel
  90  

train_real_world/transformers_4573/src/transformers/models/mamba/convert_mamba_ssm_checkpoint_to_pytorch.py:
  22  
  23: from transformers import AutoTokenizer, MambaConfig, MambaForCausalLM
  24: from transformers.utils import logging
  25: from transformers.utils.import_utils import is_mamba_ssm_available
  26  

  32      def convert_ssm_config_to_hf_config(config_ssm: MambaConfigSSM) -> MambaConfig:
  33:         """Convert a MambaConfig from mamba_ssm to a MambaConfig from transformers."""
  34          hf_config = MambaConfig()

train_real_world/transformers_4573/src/transformers/models/mamba/modeling_mamba.py:
  67          >>> import torch
  68:         >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache
  69  

train_real_world/transformers_4573/src/transformers/models/mamba2/configuration_mamba2.py:
  96      ```python
  97:     >>> from transformers import Mamba2Config, Mamba2Model
  98  

train_real_world/transformers_4573/src/transformers/models/mamba2/convert_mamba2_ssm_checkpoint_to_pytorch.py:
  26  
  27: from transformers import GPTNeoXTokenizerFast, LlamaTokenizerFast, Mamba2Config, Mamba2ForCausalLM
  28  

train_real_world/transformers_4573/src/transformers/models/mamba2/modeling_mamba2.py:
  776  )
  777: # Copied from transformers.models.mamba.modeling_mamba.MambaOutput with MAMBA->MAMBA2,Mamba->Mamba2
  778  class Mamba2Output(ModelOutput):

  797  )
  798: # Copied from transformers.models.mamba.modeling_mamba.MambaCausalLMOutput with Mamba->Mamba2
  799  class Mamba2CausalLMOutput(ModelOutput):

train_real_world/transformers_4573/src/transformers/models/marian/configuration_marian.py:
  83      ```python
  84:     >>> from transformers import MarianModel, MarianConfig
  85  

train_real_world/transformers_4573/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py:
  24  
  25: from transformers.models.marian.convert_marian_to_pytorch import (
  26      FRONT_MATTER_TEMPLATE,

train_real_world/transformers_4573/src/transformers/models/marian/convert_marian_to_pytorch.py:
  30  
  31: from transformers import MarianConfig, MarianMTModel, MarianTokenizer
  32  

train_real_world/transformers_4573/src/transformers/models/marian/modeling_marian.py:
    53  
    54: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    55  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

   104  
   105: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   106  def eager_attention_forward(

   134  
   135: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Marian
   136  class MarianAttention(nn.Module):

   260  
   261: # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->Marian, BART->MARIAN
   262  class MarianEncoderLayer(GradientCheckpointingLayer):

   328  
   329: # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN
   330  class MarianDecoderLayer(GradientCheckpointingLayer):

   952          ```python
   953:         >>> from transformers import AutoTokenizer, MarianModel
   954  

  1172          ```python
  1173:         >>> from transformers import AutoTokenizer, MarianMTModel
  1174  

  1242  
  1243: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Marian
  1244  class MarianDecoderWrapper(MarianPreTrainedModel):

  1258  
  1259: # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Marian, facebook/bart-base->Helsinki-NLP/opus-mt-fr-en
  1260  class MarianForCausalLM(MarianPreTrainedModel, GenerationMixin):

  1308          ```python
  1309:         >>> from transformers import AutoTokenizer, MarianForCausalLM
  1310  

train_real_world/transformers_4573/src/transformers/models/marian/tokenization_marian.py:
  92      ```python
  93:     >>> from transformers import MarianForCausalLM, MarianTokenizer
  94  

train_real_world/transformers_4573/src/transformers/models/markuplm/configuration_markuplm.py:
  83      ```python
  84:     >>> from transformers import MarkupLMModel, MarkupLMConfig
  85  

train_real_world/transformers_4573/src/transformers/models/markuplm/feature_extraction_markuplm.py:
  115          ```python
  116:         >>> from transformers import MarkupLMFeatureExtractor
  117  

train_real_world/transformers_4573/src/transformers/models/markuplm/modeling_markuplm.py:
  121      @staticmethod
  122:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds
  123      def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):

  140      @staticmethod
  141:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
  142      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

  208  
  209: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->MarkupLM
  210  class MarkupLMSelfOutput(nn.Module):

  223  
  224: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  225  class MarkupLMIntermediate(nn.Module):

  239  
  240: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->MarkupLM
  241  class MarkupLMOutput(nn.Module):

  254  
  255: # Copied from transformers.models.bert.modeling_bert.BertPooler
  256  class MarkupLMPooler(nn.Module):

  270  
  271: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->MarkupLM
  272  class MarkupLMPredictionHeadTransform(nn.Module):

  288  
  289: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->MarkupLM
  290  class MarkupLMLMPredictionHead(nn.Module):

  305  
  306: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->MarkupLM
  307  class MarkupLMOnlyMLMHead(nn.Module):

  316  
  317: # Copied from transformers.models.align.modeling_align.eager_attention_forward
  318  def eager_attention_forward(

  340  
  341: # Copied from transformers.models.align.modeling_align.AlignTextSelfAttention with AlignText->MarkupLM
  342  class MarkupLMSelfAttention(nn.Module):

  397  
  398: # Copied from transformers.models.align.modeling_align.AlignTextAttention with AlignText->MarkupLM
  399  class MarkupLMAttention(nn.Module):

  422  
  423: # Copied from transformers.models.align.modeling_align.AlignTextLayer with AlignText->MarkupLM
  424  class MarkupLMLayer(GradientCheckpointingLayer):

  461  
  462: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with AlignText->MarkupLM
  463  class MarkupLMEncoder(nn.Module):

  524  class MarkupLMModel(MarkupLMPreTrainedModel):
  525:     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->MarkupLM
  526      def __init__(self, config, add_pooling_layer=True):

  572          ```python
  573:         >>> from transformers import AutoProcessor, MarkupLMModel
  574  

  642  class MarkupLMForQuestionAnswering(MarkupLMPreTrainedModel):
  643:     # Copied from transformers.models.bert.modeling_bert.BertForQuestionAnswering.__init__ with bert->markuplm, Bert->MarkupLM
  644      def __init__(self, config):

  680          ```python
  681:         >>> from transformers import AutoProcessor, MarkupLMForQuestionAnswering
  682          >>> import torch

  755  class MarkupLMForTokenClassification(MarkupLMPreTrainedModel):
  756:     # Copied from transformers.models.bert.modeling_bert.BertForTokenClassification.__init__ with bert->markuplm, Bert->MarkupLM
  757      def __init__(self, config):

  798          ```python
  799:         >>> from transformers import AutoProcessor, AutoModelForTokenClassification
  800          >>> import torch

  857  class MarkupLMForSequenceClassification(MarkupLMPreTrainedModel):
  858:     # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification.__init__ with bert->markuplm, Bert->MarkupLM
  859      def __init__(self, config):

  903          ```python
  904:         >>> from transformers import AutoProcessor, AutoModelForSequenceClassification
  905          >>> import torch

train_real_world/transformers_4573/src/transformers/models/mask2former/configuration_mask2former.py:
  115      ```python
  116:     >>> from transformers import Mask2FormerConfig, Mask2FormerModel
  117  

train_real_world/transformers_4573/src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py:
  33  
  34: from transformers import (
  35      Mask2FormerConfig,

  40  )
  41: from transformers.models.mask2former.modeling_mask2former import (
  42      Mask2FormerForUniversalSegmentationOutput,

  44  )
  45: from transformers.utils import logging
  46  

train_real_world/transformers_4573/src/transformers/models/mask2former/image_processing_mask2former_fast.py:
  27  
  28: from transformers.image_transforms import get_size_with_aspect_ratio
  29  

train_real_world/transformers_4573/src/transformers/models/mask2former/image_processing_mask2former.py:
   89  
   90: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   91  def get_max_height_width(

  108  
  109: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
  110  def make_pixel_mask(

  127  
  128: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
  129  def binary_mask_to_rle(mask):

  150  
  151: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
  152  def convert_segmentation_to_rle(segmentation):

  172  
  173: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
  174  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

  201  
  202: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
  203  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

  220  
  221: # Copied from transformers.models.detr.image_processing_detr.compute_segments
  222  def compute_segments(

  282  # TODO: (Amy) Move to image_transforms
  283: # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks
  284  def convert_segmentation_map_to_binary_masks(

  324  
  325: # Copied from transformers.models.maskformer.image_processing_maskformer.get_maskformer_resize_output_image_size with maskformer->mask2former
  326  def get_mask2former_resize_output_image_size(

  464  
  465:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.to_dict
  466      def to_dict(self) -> dict[str, Any]:

  474  
  475:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.resize with get_maskformer_resize_output_image_size->get_mask2former_resize_output_image_size
  476      def resize(

  532  
  533:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
  534      def rescale(

  561  
  562:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.convert_segmentation_map_to_binary_masks
  563      def convert_segmentation_map_to_binary_masks(

  783  
  784:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor._pad_image
  785      def _pad_image(

  811  
  812:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.pad
  813      def pad(

train_real_world/transformers_4573/src/transformers/models/mask2former/modeling_mask2former.py:
   278  
   279: # Copied from transformers.models.maskformer.modeling_maskformer.dice_loss
   280  def dice_loss(inputs: Tensor, labels: Tensor, num_masks: int) -> Tensor:

   328  
   329: # Copied from transformers.models.maskformer.modeling_maskformer.pair_wise_dice_loss
   330  def pair_wise_dice_loss(inputs: Tensor, labels: Tensor) -> Tensor:

   798  
   799: # Copied from transformers.models.oneformer.modeling_oneformer.multi_scale_deformable_attention
   800  def multi_scale_deformable_attention(

   841  
   842: # Copied from transformers.models.maskformer.modeling_maskformer.MaskFormerSinePositionEmbedding with MaskFormer->Mask2Former
   843  class Mask2FormerSinePositionEmbedding(nn.Module):

   888  
   889: # Modified from transformers.models.detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention
   890  class Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):

  1076  
  1077: # Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrEncoder with DeformableDetrEncoder->Mask2FormerPixelDecoderEncoderOnly
  1078  class Mask2FormerPixelDecoderEncoderOnly(nn.Module):

  1205  
  1206: # Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrModel with DeformableDetrModel->Mask2FormerPixelDecoder
  1207  class Mask2FormerPixelDecoder(nn.Module):

  1418  
  1419: # Modified from transformers.models.detr.modeling_detr.DetrAttention with Detr->Mask2Former
  1420  class Mask2FormerAttention(nn.Module):

  1931  
  1932: # Copied from transformers.models.maskformer.modeling_maskformer.PredictionBlock with MaskFormer->Mask2Former
  1933  class Mask2FormerPredictionBlock(nn.Module):

  2333          ```python
  2334:         >>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
  2335          >>> from PIL import Image

  2366          ```python
  2367:         >>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
  2368          >>> from PIL import Image

  2400          ```python
  2401:         >>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
  2402          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/mask2former/modular_mask2former.py:
  19  
  20: from transformers.models.maskformer.image_processing_maskformer_fast import MaskFormerImageProcessorFast
  21  

train_real_world/transformers_4573/src/transformers/models/maskformer/configuration_maskformer_swin.py:
  83      ```python
  84:     >>> from transformers import MaskFormerSwinConfig, MaskFormerSwinModel
  85  

train_real_world/transformers_4573/src/transformers/models/maskformer/configuration_maskformer.py:
  90      ```python
  91:     >>> from transformers import MaskFormerConfig, MaskFormerModel
  92  

train_real_world/transformers_4573/src/transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py:
  32  
  33: from transformers.models.maskformer.feature_extraction_maskformer import MaskFormerImageProcessor
  34: from transformers.models.maskformer.modeling_maskformer import (
  35      MaskFormerConfig,

  40  )
  41: from transformers.utils import logging
  42  

train_real_world/transformers_4573/src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py:
  28  
  29: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, ResNetConfig
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py:
  28  
  29: from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, SwinConfig
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/maskformer/image_processing_maskformer_fast.py:
   23  
   24: from transformers.image_transforms import get_size_with_aspect_ratio
   25  

  404  
  405:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation
  406      def post_process_semantic_segmentation(

  455  
  456:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_instance_segmentation
  457      def post_process_instance_segmentation(

  572  
  573:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_panoptic_segmentation
  574      def post_process_panoptic_segmentation(

train_real_world/transformers_4573/src/transformers/models/maskformer/image_processing_maskformer.py:
   90  
   91: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   92  def get_max_height_width(

  109  
  110: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
  111  def make_pixel_mask(

  128  
  129: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
  130  def binary_mask_to_rle(mask):

  151  
  152: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
  153  def convert_segmentation_to_rle(segmentation):

  173  
  174: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
  175  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

  202  
  203: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
  204  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

  221  
  222: # Copied from transformers.models.detr.image_processing_detr.compute_segments
  223  def compute_segments(

  531  
  532:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
  533      def rescale(

  781  
  782:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor._pad_image
  783      def _pad_image(

train_real_world/transformers_4573/src/transformers/models/maskformer/modeling_maskformer_swin.py:
   81  
   82: # Copied from transformers.models.swin.modeling_swin.window_partition
   83  def window_partition(input_feature, window_size):

   94  
   95: # Copied from transformers.models.swin.modeling_swin.window_reverse
   96  def window_reverse(windows, window_size, height, width):

  105  
  106: # Copied from transformers.models.swin.modeling_swin.drop_path
  107  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  142  
  143:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
  144      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

  199  
  200: # Copied from transformers.models.swin.modeling_swin.SwinPatchEmbeddings with Swin->MaskFormerSwin
  201  class MaskFormerSwinPatchEmbeddings(nn.Module):

  243  
  244: # Copied from transformers.models.swin.modeling_swin.SwinPatchMerging
  245  class MaskFormerSwinPatchMerging(nn.Module):

  298  
  299: # Copied from transformers.models.swin.modeling_swin.SwinDropPath with Swin->MaskFormerSwin
  300  class MaskFormerSwinDropPath(nn.Module):

  313  
  314: # Copied from transformers.models.swin.modeling_swin.SwinSelfAttention with Swin->MaskFormerSwin
  315  class MaskFormerSwinSelfAttention(nn.Module):

  407  
  408: # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->MaskFormerSwin
  409  class MaskFormerSwinSelfOutput(nn.Module):

  421  
  422: # Copied from transformers.models.swin.modeling_swin.SwinAttention with Swin->MaskFormerSwin
  423  class MaskFormerSwinAttention(nn.Module):

  440  
  441: # Copied from transformers.models.swin.modeling_swin.SwinIntermediate with Swin->MaskFormerSwin
  442  class MaskFormerSwinIntermediate(nn.Module):

  456  
  457: # Copied from transformers.models.swin.modeling_swin.SwinOutput with Swin->MaskFormerSwin
  458  class MaskFormerSwinOutput(nn.Module):

  578  class MaskFormerSwinStage(GradientCheckpointingLayer):
  579:     # Copied from transformers.models.swin.modeling_swin.SwinStage.__init__ with Swin->MaskFormerSwin
  580      def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):

  631  class MaskFormerSwinEncoder(nn.Module):
  632:     # Copied from transformers.models.swin.modeling_swin.SwinEncoder.__init__ with Swin->MaskFormerSwin
  633      def __init__(self, config, grid_size):

train_real_world/transformers_4573/src/transformers/models/maskformer/modeling_maskformer.py:
    64  )
    65: # Copied from transformers.models.detr.modeling_detr.DetrDecoderOutput
    66  class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):

   389  
   390: # Copied from transformers.models.detr.modeling_detr.DetrAttention
   391  class DetrAttention(nn.Module):

   523  
   524: # Copied from transformers.models.detr.modeling_detr.DetrDecoderLayer
   525  class DetrDecoderLayer(GradientCheckpointingLayer):

  1512          ```python
  1513:         >>> from transformers import AutoImageProcessor, MaskFormerModel
  1514          >>> from PIL import Image

  1694          ```python
  1695:         >>> from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation
  1696          >>> from PIL import Image

  1727          ```python
  1728:         >>> from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation
  1729          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/mbart/configuration_mbart.py:
  85      ```python
  86:     >>> from transformers import MBartConfig, MBartModel
  87  

train_real_world/transformers_4573/src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py:
  19  
  20: from transformers import MBartConfig, MBartForConditionalGeneration
  21  

train_real_world/transformers_4573/src/transformers/models/mbart/modeling_mbart.py:
    81  
    82: # Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->MBart
    83  class MBartLearnedPositionalEmbedding(nn.Embedding):

   109  
   110: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->MBart
   111  class MBartScaledWordEmbedding(nn.Embedding):

   123  
   124: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   125  def eager_attention_forward(

   153  
   154: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->MBart
   155  class MBartAttention(nn.Module):

   445  
   446: # Copied from transformers.models.bart.modeling_bart.BartClassificationHead with Bart->MBart
   447  class MBartClassificationHead(nn.Module):

  1093          ```python
  1094:         >>> from transformers import AutoTokenizer, MBartForConditionalGeneration
  1095  

  1110          ```python
  1111:         >>> from transformers import AutoTokenizer, MBartForConditionalGeneration
  1112  

  1201      @auto_docstring
  1202:     # Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward
  1203      def forward(

  1334      @auto_docstring
  1335:     # Copied from transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward
  1336      def forward(

  1440  
  1441: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->MBart
  1442  class MBartDecoderWrapper(MBartPreTrainedModel):

  1456  
  1457: # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->MBart, facebook/bart-base->facebook/mbart-large-cc25
  1458  class MBartForCausalLM(MBartPreTrainedModel, GenerationMixin):

  1506          ```python
  1507:         >>> from transformers import AutoTokenizer, MBartForCausalLM
  1508  

train_real_world/transformers_4573/src/transformers/models/mbart/tokenization_mbart.py:
  48      ```python
  49:     >>> from transformers import MBartTokenizer
  50  

train_real_world/transformers_4573/src/transformers/models/mbart50/tokenization_mbart50.py:
  70      ```python
  71:     >>> from transformers import MBart50Tokenizer
  72  

train_real_world/transformers_4573/src/transformers/models/megatron_bert/configuration_megatron_bert.py:
  71      ```python
  72:     >>> from transformers import MegatronBertConfig, MegatronBertModel
  73  

train_real_world/transformers_4573/src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py:
  41  
  42: from transformers import MegatronBertConfig
  43  

train_real_world/transformers_4573/src/transformers/models/megatron_bert/modeling_megatron_bert.py:
   104  
   105: # copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->MegatronBert
   106  class MegatronBertSelfAttention(nn.Module):

   248  
   249: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->MegatronBert
   250  class MegatronBertIntermediate(nn.Module):

   432  
   433: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->MegatronBert
   434  class MegatronBertPooler(nn.Module):

   448  
   449: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->MegatronBert
   450  class MegatronBertPredictionHeadTransform(nn.Module):

   466  
   467: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->MegatronBert
   468  class MegatronBertLMPredictionHead(nn.Module):

   483  
   484: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->MegatronBert
   485  class MegatronBertOnlyMLMHead(nn.Module):

   494  
   495: # Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->MegatronBert
   496  class MegatronBertOnlyNSPHead(nn.Module):

   505  
   506: # Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->MegatronBert
   507  class MegatronBertPreTrainingHeads(nn.Module):

   540  )
   541: # Copied from transformers.models.bert.modeling_bert.BertForPreTrainingOutput with Bert->MegatronBert
   542  class MegatronBertForPreTrainingOutput(ModelOutput):

   757          ```python
   758:         >>> from transformers import AutoTokenizer, MegatronBertForPreTraining
   759          >>> import torch

   864          ```python
   865:         >>> from transformers import AutoTokenizer, MegatronBertForCausalLM, MegatronBertConfig
   866          >>> import torch

  1060          ```python
  1061:         >>> from transformers import AutoTokenizer, MegatronBertForNextSentencePrediction
  1062          >>> import torch

train_real_world/transformers_4573/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py:
   26  
   27: from transformers import AutoTokenizer, GPT2Config
   28: from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME
   29: from transformers.utils import check_torch_load_is_safe
   30  

  269      """
  270:     Merge sharded checkpoints from transformers into a single checkpoint.
  271  

train_real_world/transformers_4573/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py:
  41  
  42: from transformers import AutoTokenizer, GPT2Config
  43  

train_real_world/transformers_4573/src/transformers/models/metaclip_2/configuration_metaclip_2.py:
   62      ```python
   63:     >>> from transformers import MetaClip2TextConfig, MetaClip2TextModel
   64  

  157      ```python
  158:     >>> from transformers import MetaClip2VisionConfig, MetaClip2VisionModel
  159  

  231      ```python
  232:     >>> from transformers import MetaClip2Config, MetaClip2Model
  233  

  243      >>> # We can also initialize a MetaClip2Config from a MetaClip2TextConfig and a MetaClip2VisionConfig
  244:     >>> from transformers import MetaClip2TextConfig, MetaClip2VisionConfig
  245  

train_real_world/transformers_4573/src/transformers/models/metaclip_2/convert_metaclip_2_to_hf.py:
  29  
  30: from transformers import (
  31      AutoTokenizer,

train_real_world/transformers_4573/src/transformers/models/metaclip_2/modeling_metaclip_2.py:
   491      ```python
   492:     >>> from transformers import AutoTokenizer, MetaClip2TextModel
   493  

   533          ```python
   534:         >>> from transformers import AutoTokenizer, MetaClip2TextModel
   535  

   592      ```python
   593:     >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection
   594  

   638          ```python
   639:         >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection
   640  

   745      >>> import requests
   746:     >>> from transformers import AutoProcessor, MetaClip2Model
   747  

   816          ```python
   817:         >>> from transformers import AutoTokenizer, MetaClip2Model
   818  

   851          >>> import requests
   852:         >>> from transformers import AutoProcessor, MetaClip2Model
   853  

   893          >>> import requests
   894:         >>> from transformers import AutoProcessor, MetaClip2Model
   895  

  1019      >>> import requests
  1020:     >>> from transformers import AutoProcessor, MetaClip2VisionModel
  1021  

  1062          >>> import requests
  1063:         >>> from transformers import AutoProcessor, MetaClip2VisionModel
  1064  

  1125      >>> import requests
  1126:     >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection
  1127  

  1171          >>> import requests
  1172:         >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection
  1173  

train_real_world/transformers_4573/src/transformers/models/metaclip_2/modular_metaclip_2.py:
   84      ```python
   85:     >>> from transformers import MetaClip2TextConfig, MetaClip2TextModel
   86  

  140      ```python
  141:     >>> from transformers import MetaClip2VisionConfig, MetaClip2VisionModel
  142  

  178      ```python
  179:     >>> from transformers import MetaClip2Config, MetaClip2Model
  180  

  190      >>> # We can also initialize a MetaClip2Config from a MetaClip2TextConfig and a MetaClip2VisionConfig
  191:     >>> from transformers import MetaClip2TextConfig, MetaClip2VisionConfig
  192  

  346      ```python
  347:     >>> from transformers import AutoTokenizer, MetaClip2TextModel
  348  

  371          ```python
  372:         >>> from transformers import AutoTokenizer, MetaClip2TextModel
  373  

  410      ```python
  411:     >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection
  412  

  434          ```python
  435:         >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection
  436  

  472      >>> import requests
  473:     >>> from transformers import AutoProcessor, MetaClip2Model
  474  

  533          >>> import requests
  534:         >>> from transformers import AutoProcessor, MetaClip2Model
  535  

  573          ```python
  574:         >>> from transformers import AutoTokenizer, MetaClip2Model
  575  

  602          >>> import requests
  603:         >>> from transformers import AutoProcessor, MetaClip2Model
  604  

  642      >>> import requests
  643:     >>> from transformers import AutoProcessor, MetaClip2VisionModel
  644  

  671          >>> import requests
  672:         >>> from transformers import AutoProcessor, MetaClip2VisionModel
  673  

  714      >>> import requests
  715:     >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection
  716  

  742          >>> import requests
  743:         >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection
  744  

train_real_world/transformers_4573/src/transformers/models/mgp_str/configuration_mgp_str.py:
  76      ```python
  77:     >>> from transformers import MgpstrConfig, MgpstrForSceneTextRecognition
  78  

train_real_world/transformers_4573/src/transformers/models/mgp_str/modeling_mgp_str.py:
   34  
   35: # Copied from transformers.models.beit.modeling_beit.drop_path
   36  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   50  
   51: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Mgpstr
   52  class MgpstrDropPath(nn.Module):

  398          ```python
  399:         >>> from transformers import (
  400          ...     MgpstrProcessor,

train_real_world/transformers_4573/src/transformers/models/mgp_str/processing_mgp_str.py:
  16  
  17: from transformers import AutoTokenizer
  18: from transformers.utils import is_torch_available
  19: from transformers.utils.generic import ExplicitEnum
  20  

train_real_world/transformers_4573/src/transformers/models/mimi/configuration_mimi.py:
  132      ```python
  133:     >>> from transformers import MimiModel, MimiConfig
  134  

train_real_world/transformers_4573/src/transformers/models/mimi/convert_mimi_checkpoint_to_pytorch.py:
  21  
  22: from transformers import (
  23      EncodecFeatureExtractor,

train_real_world/transformers_4573/src/transformers/models/mimi/modeling_mimi.py:
   263  
   264:     # Copied from transformers.models.encodec.modeling_encodec.EncodecConv1d._get_extra_padding_for_conv1d
   265      def _get_extra_padding_for_conv1d(

   277      @staticmethod
   278:     # Copied from transformers.models.encodec.modeling_encodec.EncodecConv1d._pad1d
   279      def _pad1d(hidden_states: torch.Tensor, paddings: tuple[int, int], mode: str = "zero", value: float = 0.0):

   505  
   506: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Mimi
   507  class MimiRotaryEmbedding(nn.Module):

   571  
   572: # Copied from transformers.models.llama.modeling_llama.rotate_half
   573  def rotate_half(x):

   579  
   580: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
   581  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   615  
   616:     # Copied from transformers.models.clip.modeling_clip.CLIPMLP.forward
   617      def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:

   623  
   624: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   625  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   636  
   637: # copied from transformers.models.gemma.modeling_gemma.GemmaAttention with Gemma->Mimi
   638  # no longer copied after attention refactors

   735  
   736: # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Mimi
   737  # TODO cyril: modular

   853  
   854: # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Mimi
   855  # TODO cyril: modular

  1192  
  1193:     # Copied from transformers.models.encodec.modeling_encodec.EncodecDecoder.forward
  1194      def forward(self, hidden_states):

  1227  
  1228:     # Copied from transformers.models.encodec.modeling_encodec.EncodecEuclideanCodebook.encode
  1229      def encode(self, hidden_states):

  1238  
  1239:     # Copied from transformers.models.encodec.modeling_encodec.EncodecEuclideanCodebook.decode
  1240      def decode(self, embed_ind):

  1244  
  1245: # Copied from transformers.models.encodec.modeling_encodec.EncodecVectorQuantization with Encodec->Mimi
  1246  class MimiVectorQuantization(nn.Module):

  1742          >>> from datasets import load_dataset
  1743:         >>> from transformers import AutoFeatureExtractor, MimiModel
  1744  

train_real_world/transformers_4573/src/transformers/models/minimax/configuration_minimax.py:
  120      ```python
  121:     >>> from transformers import MiniMaxModel, MiniMaxConfig
  122  

train_real_world/transformers_4573/src/transformers/models/minimax/modeling_minimax.py:
  839          ```python
  840:         >>> from transformers import AutoTokenizer, MiniMaxForCausalLM
  841  

train_real_world/transformers_4573/src/transformers/models/minimax/modular_minimax.py:
  146      ```python
  147:     >>> from transformers import MiniMaxModel, MiniMaxConfig
  148  

  625          ```python
  626:         >>> from transformers import AutoTokenizer, MiniMaxForCausalLM
  627  

train_real_world/transformers_4573/src/transformers/models/ministral/configuration_ministral.py:
  78      ```python
  79:     >>> from transformers import MinistralModel, MinistralConfig
  80  

train_real_world/transformers_4573/src/transformers/models/ministral/modeling_ministral.py:
  461          ```python
  462:         >>> from transformers import AutoTokenizer, MinistralForCausalLM
  463  

train_real_world/transformers_4573/src/transformers/models/ministral/modular_ministral.py:
  95      ```python
  96:     >>> from transformers import MinistralModel, MinistralConfig
  97  

train_real_world/transformers_4573/src/transformers/models/ministral3/configuration_ministral3.py:
  87      ```python
  88:     >>> from transformers import Ministral3Config, Ministral3ForCausalLM, Mistral3Config, Mistral3ForConditionalGeneration, PixtralVisionConfig
  89  

train_real_world/transformers_4573/src/transformers/models/ministral3/convert_ministral3_weights_to_hf.py:
  21  
  22: from transformers import (
  23      GenerationConfig,

  31  )
  32: from transformers.integrations.finegrained_fp8 import replace_with_fp8_linear
  33: from transformers.integrations.mistral import convert_tekken_tokenizer
  34: from transformers.quantizers.auto import AutoQuantizationConfig
  35  

train_real_world/transformers_4573/src/transformers/models/ministral3/modeling_ministral3.py:
   12  
   13: from transformers.utils.generic import check_model_inputs
   14  

  459          ```python
  460:         >>> from transformers import AutoTokenizer, Ministral3ForCausalLM
  461  

train_real_world/transformers_4573/src/transformers/models/mistral/configuration_mistral.py:
  90      ```python
  91:     >>> from transformers import MistralModel, MistralConfig
  92  

train_real_world/transformers_4573/src/transformers/models/mistral/convert_mistral_weights_to_hf.py:
  21  
  22: from transformers import AutoTokenizer, LlamaTokenizerFast, MistralConfig, MistralForCausalLM
  23: from transformers.integrations.mistral import convert_tekken_tokenizer
  24  

train_real_world/transformers_4573/src/transformers/models/mistral/modeling_mistral.py:
   12  
   13: from transformers.utils.generic import check_model_inputs
   14  

  449          ```python
  450:         >>> from transformers import AutoTokenizer, MistralForCausalLM
  451  

train_real_world/transformers_4573/src/transformers/models/mistral/modular_mistral.py:
  6  
  7: from transformers.utils.generic import check_model_inputs
  8  

train_real_world/transformers_4573/src/transformers/models/mistral3/configuration_mistral3.py:
  51      ```python
  52:     >>> from transformers import Mistral3ForConditionalGeneration, Mistral3Config, PixtralVisionConfig, MistralConfig
  53  

train_real_world/transformers_4573/src/transformers/models/mistral3/convert_mistral3_weights_to_hf.py:
  21  
  22: from transformers import (
  23      Mistral3Config,

  29  )
  30: from transformers.integrations.mistral import convert_tekken_tokenizer
  31  

train_real_world/transformers_4573/src/transformers/models/mistral3/modeling_mistral3.py:
  426          >>> import requests
  427:         >>> from transformers import AutoProcessor, Mistral3ForConditionalGeneration
  428  

train_real_world/transformers_4573/src/transformers/models/mistral3/modular_mistral3.py:
  275          >>> import requests
  276:         >>> from transformers import AutoProcessor, Mistral3ForConditionalGeneration
  277  

train_real_world/transformers_4573/src/transformers/models/mixtral/configuration_mixtral.py:
  102      ```python
  103:     >>> from transformers import MixtralModel, MixtralConfig
  104  

train_real_world/transformers_4573/src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py:
  19  
  20: from transformers import (
  21      MixtralConfig,

  36  ```py
  37: from transformers import MixtralForCausalLM
  38  

train_real_world/transformers_4573/src/transformers/models/mixtral/modeling_mixtral.py:
   33  
   34: from transformers.utils.generic import check_model_inputs
   35  

  630          ```python
  631:         >>> from transformers import AutoTokenizer, MixtralForCausalLM
  632  

train_real_world/transformers_4573/src/transformers/models/mixtral/modular_mixtral.py:
  378          ```python
  379:         >>> from transformers import AutoTokenizer, MixtralForCausalLM
  380  

train_real_world/transformers_4573/src/transformers/models/mlcd/configuration_mlcd.py:
  66      ```python
  67:     >>> from transformers import MLCDVisionConfig, MLCDVisionModel
  68  

train_real_world/transformers_4573/src/transformers/models/mlcd/convert_mlcd_weights_to_hf.py:
  29  
  30: from transformers import CLIPImageProcessor
  31  

train_real_world/transformers_4573/src/transformers/models/mlcd/modeling_mlcd.py:
  539          >>> from PIL import Image
  540:         >>> from transformers import AutoProcessor, MLCDVisionModel
  541          >>> model = MLCDVisionModel.from_pretrained("DeepGlint-AI/mlcd-vit-bigG-patch14-448")

train_real_world/transformers_4573/src/transformers/models/mlcd/modular_mlcd.py:
   86      ```python
   87:     >>> from transformers import MLCDVisionConfig, MLCDVisionModel
   88  

  450          >>> from PIL import Image
  451:         >>> from transformers import AutoProcessor, MLCDVisionModel
  452          >>> model = MLCDVisionModel.from_pretrained("DeepGlint-AI/mlcd-vit-bigG-patch14-448")

train_real_world/transformers_4573/src/transformers/models/mllama/configuration_mllama.py:
   75      ```python
   76:     >>> from transformers import MllamaVisionConfig, MllamaVisionModel
   77  

  195      ```python
  196:     >>> from transformers import MllamaTextModel, MllamaTextConfig
  197  

  282      ```python
  283:     >>> from transformers import MllamaForConditionalGeneration, MllamaConfig, MllamaVisionConfig, MllamaTextConfig
  284  

train_real_world/transformers_4573/src/transformers/models/mllama/convert_mllama_weights_to_hf.py:
  25  
  26: from transformers import (
  27      GenerationConfig,

  32  )
  33: from transformers.convert_slow_tokenizer import TikTokenConverter
  34: from transformers.models.mllama.configuration_mllama import MllamaTextConfig, MllamaVisionConfig
  35: from transformers.models.mllama.image_processing_mllama import get_all_supported_aspect_ratios
  36  

train_real_world/transformers_4573/src/transformers/models/mllama/image_processing_mllama_fast.py:
  180  
  181: # Copied from transformers.models.idefics2.image_processing_idefics2.convert_to_rgb
  182  def convert_to_rgb(image: ImageInput) -> ImageInput:

train_real_world/transformers_4573/src/transformers/models/mllama/image_processing_mllama.py:
  479  
  480: # Copied from transformers.models.idefics2.image_processing_idefics2.convert_to_rgb
  481  def convert_to_rgb(image: ImageInput) -> ImageInput:

train_real_world/transformers_4573/src/transformers/models/mllama/modeling_mllama.py:
   166  
   167: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->MllamaVision
   168  class MllamaVisionMLP(nn.Module):

   182  
   183: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   184  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   195  
   196: # Copied from transformers.models.llama.modeling_llama.eager_attention_forward
   197  def eager_attention_forward(

   367  
   368: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->MllamaText
   369  class MllamaTextRMSNorm(nn.Module):

   476  
   477: # Copied from transformers.models.llama.modeling_llama.rotate_half
   478  def rotate_half(x):

   484  
   485: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
   486  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   581  
   582: # Copied from transformers.models.gemma2.modeling_gemma2.Gemma2MLP with Gemma2->MllamaText
   583  class MllamaTextMLP(nn.Module):

   599  
   600: # Modified from transformers.models.llama.modeling_llama.LlamaDecoderLayer
   601  class MllamaSelfAttentionDecoderLayer(GradientCheckpointingLayer):

   725  
   726: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LlamaConfig->MllamaTextConfig,Llama->Mllama
   727  class MllamaRotaryEmbedding(nn.Module):

   859  
   860:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
   861      def _update_causal_mask(

   929      @staticmethod
   930:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
   931      def _prepare_4d_causal_attention_mask_with_cache_position(

  1073          >>> import requests
  1074:         >>> from transformers import AutoProcessor, MllamaVisionModel
  1075  

  1255          ```python
  1256:         >>> from transformers import AutoProcessor, MllamaTextModel
  1257  

  1394          ```python
  1395:         >>> from transformers import AutoTokenizer, MllamaForCausalLM
  1396  

  1663          >>> import requests
  1664:         >>> from transformers import AutoProcessor, MllamaForConditionalGeneration
  1665  

train_real_world/transformers_4573/src/transformers/models/mllama/processing_mllama.py:
  176          ```python
  177:         from transformers import MllamaProcessor
  178          from PIL import Image

train_real_world/transformers_4573/src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py:
  23  
  24: from transformers import LukeConfig, LukeForMaskedLM, MLukeTokenizer, XLMRobertaTokenizer
  25: from transformers.tokenization_utils_base import AddedToken
  26  

train_real_world/transformers_4573/src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py:
  134      ```python
  135:     >>> from transformers import MMGroundingDinoConfig, MMGroundingDinoModel
  136  

train_real_world/transformers_4573/src/transformers/models/mm_grounding_dino/convert_mm_grounding_dino_to_hf.py:
   21  
   22: from transformers.models.bert.tokenization_bert import BertTokenizer
   23: from transformers.models.grounding_dino.image_processing_grounding_dino import GroundingDinoImageProcessor
   24: from transformers.models.grounding_dino.processing_grounding_dino import GroundingDinoProcessor
   25: from transformers.models.mm_grounding_dino.configuration_mm_grounding_dino import MMGroundingDinoConfig
   26: from transformers.models.mm_grounding_dino.modeling_mm_grounding_dino import MMGroundingDinoForObjectDetection
   27: from transformers.models.swin.configuration_swin import SwinConfig
   28  

  389  
  390: # Copied from transformers/models/siglip2/convert_siglip2_to_hf.py
  391  def convert_old_keys_to_new_keys(state_dict_keys: list) -> dict:

train_real_world/transformers_4573/src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py:
  1972          ```python
  1973:         >>> from transformers import AutoProcessor, AutoModel
  1974          >>> from PIL import Image

  2462          >>> from PIL import Image
  2463:         >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
  2464  

train_real_world/transformers_4573/src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py:
  147      ```python
  148:     >>> from transformers import MMGroundingDinoConfig, MMGroundingDinoModel
  149  

train_real_world/transformers_4573/src/transformers/models/mobilebert/configuration_mobilebert.py:
  87      ```python
  88:     >>> from transformers import MobileBertConfig, MobileBertModel
  89  

train_real_world/transformers_4573/src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py:
  19  
  20: from transformers import MobileBertConfig, MobileBertForPreTraining
  21: from transformers.utils import logging
  22  

train_real_world/transformers_4573/src/transformers/models/mobilebert/modeling_mobilebert.py:
   144  
   145: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   146  def eager_attention_forward(

   720          ```python
   721:         >>> from transformers import AutoTokenizer, MobileBertForPreTraining
   722          >>> import torch

   883          ```python
   884:         >>> from transformers import AutoTokenizer, MobileBertForNextSentencePrediction
   885          >>> import torch

   938  )
   939: # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing
   940  class MobileBertForSequenceClassification(MobileBertPreTrainedModel):

  1020  @auto_docstring
  1021: # Copied from transformers.models.bert.modeling_bert.BertForQuestionAnswering with Bert->MobileBert all-casing
  1022  class MobileBertForQuestionAnswering(MobileBertPreTrainedModel):

  1089  @auto_docstring
  1090: # Copied from transformers.models.bert.modeling_bert.BertForMultipleChoice with Bert->MobileBert all-casing
  1091  class MobileBertForMultipleChoice(MobileBertPreTrainedModel):

  1188  @auto_docstring
  1189: # Copied from transformers.models.bert.modeling_bert.BertForTokenClassification with Bert->MobileBert all-casing
  1190  class MobileBertForTokenClassification(MobileBertPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/mobilenet_v1/configuration_mobilenet_v1.py:
  57      ```python
  58:     >>> from transformers import MobileNetV1Config, MobileNetV1Model
  59  

train_real_world/transformers_4573/src/transformers/models/mobilenet_v1/convert_original_tf_checkpoint_to_pytorch.py:
  26  
  27: from transformers import (
  28      MobileNetV1Config,

  31  )
  32: from transformers.utils import logging
  33  

train_real_world/transformers_4573/src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py:
  118  
  119:     # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize
  120      def resize(

train_real_world/transformers_4573/src/transformers/models/mobilenet_v2/configuration_mobilenet_v2.py:
  73      ```python
  74:     >>> from transformers import MobileNetV2Config, MobileNetV2Model
  75  

train_real_world/transformers_4573/src/transformers/models/mobilenet_v2/convert_original_tf_checkpoint_to_pytorch.py:
  26  
  27: from transformers import (
  28      MobileNetV2Config,

  32  )
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py:
   62  
   63:     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label
   64      def reduce_label(self, labels: list["torch.Tensor"]):

  185  
  186:     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.post_process_semantic_segmentation with Beit->MobileNetV2
  187      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):

train_real_world/transformers_4573/src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py:
  144  
  145:     # Copied from transformers.models.mobilenet_v1.image_processing_mobilenet_v1.MobileNetV1ImageProcessor.resize
  146      def resize(

  194  
  195:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label
  196      def reduce_label(self, label: ImageInput) -> np.ndarray:

  484  
  485:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileNetV2
  486      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):

train_real_world/transformers_4573/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py:
  538          ```python
  539:         >>> from transformers import AutoImageProcessor, MobileNetV2ForSemanticSegmentation
  540          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/mobilevit/configuration_mobilevit.py:
  80      ```python
  81:     >>> from transformers import MobileViTConfig, MobileViTModel
  82  

train_real_world/transformers_4573/src/transformers/models/mobilevit/convert_mlcvnets_to_pytorch.py:
  25  
  26: from transformers import (
  27      MobileViTConfig,

  31  )
  32: from transformers.utils import logging
  33  

train_real_world/transformers_4573/src/transformers/models/mobilevit/image_processing_mobilevit_fast.py:
  60  
  61:     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label
  62      def reduce_label(self, labels: list["torch.Tensor"]):

train_real_world/transformers_4573/src/transformers/models/mobilevit/image_processing_mobilevit.py:
  139  
  140:     # Copied from transformers.models.mobilenet_v1.image_processing_mobilenet_v1.MobileNetV1ImageProcessor.resize with PILImageResampling.BICUBIC->PILImageResampling.BILINEAR
  141      def resize(

  208  
  209:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label
  210      def reduce_label(self, label: ImageInput) -> np.ndarray:

  482  
  483:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileViT
  484      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):

train_real_world/transformers_4573/src/transformers/models/mobilevit/modeling_mobilevit.py:
  910          >>> from PIL import Image
  911:         >>> from transformers import AutoImageProcessor, MobileViTForSemanticSegmentation
  912  

train_real_world/transformers_4573/src/transformers/models/mobilevitv2/configuration_mobilevitv2.py:
  78      ```python
  79:     >>> from transformers import MobileViTV2Config, MobileViTV2Model
  80  

train_real_world/transformers_4573/src/transformers/models/mobilevitv2/convert_mlcvnets_to_pytorch.py:
  27  
  28: from transformers import (
  29      MobileViTImageProcessor,

  33  )
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py:
   41  
   42: # Copied from transformers.models.mobilevit.modeling_mobilevit.make_divisible
   43  def make_divisible(value: int, divisor: int = 8, min_value: Optional[int] = None) -> int:

   59  
   60: # Copied from transformers.models.mobilevit.modeling_mobilevit.MobileViTConvLayer with MobileViT->MobileViTV2
   61  class MobileViTV2ConvLayer(nn.Module):

  124  
  125: # Copied from transformers.models.mobilevit.modeling_mobilevit.MobileViTInvertedResidual with MobileViT->MobileViTV2
  126  class MobileViTV2InvertedResidual(nn.Module):

  173  
  174: # Copied from transformers.models.mobilevit.modeling_mobilevit.MobileViTMobileNetLayer with MobileViT->MobileViTV2
  175  class MobileViTV2MobileNetLayer(nn.Module):

  729  
  730: # Copied from transformers.models.mobilevit.modeling_mobilevit.MobileViTASPPPooling with MobileViT->MobileViTV2
  731  class MobileViTV2ASPPPooling(nn.Module):

  814  
  815: # Copied from transformers.models.mobilevit.modeling_mobilevit.MobileViTDeepLabV3 with MobileViT->MobileViTV2
  816  class MobileViTV2DeepLabV3(nn.Module):

  879          >>> from PIL import Image
  880:         >>> from transformers import AutoImageProcessor, MobileViTV2ForSemanticSegmentation
  881  

train_real_world/transformers_4573/src/transformers/models/modernbert/configuration_modernbert.py:
  119      ```python
  120:     >>> from transformers import ModernBertModel, ModernBertConfig
  121  

train_real_world/transformers_4573/src/transformers/models/modernbert/modular_modernbert.py:
  147      ```python
  148:     >>> from transformers import ModernBertModel, ModernBertConfig
  149  

train_real_world/transformers_4573/src/transformers/models/modernbert_decoder/configuration_modernbert_decoder.py:
  109      ```python
  110:     >>> from transformers import ModernBertDecoderModel, ModernBertDecoderConfig
  111  

train_real_world/transformers_4573/src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py:
  607          ```python
  608:         >>> from transformers import AutoTokenizer, ModernBertDecoderForCausalLM
  609  

train_real_world/transformers_4573/src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py:
  130      ```python
  131:     >>> from transformers import ModernBertDecoderModel, ModernBertDecoderConfig
  132  

  655          ```python
  656:         >>> from transformers import AutoTokenizer, ModernBertDecoderForCausalLM
  657  

train_real_world/transformers_4573/src/transformers/models/moonshine/configuration_moonshine.py:
  103      ```python
  104:     >>> from transformers import MoonshineModel, MoonshineConfig
  105  

train_real_world/transformers_4573/src/transformers/models/moonshine/convert_usefulsensors_to_hf.py:
  22  
  23: from transformers.models.moonshine.modeling_moonshine import MoonshineConfig, MoonshineForConditionalGeneration
  24  

train_real_world/transformers_4573/src/transformers/models/moonshine/modeling_moonshine.py:
    27  
    28: from transformers.utils.generic import OutputRecorder, check_model_inputs
    29  

   947          >>> import torch
   948:         >>> from transformers import AutoFeatureExtractor, MoonshineModel
   949          >>> from datasets import load_dataset

  1063          >>> import torch
  1064:         >>> from transformers import AutoProcessor, MoonshineForConditionalGeneration
  1065          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/moonshine/modular_moonshine.py:
   20  
   21: from transformers.utils.generic import OutputRecorder, check_model_inputs
   22  

  126      ```python
  127:     >>> from transformers import MoonshineModel, MoonshineConfig
  128  

  709          >>> import torch
  710:         >>> from transformers import AutoFeatureExtractor, MoonshineModel
  711          >>> from datasets import load_dataset

  809          >>> import torch
  810:         >>> from transformers import AutoProcessor, MoonshineForConditionalGeneration
  811          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/moshi/configuration_moshi.py:
   89      ```python
   90:     >>> from transformers import (
   91      ...     MoshiDepthConfig,

  220      ```python
  221:     >>> from transformers import (
  222      ...     MoshiConfig,

train_real_world/transformers_4573/src/transformers/models/moshi/convert_moshi_transformers.py:
  22  
  23: from transformers import (
  24      AutoFeatureExtractor,

  31  )
  32: from transformers.convert_slow_tokenizer import MoshiConverter
  33  

train_real_world/transformers_4573/src/transformers/models/moshi/modeling_moshi.py:
   196  
   197: # Copied from transformers.models.gemma.modeling_gemma.GemmaRMSNorm with Gemma->Moshi
   198  class MoshiRMSNorm(nn.Module):

   273  
   274: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Moshi
   275  class MoshiRotaryEmbedding(nn.Module):

   339  
   340: # Copied from transformers.models.llama.modeling_llama.rotate_half
   341  def rotate_half(x):

   347  
   348: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
   349  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

   400  
   401: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   402  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   525  
   526: # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Moshi
   527  # TODO cyril: modular

   648  
   649: # Copied from transformers.models.mimi.modeling_mimi.MimiSdpaAttention with Mimi->Moshi
   650  class MoshiSdpaAttention(MoshiAttention):

  1473  
  1474:     # Copied from transformers.models.gemma.modeling_gemma.GemmaForCausalLM.__init__ with Gemma->Moshi
  1475      def __init__(self, config):

  1509          ```python
  1510:         >>> from transformers import AutoTokenizer, MoshiForCausalLM
  1511  

  1659          ```python
  1660:         >>> from transformers import MoshiForConditionalGeneration
  1661          >>> import torch

  2296      @staticmethod
  2297:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM.apply_delay_pattern_mask
  2298      def apply_delay_pattern_mask(input_ids, decoder_pad_token_mask):

  2364          ```python
  2365:         >>> from transformers import MoshiForConditionalGeneration
  2366  

train_real_world/transformers_4573/src/transformers/models/mpnet/configuration_mpnet.py:
  66      ```python
  67:     >>> from transformers import MPNetModel, MPNetConfig
  68  

train_real_world/transformers_4573/src/transformers/models/mpnet/modeling_mpnet.py:
  216  
  217: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  218  class MPNetIntermediate(nn.Module):

  232  
  233: # Copied from transformers.models.bert.modeling_bert.BertOutput
  234  class MPNetOutput(nn.Module):

  364  
  365: # Copied from transformers.models.bert.modeling_bert.BertPooler
  366  class MPNetPooler(nn.Module):

train_real_world/transformers_4573/src/transformers/models/mpt/configuration_mpt.py:
  157      ```python
  158:     >>> from transformers import MptConfig, MptModel
  159  

train_real_world/transformers_4573/src/transformers/models/mra/configuration_mra.py:
  75      ```python
  76:     >>> from transformers import MraConfig, MraModel
  77  

train_real_world/transformers_4573/src/transformers/models/mra/convert_mra_pytorch_to_pytorch.py:
  20  
  21: from transformers import MraConfig, MraForMaskedLM
  22  

train_real_world/transformers_4573/src/transformers/models/mra/modeling_mra.py:
  616  
  617: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  618  class MraSelfOutput(nn.Module):

  644  
  645: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  646  class MraIntermediate(nn.Module):

  660  
  661: # Copied from transformers.models.bert.modeling_bert.BertOutput
  662  class MraOutput(nn.Module):

  739  
  740: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform
  741  class MraPredictionHeadTransform(nn.Module):

  757  
  758: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Mra
  759  class MraLMPredictionHead(nn.Module):

  774  
  775: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Mra
  776  class MraOnlyMLMHead(nn.Module):

  786  @auto_docstring
  787: # Copied from transformers.models.yoso.modeling_yoso.YosoPreTrainedModel with Yoso->Mra,yoso->mra
  788  class MraPreTrainedModel(PreTrainedModel):

  965  
  966: # Copied from transformers.models.yoso.modeling_yoso.YosoClassificationHead with Yoso->Mra
  967  class MraClassificationHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/mt5/modeling_mt5.py:
    52  
    53: # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->MT5
    54  class MT5LayerNorm(nn.Module):

    78  
    79: # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->MT5
    80  class MT5DenseActDense(nn.Module):

   101  
   102: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->MT5
   103  class MT5DenseGatedActDense(nn.Module):

   131  
   132: # Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->MT5
   133  class MT5LayerFF(nn.Module):

   150  
   151: # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->MT5
   152  class MT5Attention(nn.Module):

   355  
   356: # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->MT5
   357  class MT5LayerSelfAttention(nn.Module):

   390  
   391: # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->MT5
   392  class MT5LayerCrossAttention(nn.Module):

   427  
   428: # Copied from transformers.models.t5.modeling_t5.T5Block with T5->MT5
   429  class MT5Block(GradientCheckpointingLayer):

   521  
   522: # Copied from transformers.models.t5.modeling_t5.T5ClassificationHead with T5->MT5
   523  class MT5ClassificationHead(nn.Module):

   541  @auto_docstring
   542: # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel with T5->MT5, t5->mt5
   543  class MT5PreTrainedModel(PreTrainedModel):

   639  
   640: # Copied from transformers.models.t5.modeling_t5.T5Stack with T5->MT5
   641  class MT5Stack(MT5PreTrainedModel):

   833      ```python
   834:     >>> from transformers import MT5Model, AutoTokenizer
   835  

   854  
   855:     # Copied from transformers.models.t5.modeling_t5.T5Model.__init__ with T5->MT5
   856      def __init__(self, config: MT5Config):

   872  
   873:     # Copied from transformers.models.t5.modeling_t5.T5Model.get_input_embeddings
   874      def get_input_embeddings(self):

   876  
   877:     # Copied from transformers.models.t5.modeling_t5.T5Model.set_input_embeddings
   878      def set_input_embeddings(self, new_embeddings):

   883      @auto_docstring
   884:     # Copied from transformers.models.t5.modeling_t5.T5Model.forward with google-t5/->google/, T5->MT5, t5->mt5
   885      def forward(

   932          ```python
   933:         >>> from transformers import AutoTokenizer, MT5Model
   934  

  1012      ```python
  1013:     >>> from transformers import MT5ForConditionalGeneration, AutoTokenizer
  1014  

  1033  
  1034:     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.__init__ with T5->MT5
  1035      def __init__(self, config: MT5Config):

  1117          ```python
  1118:         >>> from transformers import AutoTokenizer, MT5ForConditionalGeneration
  1119  

  1206  
  1207:     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels
  1208      def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):

  1217      ```python
  1218:     >>> from transformers import MT5EncoderModel, AutoTokenizer
  1219  

  1233  
  1234:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.__init__ with T5->MT5
  1235      def __init__(self, config: MT5Config):

  1246  
  1247:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings
  1248      def get_input_embeddings(self):

  1250  
  1251:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.set_input_embeddings
  1252      def set_input_embeddings(self, new_embeddings):

  1256      @auto_docstring
  1257:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with google-t5/->google/, T5->MT5, t5->mt5
  1258      def forward(

  1280          ```python
  1281:         >>> from transformers import AutoTokenizer, MT5EncoderModel
  1282  

  1313  
  1314:     # Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.__init__ with T5->MT5
  1315      def __init__(self, config: MT5Config):

  1323      @auto_docstring
  1324:     # Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.forward with T5->MT5, t5->mt5
  1325      def forward(

  1456  class MT5ForTokenClassification(MT5PreTrainedModel):
  1457:     # Copied from transformers.models.t5.modeling_t5.T5ForTokenClassification.__init__ with T5->MT5
  1458      def __init__(self, config: MT5Config):

  1469      @auto_docstring
  1470:     # Copied from transformers.models.t5.modeling_t5.T5ForTokenClassification.forward with T5->MT5
  1471      def forward(

  1535  
  1536:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.__init__ with T5->MT5
  1537      def __init__(self, config: MT5Config):

  1558  
  1559:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_input_embeddings
  1560      def get_input_embeddings(self):

  1562  
  1563:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.set_input_embeddings
  1564      def set_input_embeddings(self, new_embeddings):

  1569      @auto_docstring
  1570:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.forward
  1571      def forward(

train_real_world/transformers_4573/src/transformers/models/musicgen/configuration_musicgen.py:
  154      ```python
  155:     >>> from transformers import (
  156      ...     MusicgenConfig,

train_real_world/transformers_4573/src/transformers/models/musicgen/convert_musicgen_transformers.py:
  23  
  24: from transformers import (
  25      AutoFeatureExtractor,

  32  )
  33: from transformers.models.musicgen.modeling_musicgen import MusicgenForCausalLM
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/musicgen/modeling_musicgen.py:
   157  
   158: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   159  def eager_attention_forward(

  1467          ```python
  1468:         >>> from transformers import MusicgenForConditionalGeneration
  1469  

  1662          ```python
  1663:         >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
  1664          >>> import torch

  2339          ```python
  2340:         >>> from transformers import MusicgenForConditionalGeneration
  2341  

train_real_world/transformers_4573/src/transformers/models/musicgen_melody/configuration_musicgen_melody.py:
  161      ```python
  162:     >>> from transformers import (
  163      ...     MusicgenMelodyConfig,

train_real_world/transformers_4573/src/transformers/models/musicgen_melody/convert_musicgen_melody_transformers.py:
  23  
  24: from transformers import (
  25      AutoTokenizer,

  28  )
  29: from transformers.models.musicgen_melody.configuration_musicgen_melody import MusicgenMelodyDecoderConfig
  30: from transformers.models.musicgen_melody.feature_extraction_musicgen_melody import MusicgenMelodyFeatureExtractor
  31: from transformers.models.musicgen_melody.modeling_musicgen_melody import (
  32      MusicgenMelodyForCausalLM,

  34  )
  35: from transformers.models.musicgen_melody.processing_musicgen_melody import MusicgenMelodyProcessor
  36: from transformers.utils import logging
  37  

train_real_world/transformers_4573/src/transformers/models/musicgen_melody/modeling_musicgen_melody.py:
    96  
    97: # Copied from transformers.models.musicgen.modeling_musicgen.shift_tokens_right
    98  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

   117  
   118: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenSinusoidalPositionalEmbedding with Musicgen->MusicgenMelody
   119  class MusicgenMelodySinusoidalPositionalEmbedding(nn.Module):

   163  
   164: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   165  def eager_attention_forward(

   193  
   194: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenAttention with Musicgen->MusicgenMelody
   195  class MusicgenMelodyAttention(nn.Module):

   381  @auto_docstring
   382: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenPreTrainedModel with Musicgen->MusicgenMelody
   383  class MusicgenMelodyPreTrainedModel(PreTrainedModel):

   411  
   412: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody
   413  class MusicgenMelodyDecoder(MusicgenMelodyPreTrainedModel):

   648  @auto_docstring
   649: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenModel with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody
   650  class MusicgenMelodyModel(MusicgenMelodyPreTrainedModel):

   748  )
   749: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody,MusicGen->Musicgen Melody
   750  class MusicgenMelodyForCausalLM(MusicgenMelodyPreTrainedModel, GenerationMixin):

  1331      @classmethod
  1332:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration.from_sub_models_pretrained with Musicgen->MusicgenMelody, musicgen-small->musicgen-melody
  1333      def from_sub_models_pretrained(

  1389          ```python
  1390:         >>> from transformers import MusicgenMelodyForConditionalGeneration
  1391  

  1581          ```python
  1582:         >>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
  1583          >>> import torch

  1758  
  1759:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration._prepare_decoder_input_ids_for_generation
  1760      def _prepare_decoder_input_ids_for_generation(

  1954  
  1955:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration._get_decoder_start_token_id
  1956      def _get_decoder_start_token_id(

train_real_world/transformers_4573/src/transformers/models/musicgen_melody/processing_musicgen_melody.py:
   46  
   47:     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor.get_decoder_prompt_ids
   48      def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):

   61  
   62:     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor.batch_decode with padding_mask->attention_mask
   63      def batch_decode(self, *args, **kwargs):

   80  
   81:     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor._decode_audio with padding_mask->attention_mask
   82      def _decode_audio(self, audio_values, attention_mask: Any = None) -> list[np.ndarray]:

  119          ```python
  120:         >>> from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor
  121  

train_real_world/transformers_4573/src/transformers/models/mvp/configuration_mvp.py:
  87      ```python
  88:     >>> from transformers import MvpConfig, MvpModel
  89  

train_real_world/transformers_4573/src/transformers/models/mvp/modeling_mvp.py:
    49  
    50: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    51  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    66  
    67: # Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->Mvp
    68  class MvpLearnedPositionalEmbedding(nn.Embedding):

   416  
   417: # Copied from transformers.models.bart.modeling_bart.BartClassificationHead with Bart->MVP
   418  class MvpClassificationHead(nn.Module):

  1110          >>> import torch
  1111:         >>> from transformers import AutoTokenizer, MvpForConditionalGeneration
  1112  

  1258          >>> import torch
  1259:         >>> from transformers import AutoTokenizer, MvpForSequenceClassification
  1260  

  1416          >>> import torch
  1417:         >>> from transformers import AutoTokenizer, MvpForQuestionAnswering
  1418  

  1507  
  1508: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Mvp
  1509  class MvpDecoderWrapper(MvpPreTrainedModel):

  1574          ```python
  1575:         >>> from transformers import AutoTokenizer, MvpForCausalLM
  1576  

train_real_world/transformers_4573/src/transformers/models/myt5/convert_myt5_original_tf_checkpoint_to_pytorch.py:
   21  
   22: from transformers import T5Config, T5ForConditionalGeneration
   23: from transformers.utils import logging
   24  

  134  
  135: # Copied from transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch
  136  def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, config_file, pytorch_dump_path):

train_real_world/transformers_4573/src/transformers/models/myt5/tokenization_myt5.py:
  214  
  215:     # Copied from transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.get_vocab
  216      def get_vocab(self):

  220  
  221:     # Copied from transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.get_special_tokens_mask
  222      def get_special_tokens_mask(

  282  
  283:     # Copied from transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.build_inputs_with_special_tokens
  284      def build_inputs_with_special_tokens(

train_real_world/transformers_4573/src/transformers/models/nanochat/configuration_nanochat.py:
  80      ```python
  81:     >>> from transformers import NanoChatModel, NanoChatConfig
  82  

train_real_world/transformers_4573/src/transformers/models/nanochat/convert_nanochat_checkpoints.py:
   23  
   24: from transformers import AutoTokenizer, NanoChatConfig, NanoChatForCausalLM
   25  

  234  
  235:             from transformers.integrations.tiktoken import convert_tiktoken_to_fast
  236  

train_real_world/transformers_4573/src/transformers/models/nanochat/modeling_nanochat.py:
  479          ```python
  480:         >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  481  

train_real_world/transformers_4573/src/transformers/models/nanochat/modular_nanochat.py:
  219          ```python
  220:         >>> from transformers import AutoTokenizer, AutoModelForCausalLM
  221  

train_real_world/transformers_4573/src/transformers/models/nemotron/configuration_nemotron.py:
  90      ```python
  91:     >>> from transformers import NemotronModel, NemotronConfig
  92  

train_real_world/transformers_4573/src/transformers/models/nemotron/convert_nemotron_nemo_to_hf.py:
  27  
  28: from transformers import LlamaTokenizer, PreTrainedTokenizerFast
  29: from transformers.convert_slow_tokenizer import LlamaConverter
  30  

train_real_world/transformers_4573/src/transformers/models/nemotron/modeling_nemotron.py:
   94  
   95: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron
   96  class NemotronRotaryEmbedding(nn.Module):

  163  
  164: # Copied from transformers.models.llama.modeling_llama.rotate_half
  165  def rotate_half(x):

  218  
  219: # Copied from transformers.models.llama.modeling_llama.repeat_kv
  220  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

  317  
  318: # NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron
  319  # TODO cyril: modular

  436  
  437: # NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron
  438  # TODO cyril: modular

  522  
  523: # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron
  524  # no longer copied after attention refactors

  739  
  740:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  741      def _update_causal_mask(

  809      @staticmethod
  810:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  811      def _prepare_4d_causal_attention_mask_with_cache_position(

  865  
  866: # TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron
  867  class NemotronForCausalLM(NemotronPreTrainedModel, GenerationMixin):

  904          ```python
  905:         >>> from transformers import AutoTokenizer, NemotronForCausalLM
  906  

train_real_world/transformers_4573/src/transformers/models/nllb/tokenization_nllb.py:
  48      ```python
  49:     >>> from transformers import NllbTokenizer
  50  

train_real_world/transformers_4573/src/transformers/models/nllb_moe/configuration_nllb_moe.py:
  113      ```python
  114:     >>> from transformers import NllbMoeModel, NllbMoeConfig
  115  

train_real_world/transformers_4573/src/transformers/models/nllb_moe/convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py:
  20  
  21: from transformers import NllbMoeConfig, NllbMoeModel
  22: from transformers.utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME
  23  

train_real_world/transformers_4573/src/transformers/models/nllb_moe/modeling_nllb_moe.py:
   62  
   63: # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding with M2M100->NllbMoe
   64  class NllbMoeSinusoidalPositionalEmbedding(nn.Module):

  148      @staticmethod
  149:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
  150      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

  386  
  387: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  388  def eager_attention_forward(

train_real_world/transformers_4573/src/transformers/models/nougat/convert_nougat_to_hf.py:
   26  
   27: from transformers import (
   28      DonutSwinConfig,

   66  
   67: # Copied from transformers.models.donut.convert_donut_to_pytorch.rename_key
   68  def rename_key(name):

  100  
  101: # Copied from transformers.models.donut.convert_donut_to_pytorch.convert_state_dict
  102  def convert_state_dict(orig_state_dict, model):

train_real_world/transformers_4573/src/transformers/models/nougat/image_processing_nougat.py:
  209  
  210:     # Copied from transformers.models.donut.image_processing_donut.DonutImageProcessor.align_long_axis
  211      def align_long_axis(

  292  
  293:     # Copied from transformers.models.donut.image_processing_donut.DonutImageProcessor.thumbnail
  294      def thumbnail(

  343  
  344:     # Copied from transformers.models.donut.image_processing_donut.DonutImageProcessor.resize
  345      def resize(

train_real_world/transformers_4573/src/transformers/models/nougat/processing_nougat.py:
  20  
  21: from transformers.tokenization_utils_base import PreTokenizedInput, TextInput, TruncationStrategy
  22  

train_real_world/transformers_4573/src/transformers/models/nystromformer/configuration_nystromformer.py:
  75      ```python
  76:     >>> from transformers import NystromformerModel, NystromformerConfig
  77  

train_real_world/transformers_4573/src/transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py:
  21  
  22: from transformers import NystromformerConfig, NystromformerForMaskedLM
  23  

train_real_world/transformers_4573/src/transformers/models/nystromformer/modeling_nystromformer.py:
  236  
  237: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  238  class NystromformerSelfOutput(nn.Module):

  264  
  265: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Nystromformer
  266  class NystromformerIntermediate(nn.Module):

  280  
  281: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Nystromformer
  282  class NystromformerOutput(nn.Module):

  364  
  365: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->Nystromformer
  366  class NystromformerPredictionHeadTransform(nn.Module):

  382  
  383: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Nystromformer
  384  class NystromformerLMPredictionHead(nn.Module):

  399  
  400: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Nystromformer
  401  class NystromformerOnlyMLMHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/olmo/configuration_olmo.py:
  91      ```python
  92:     >>> from transformers import OlmoModel, OlmoConfig
  93  

train_real_world/transformers_4573/src/transformers/models/olmo/convert_olmo_weights_to_hf.py:
  24  
  25: from transformers import OlmoConfig, OlmoForCausalLM
  26: from transformers.models.gpt_neox.tokenization_gpt_neox_fast import GPTNeoXTokenizerFast
  27  

  39  ```py
  40: from transformers import OlmoForCausalLM, AutoTokenizer
  41  

train_real_world/transformers_4573/src/transformers/models/olmo/modeling_olmo.py:
  472          ```python
  473:         >>> from transformers import AutoTokenizer, OlmoForCausalLM
  474  

train_real_world/transformers_4573/src/transformers/models/olmo2/configuration_olmo2.py:
  91      ```python
  92:     >>> from transformers import Olmo2Model, Olmo2Config
  93  

train_real_world/transformers_4573/src/transformers/models/olmo2/convert_olmo2_weights_to_hf.py:
  27  
  28: from transformers import Olmo2Config, Olmo2ForCausalLM
  29: from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast
  30  

  42  ```py
  43: from transformers import Olmo2ForCausalLM, AutoTokenizer
  44  

train_real_world/transformers_4573/src/transformers/models/olmo2/modeling_olmo2.py:
   32  
   33: from transformers.utils.generic import TransformersKwargs
   34  

  476          ```python
  477:         >>> from transformers import AutoTokenizer, Olmo2ForCausalLM
  478  

train_real_world/transformers_4573/src/transformers/models/olmo2/modular_olmo2.py:
   26  
   27: from transformers.utils.generic import TransformersKwargs
   28  

  107      ```python
  108:     >>> from transformers import Olmo2Model, Olmo2Config
  109  

train_real_world/transformers_4573/src/transformers/models/olmo3/configuration_olmo3.py:
  91      ```python
  92:     >>> from transformers import Olmo3Model, Olmo3Config
  93  

train_real_world/transformers_4573/src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py:
  39  
  40: from transformers import AutoTokenizer, Olmo3Config, Olmo3ForCausalLM
  41  

  55  ```py
  56: from transformers import Olmo3ForCausalLM, AutoTokenizer
  57  

train_real_world/transformers_4573/src/transformers/models/olmo3/modeling_olmo3.py:
   27  
   28: from transformers.utils.generic import TransformersKwargs
   29  

  483          ```python
  484:         >>> from transformers import AutoTokenizer, Olmo3ForCausalLM
  485  

train_real_world/transformers_4573/src/transformers/models/olmo3/modular_olmo3.py:
   21  
   22: from transformers.utils.generic import TransformersKwargs
   23  

  107      ```python
  108:     >>> from transformers import Olmo3Model, Olmo3Config
  109  

train_real_world/transformers_4573/src/transformers/models/olmoe/configuration_olmoe.py:
  92      ```python
  93:     >>> from transformers import OlmoeModel, OlmoeConfig
  94  

train_real_world/transformers_4573/src/transformers/models/olmoe/convert_olmoe_weights_to_hf.py:
  23  ```
  24: from transformers import OlmoeForCausalLM, AutoTokenizer
  25  import torch

  49  model = model.to(torch.bfloat16)
  50: from transformers import AutoTokenizer
  51  tokenizer = AutoTokenizer.from_pretrained("../transformers/olmoe")

  72  
  73: from transformers import OlmoeConfig, OlmoeForCausalLM
  74: from transformers.models.gpt_neox.tokenization_gpt_neox_fast import GPTNeoXTokenizerFast
  75  

train_real_world/transformers_4573/src/transformers/models/olmoe/modeling_olmoe.py:
  652          ```python
  653:         >>> from transformers import AutoTokenizer, OlmoeForCausalLM
  654  

train_real_world/transformers_4573/src/transformers/models/olmoe/modular_olmoe.py:
  271          ```python
  272:         >>> from transformers import AutoTokenizer, OlmoeForCausalLM
  273  

train_real_world/transformers_4573/src/transformers/models/omdet_turbo/configuration_omdet_turbo.py:
  134      ```python
  135:     >>> from transformers import OmDetTurboConfig, OmDetTurboForObjectDetection
  136  

train_real_world/transformers_4573/src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py:
  24  
  25: from transformers import (
  26      CLIPTokenizer,

train_real_world/transformers_4573/src/transformers/models/omdet_turbo/modeling_omdet_turbo.py:
   160  @use_kernel_forward_from_hub("MultiScaleDeformableAttention")
   161: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention
   162  class MultiScaleDeformableAttention(nn.Module):

   294  
   295: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OmDetTurbo, Deformable DETR->OmDet-Turbo
   296  class OmDetTurboMultiscaleDeformableAttention(nn.Module):

   406  
   407: # Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrConvNormLayer with RTDetr->OmDetTurbo
   408  class OmDetTurboConvNormLayer(nn.Module):

   428  
   429: # Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrRepVggBlock with RTDetr->OmDetTurbo, activation_function->csp_activation
   430  class OmDetTurboRepVggBlock(nn.Module):

   448  
   449: # Copied from transformers.models.rt_detr.modeling_rt_detr.RTDetrCSPRepLayer with RTDetr->OmDetTurbo, activation_function->csp_activation
   450  class OmDetTurboCSPRepLayer(nn.Module):

  1546  
  1547:         >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection
  1548  

train_real_world/transformers_4573/src/transformers/models/oneformer/configuration_oneformer.py:
  136      ```python
  137:     >>> from transformers import OneFormerConfig, OneFormerModel
  138  

train_real_world/transformers_4573/src/transformers/models/oneformer/convert_to_hf_oneformer.py:
  40      pass
  41: from transformers import CLIPTokenizer, DinatConfig, SwinConfig
  42: from transformers.models.oneformer.image_processing_oneformer import OneFormerImageProcessor
  43: from transformers.models.oneformer.modeling_oneformer import (
  44      OneFormerConfig,

  49  )
  50: from transformers.models.oneformer.processing_oneformer import OneFormerProcessor
  51: from transformers.utils import logging
  52  

train_real_world/transformers_4573/src/transformers/models/oneformer/image_processing_oneformer_fast.py:
  860  
  861:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_panoptic_segmentation
  862      def post_process_panoptic_segmentation(

train_real_world/transformers_4573/src/transformers/models/oneformer/image_processing_oneformer.py:
    90  
    91: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
    92  def max_across_indices(values: Iterable[Any]) -> list[Any]:

    98  
    99: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   100  def get_max_height_width(

   117  
   118: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
   119  def make_pixel_mask(

   136  
   137: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
   138  def binary_mask_to_rle(mask):

   159  
   160: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
   161  def convert_segmentation_to_rle(segmentation):

   181  
   182: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
   183  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

   210  
   211: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
   212  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

   229  
   230: # Copied from transformers.models.detr.image_processing_detr.compute_segments
   231  def compute_segments(

   290  
   291: # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks
   292  def convert_segmentation_map_to_binary_masks(

   498  
   499:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.to_dict
   500      def to_dict(self) -> dict[str, Any]:

   545  
   546:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
   547      def rescale(

   574  
   575:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.convert_segmentation_map_to_binary_masks
   576      def convert_segmentation_map_to_binary_masks(

   787  
   788:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor._pad_image
   789      def _pad_image(

   815  
   816:     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.pad
   817      def pad(

  1100  
  1101:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation
  1102      def post_process_semantic_segmentation(

  1270  
  1271:     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_panoptic_segmentation
  1272      def post_process_panoptic_segmentation(

train_real_world/transformers_4573/src/transformers/models/oneformer/modeling_oneformer.py:
   102  
   103: # Copied from transformers.models.maskformer.modeling_maskformer.dice_loss
   104  def dice_loss(inputs: Tensor, labels: Tensor, num_masks: int) -> Tensor:

   133  
   134: # Copied from transformers.models.mask2former.modeling_mask2former.sigmoid_cross_entropy_loss
   135  def sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor, num_masks: int) -> torch.Tensor:

   153  
   154: # Copied from transformers.models.maskformer.modeling_maskformer.pair_wise_dice_loss
   155  def pair_wise_dice_loss(inputs: Tensor, labels: Tensor) -> Tensor:

   176  
   177: # Copied from transformers.models.mask2former.modeling_mask2former.pair_wise_sigmoid_cross_entropy_loss
   178  def pair_wise_sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:

   204  
   205: # Copied from transformers.models.mask2former.modeling_mask2former.sample_point
   206  def sample_point(

   557  
   558:     # Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerLoss.calculate_uncertainty
   559      def calculate_uncertainty(self, logits: torch.Tensor) -> torch.Tensor:

   575  
   576:     # Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerLoss.sample_points_using_uncertainty
   577      def sample_points_using_uncertainty(

   769  )
   770: # Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoderOutput with Mask2->One
   771  class OneFormerPixelDecoderOutput(ModelOutput):

   937  
   938: # Modified from transformers.models.detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OneFormerPixelDecoderEncoder
   939  class OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):

  1122  
  1123: # Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrEncoder with DeformableDetrEncoder->OneFormerPixelDecoderEncoderOnly
  1124  class OneFormerPixelDecoderEncoderOnly(nn.Module):

  1245  
  1246: # Modified from from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoder with Mask2->One
  1247  class OneFormerPixelDecoder(nn.Module):

  1445  
  1446: # Modified from from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelLevelModule with Mask2->One
  1447  class OneFormerPixelLevelModule(nn.Module):

  1471  
  1472: # Modified from transformers.models.detr.modeling_detr.DetrAttention with Detr->OneFormer
  1473  class OneFormerAttention(nn.Module):

  2351  
  2352: # Copied from transformers.models.maskformer.modeling_maskformer.MaskFormerSinePositionEmbedding with Mask->One
  2353  class OneFormerSinePositionEmbedding(nn.Module):

  2398  
  2399: # Copied from transformers.models.maskformer.modeling_maskformer.PredictionBlock
  2400  class PredictionBlock(nn.Module):

  2859          >>> import requests
  2860:         >>> from transformers import OneFormerProcessor, OneFormerModel
  2861  

  3052          ```python
  3053:         >>> from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation
  3054          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/openai/configuration_openai.py:
  96      ```python
  97:     >>> from transformers import OpenAIGPTConfig, OpenAIGPTModel
  98  

train_real_world/transformers_4573/src/transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py:
  22  
  23: from transformers import OpenAIGPTConfig, OpenAIGPTModel
  24: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, logging
  25  

train_real_world/transformers_4573/src/transformers/models/openai/modeling_openai.py:
  158  
  159: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->OpenAIGPT
  160  class OpenAIGPTSequenceSummary(nn.Module):

  546          ```python
  547:         >>> from transformers import AutoTokenizer, OpenAIGPTDoubleHeadsModel
  548          >>> import torch

train_real_world/transformers_4573/src/transformers/models/opt/configuration_opt.py:
  76      ```python
  77:     >>> from transformers import OPTConfig, OPTModel
  78  

train_real_world/transformers_4573/src/transformers/models/opt/convert_opt_original_pytorch_checkpoint_to_pytorch.py:
  21  
  22: from transformers import OPTConfig, OPTModel
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/opt/modeling_opt.py:
   78  
   79: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   80  def eager_attention_forward(

  351  
  352:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  353      def _update_causal_mask(

  421      @staticmethod
  422:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  423      def _prepare_4d_causal_attention_mask_with_cache_position(

  749          ```python
  750:         >>> from transformers import AutoTokenizer, OPTForCausalLM
  751  

  956          ```python
  957:         >>> from transformers import AutoTokenizer, OPTForQuestionAnswering
  958          >>> import torch

train_real_world/transformers_4573/src/transformers/models/ovis2/configuration_ovis2.py:
  134      ```python
  135:     >>> from transformers import Ovis2ForConditionalGeneration, Ovis2Config
  136  

train_real_world/transformers_4573/src/transformers/models/ovis2/convert_ovis2_weights_to_hf.py:
  23  
  24: from transformers import (
  25      AutoModelForCausalLM,

  29  )
  30: from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES
  31: from transformers.models.ovis2.configuration_ovis2 import Ovis2Config, Ovis2VisionConfig
  32: from transformers.models.ovis2.image_processing_ovis2 import Ovis2ImageProcessor
  33: from transformers.models.ovis2.modeling_ovis2 import Ovis2ForConditionalGeneration
  34: from transformers.models.ovis2.processing_ovis2 import Ovis2Processor
  35: from transformers.models.qwen2.configuration_qwen2 import Qwen2Config
  36  

train_real_world/transformers_4573/src/transformers/models/ovis2/modeling_ovis2.py:
  725          >>> import requests
  726:         >>> from transformers import AutoProcessor, Ovis2ForConditionalGeneration
  727  

train_real_world/transformers_4573/src/transformers/models/ovis2/modular_ovis2.py:
  381          >>> import requests
  382:         >>> from transformers import AutoProcessor, Ovis2ForConditionalGeneration
  383  

train_real_world/transformers_4573/src/transformers/models/owlv2/configuration_owlv2.py:
   23  
   24: # Copied from transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig with OwlViT->Owlv2, owlvit-base-patch32->owlv2-base-patch16, owlvit->owlv2, OWL-ViT->OWLv2
   25  class Owlv2TextConfig(PreTrainedConfig):

   72      ```python
   73:     >>> from transformers import Owlv2TextConfig, Owlv2TextModel
   74  

  120  
  121: # Copied from transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig with OwlViT->Owlv2, owlvit-base-patch32->owlv2-base-patch16, owlvit->owlv2, OWL-ViT->OWLv2, 32->16
  122  class Owlv2VisionConfig(PreTrainedConfig):

  162      ```python
  163:     >>> from transformers import Owlv2VisionConfig, Owlv2VisionModel
  164  

  209  
  210: # Copied from transformers.models.owlvit.configuration_owlvit.OwlViTConfig with OwlViT->Owlv2, owlvit-base-patch32->owlv2-base-patch16, owlvit->owlv2, OWL-ViT->OWLv2
  211  class Owlv2Config(PreTrainedConfig):

train_real_world/transformers_4573/src/transformers/models/owlv2/convert_owlv2_to_hf.py:
  30  
  31: from transformers import (
  32      CLIPTokenizer,

  39  )
  40: from transformers.utils import logging
  41  

train_real_world/transformers_4573/src/transformers/models/owlv2/image_processing_owlv2.py:
   99  
  100: # Copied from transformers.models.owlvit.image_processing_owlvit._upcast
  101  def _upcast(t):

  108  
  109: # Copied from transformers.models.owlvit.image_processing_owlvit.box_area
  110  def box_area(boxes):

  124  
  125: # Copied from transformers.models.owlvit.image_processing_owlvit.box_iou
  126  def box_iou(boxes1, boxes2):

  496  
  497:     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection with OwlViT->Owlv2
  498      def post_process_object_detection(

  550  
  551:     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_image_guided_detection
  552      def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_threshold=0.3, target_sizes=None):

train_real_world/transformers_4573/src/transformers/models/owlv2/modeling_owlv2.py:
    40  if is_vision_available():
    41:     from transformers.image_transforms import center_to_corners_format
    42  

    49  
    50: # Copied from transformers.models.clip.modeling_clip.contrastive_loss with clip->owlv2
    51  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

    54  
    55: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->owlv2
    56  def owlv2_loss(similarity: torch.Tensor) -> torch.Tensor:

    99  
   100: # Copied from transformers.loss.loss_for_object_detection._upcast
   101  def _upcast(t: Tensor) -> Tensor:

   108  
   109: # Copied from transformers.loss.loss_for_object_detection.box_area
   110  def box_area(boxes: Tensor) -> Tensor:

   125  
   126: # Copied from transformers.loss.loss_for_object_detection.box_iou
   127  def box_iou(boxes1, boxes2):

   142  
   143: # Copied from transformers.loss.loss_for_object_detection.generalized_box_iou
   144  def generalized_box_iou(boxes1, boxes2):

   229  )
   230: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput with OwlViT->Owlv2,OWL-ViT->OWLv2
   231  class Owlv2ImageGuidedObjectDetectionOutput(ModelOutput):

   275  
   276: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionEmbeddings with OwlViT->Owlv2
   277  class Owlv2VisionEmbeddings(nn.Module):

   297  
   298:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.interpolate_pos_encoding
   299      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   353  
   354: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextEmbeddings with OwlViT->Owlv2
   355  class Owlv2TextEmbeddings(nn.Module):

   385  
   386: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTAttention with OwlViT->Owlv2
   387  class Owlv2Attention(nn.Module):

   493  
   494: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Owlv2
   495  class Owlv2MLP(nn.Module):

   509  
   510: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->Owlv2
   511  class Owlv2EncoderLayer(GradientCheckpointingLayer):

   561  @auto_docstring
   562: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTPreTrainedModel with OwlViT->Owlv2,owlvit->owlv2
   563  class Owlv2PreTrainedModel(PreTrainedModel):

   615  
   616: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTEncoder with OwlViT->Owlv2
   617  class Owlv2Encoder(nn.Module):

   696  
   697: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextTransformer with OWLVIT->OWLV2,OwlViT->Owlv2
   698  class Owlv2TextTransformer(nn.Module):

   773  
   774: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextModel with google/owlvit-base-patch32->google/owlv2-base-patch16, OWLVIT->OWLV2,OwlViT->Owlv2
   775  class Owlv2TextModel(Owlv2PreTrainedModel):

   808          ```python
   809:         >>> from transformers import AutoProcessor, Owlv2TextModel
   810  

   830  
   831: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionTransformer with OWLVIT->OWLV2,OwlViT->Owlv2
   832  class Owlv2VisionTransformer(nn.Module):

   886  
   887: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionModel with OWLVIT->OWLV2,OwlViT->Owlv2,google/owlvit-base-patch32->google/owlv2-base-patch16
   888  class Owlv2VisionModel(Owlv2PreTrainedModel):

   916          >>> import requests
   917:         >>> from transformers import AutoProcessor, Owlv2VisionModel
   918  

   939  @auto_docstring
   940: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTModel with google/owlvit-base-patch32->google/owlv2-base-patch16-ensemble, OWLVIT->OWLV2,OwlViT->Owlv2,owlvit->owlv2,OWL-ViT->OWLv2
   941  class Owlv2Model(Owlv2PreTrainedModel):

   995          >>> import torch
   996:         >>> from transformers import AutoProcessor, Owlv2Model
   997  

  1026          >>> import torch
  1027:         >>> from transformers.image_utils import load_image
  1028:         >>> from transformers import AutoProcessor, Owlv2Model
  1029  

  1071          >>> import requests
  1072:         >>> from transformers import AutoProcessor, Owlv2Model
  1073  

  1142  
  1143: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTBoxPredictionHead with OwlViT->Owlv2
  1144  class Owlv2BoxPredictionHead(nn.Module):

  1162  
  1163: # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTClassPredictionHead with OwlViT->Owlv2
  1164  class Owlv2ClassPredictionHead(nn.Module):

  1235      @staticmethod
  1236:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.normalize_grid_corner_coordinates
  1237      def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:

  1266  
  1267:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.compute_box_bias
  1268      def compute_box_bias(

  1289  
  1290:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.box_predictor
  1291      def box_predictor(

  1323  
  1324:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.class_predictor
  1325      def class_predictor(

  1343  
  1344:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.image_text_embedder with owlvit->owlv2
  1345      def image_text_embedder(

  1395  
  1396:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.image_embedder with owlvit->owlv2, OwlViTModel->Owlv2Model
  1397      def image_embedder(

  1438  
  1439:     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.embed_image_query
  1440      def embed_image_query(

  1502          >>> import torch
  1503:         >>> from transformers import AutoProcessor, Owlv2ForObjectDetection
  1504  

  1628  
  1629:         >>> from transformers import Owlv2Processor, Owlv2ForObjectDetection
  1630  

train_real_world/transformers_4573/src/transformers/models/owlv2/processing_owlv2.py:
   70  
   71:     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OwlViT->Owlv2
   72      def __call__(

  171  
  172:     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_grounded_object_detection with OwlViT->Owlv2
  173      def post_process_grounded_object_detection(

  220  
  221:     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_image_guided_detection with OwlViT->Owlv2
  222      def post_process_image_guided_detection(

train_real_world/transformers_4573/src/transformers/models/owlvit/configuration_owlvit.py:
   71      ```python
   72:     >>> from transformers import OwlViTTextConfig, OwlViTTextModel
   73  

  160      ```python
  161:     >>> from transformers import OwlViTVisionConfig, OwlViTVisionModel
  162  

train_real_world/transformers_4573/src/transformers/models/owlvit/convert_owlvit_original_flax_to_hf.py:
  28  
  29: from transformers import (
  30      CLIPTokenizer,

train_real_world/transformers_4573/src/transformers/models/owlvit/image_processing_owlvit_fast.py:
   49  
   50:     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection
   51      def post_process_object_detection(

  103  
  104:     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_image_guided_detection
  105      def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_threshold=0.3, target_sizes=None):

train_real_world/transformers_4573/src/transformers/models/owlvit/image_processing_owlvit.py:
  273  
  274:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
  275      def rescale(

train_real_world/transformers_4573/src/transformers/models/owlvit/modeling_owlvit.py:
    40  if is_vision_available():
    41:     from transformers.image_transforms import center_to_corners_format
    42  

    49  
    50: # Copied from transformers.models.clip.modeling_clip.contrastive_loss with clip->owlvit
    51  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

    54  
    55: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->owlvit
    56  def owlvit_loss(similarity: torch.Tensor) -> torch.Tensor:

    99  
   100: # Copied from transformers.loss.loss_for_object_detection._upcast
   101  def _upcast(t: Tensor) -> Tensor:

   108  
   109: # Copied from transformers.loss.loss_for_object_detection.box_area
   110  def box_area(boxes: Tensor) -> Tensor:

   125  
   126: # Copied from transformers.loss.loss_for_object_detection.box_iou
   127  def box_iou(boxes1, boxes2):

   142  
   143: # Copied from transformers.loss.loss_for_object_detection.generalized_box_iou
   144  def generalized_box_iou(boxes1, boxes2):

   291  
   292:     # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.interpolate_pos_encoding
   293      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   481  
   482: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->OwlViT
   483  class OwlViTMLP(nn.Module):

   497  
   498: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->OwlViT
   499  class OwlViTEncoderLayer(GradientCheckpointingLayer):

   792          ```python
   793:         >>> from transformers import AutoProcessor, OwlViTTextModel
   794  

   898          >>> import requests
   899:         >>> from transformers import AutoProcessor, OwlViTVisionModel
   900  

   976          >>> import torch
   977:         >>> from transformers import AutoProcessor, OwlViTModel
   978  

  1007          >>> import torch
  1008:         >>> from transformers.image_utils import load_image
  1009:         >>> from transformers import AutoProcessor, OwlViTModel
  1010  

  1052          >>> import requests
  1053:         >>> from transformers import AutoProcessor, OwlViTModel
  1054  

  1458          >>> import torch
  1459:         >>> from transformers import AutoProcessor, OwlViTForObjectDetection
  1460  

  1569  
  1570:         >>> from transformers import OwlViTProcessor, OwlViTForObjectDetection
  1571  

train_real_world/transformers_4573/src/transformers/models/paddleocr_vl/configuration_paddleocr_vl.py:
   70      ```python
   71:     >>> from transformers import PaddleOCRVisionConfig, PaddleOCRVisionModel
   72  

  175      ```python
  176:     >>> from transformers import PaddleOCRTextModel, PaddleOCRTextConfig
  177  

  283      ```python
  284:     >>> from transformers import PaddleOCRVLForConditionalGeneration, PaddleOCRVLConfig
  285  

train_real_world/transformers_4573/src/transformers/models/paddleocr_vl/modeling_paddleocr_vl.py:
  1401          ```python
  1402:         >>> from transformers import AutoProcessor, PaddleOCRVLForConditionalGeneration
  1403  

train_real_world/transformers_4573/src/transformers/models/paddleocr_vl/modular_paddleocr_vl.py:
   599      ```python
   600:     >>> from transformers import PaddleOCRVisionConfig, PaddleOCRVisionModel
   601  

   664      ```python
   665:     >>> from transformers import PaddleOCRVLForConditionalGeneration, PaddleOCRVLConfig
   666  

  1278          ```python
  1279:         >>> from transformers import AutoProcessor, PaddleOCRVLForConditionalGeneration
  1280  

train_real_world/transformers_4573/src/transformers/models/paligemma/configuration_paligemma.py:
  52      ```python
  53:     >>> from transformers import PaliGemmaForConditionalGeneration, PaliGemmaConfig, SiglipVisionConfig, GemmaConfig
  54  

train_real_world/transformers_4573/src/transformers/models/paligemma/convert_paligemma_weights_to_hf.py:
  22  
  23: from transformers import (
  24      AutoTokenizer,

  31  )
  32: from transformers.tokenization_utils_base import AddedToken
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py:
  24  
  25: from transformers import (
  26      AutoTokenizer,

  32  )
  33: from transformers.tokenization_utils_base import AddedToken
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/paligemma/modeling_paligemma.py:
  254  
  255:     # Copied from transformers.models.llava.modeling_llava.LlavaModel.get_input_embeddings with Llava->PaliGemma
  256      def get_input_embeddings(self):

  258  
  259:     # Copied from transformers.models.llava.modeling_llava.LlavaModel.set_input_embeddings with Llava->PaliGemma
  260      def set_input_embeddings(self, value):

  332          >>> import requests
  333:         >>> from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
  334  

  483          >>> import requests
  484:         >>> from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
  485  

train_real_world/transformers_4573/src/transformers/models/paligemma/processing_paligemma.py:
  58  
  59: # Copied from transformers.models.idefics2.processing_idefics2.is_url
  60  def is_url(val) -> bool:

  63  
  64: # Copied from transformers.models.idefics2.processing_idefics2.is_image_or_image_url
  65  def is_image_or_image_url(elem):

train_real_world/transformers_4573/src/transformers/models/parakeet/configuration_parakeet.py:
   79          ```python
   80:         >>> from transformers import ParakeetEncoderModel, ParakeetEncoderConfig
   81  

  179          ```python
  180:         >>> from transformers import ParakeetForCTC, ParakeetCTCConfig
  181  

train_real_world/transformers_4573/src/transformers/models/parakeet/convert_nemo_to_hf.py:
  25  
  26: from transformers import (
  27      ParakeetCTCConfig,

  34  )
  35: from transformers.convert_slow_tokenizer import ParakeetConverter
  36: from transformers.utils.hub import cached_file
  37  

train_real_world/transformers_4573/src/transformers/models/parakeet/modeling_parakeet.py:
  592          ```python
  593:         >>> from transformers import AutoProcessor, ParakeetEncoder
  594          >>> from datasets import load_dataset, Audio

  701          ```python
  702:         >>> from transformers import AutoProcessor, ParakeetForCTC
  703          >>> from datasets import load_dataset, Audio

  773          ```python
  774:         >>> from transformers import AutoProcessor, ParakeetForCTC
  775          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/parakeet/modular_parakeet.py:
  428          ```python
  429:         >>> from transformers import AutoProcessor, ParakeetEncoder
  430          >>> from datasets import load_dataset, Audio

  537          ```python
  538:         >>> from transformers import AutoProcessor, ParakeetForCTC
  539          >>> from datasets import load_dataset, Audio

  609          ```python
  610:         >>> from transformers import AutoProcessor, ParakeetForCTC
  611          >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/patchtsmixer/configuration_patchtsmixer.py:
  131      ```python
  132:     >>> from transformers import PatchTSMixerConfig, PatchTSMixerModel
  133  

train_real_world/transformers_4573/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py:
    24  
    25: from transformers.modeling_utils import PreTrainedModel
    26: from transformers.utils import ModelOutput
    27  

    59  
    60: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTBatchNorm with PatchTST->PatchTSMixer
    61  class PatchTSMixerBatchNorm(nn.Module):

   241  
   242: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   243  def eager_attention_forward(

   271  
   272: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->PatchTSMixer
   273  class PatchTSMixerAttention(nn.Module):

   741  
   742: # Copied from transformers.models.patchtst.modeling_patchtst.random_masking
   743  def random_masking(

   800  
   801: # Copied from transformers.models.patchtst.modeling_patchtst.forecast_masking
   802  def forecast_masking(

   869  
   870: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTPatchify with PatchTST->PatchTSMixer
   871  class PatchTSMixerPatchify(nn.Module):

   918  
   919: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTMasking with PatchTST->PatchTSMixer
   920  class PatchTSMixerMasking(nn.Module):

   979  
   980: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTStdScaler with PatchTST->PatchTSMixer
   981  class PatchTSMixerStdScaler(nn.Module):

  1015  
  1016: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTMeanScaler with PatchTST->PatchTSMixer
  1017  class PatchTSMixerMeanScaler(nn.Module):

  1070  
  1071: # Copied from transformers.models.patchtst.modeling_patchtst.PatchTSTNOPScaler with PatchTST->PatchTSMixer
  1072  class PatchTSMixerNOPScaler(nn.Module):

  1497  
  1498: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll
  1499  def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:

  1505  
  1506: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average
  1507  def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:

train_real_world/transformers_4573/src/transformers/models/patchtst/configuration_patchtst.py:
   18  
   19: from transformers.configuration_utils import PreTrainedConfig
   20: from transformers.utils import logging
   21  

  128      ```python
  129:     >>> from transformers import PatchTSTConfig, PatchTSTModel
  130  

train_real_world/transformers_4573/src/transformers/models/patchtst/modeling_patchtst.py:
    39  
    40: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
    41  def eager_attention_forward(

    69  
    70: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention with Wav2Vec2->PatchTST
    71  class PatchTSTAttention(nn.Module):

   900  
   901: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll
   902  def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:

   908  
   909: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average
   910  def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:

   933  
   934: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST
   935  class PatchTSTStdScaler(nn.Module):

   969  
   970: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST
   971  class PatchTSTMeanScaler(nn.Module):

  1024  
  1025: # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST
  1026  class PatchTSTNOPScaler(nn.Module):

  1137          >>> import torch
  1138:         >>> from transformers import PatchTSTModel
  1139  

  1272          >>> import torch
  1273:         >>> from transformers import PatchTSTConfig, PatchTSTForPretraining
  1274  

  1422          ```python
  1423:         >>> from transformers import PatchTSTConfig, PatchTSTForClassification
  1424  

  1643          >>> import torch
  1644:         >>> from transformers import PatchTSTConfig, PatchTSTForPrediction
  1645  

  1878          ```python
  1879:         >>> from transformers import PatchTSTConfig, PatchTSTForRegression
  1880  

train_real_world/transformers_4573/src/transformers/models/pe_audio/configuration_pe_audio.py:
   68      ```python
   69:     >>> from transformers import PeAudioEncoder, PeAudioEncoderConfig
   70  

  156      ```python
  157:     >>> from transformers import PeAudioModel, PeAudioConfig
  158  

train_real_world/transformers_4573/src/transformers/models/pe_audio_video/configuration_pe_audio_video.py:
   72      ```python
   73:     >>> from transformers import PeAudioVideoEncoder, PeAudioVideoEncoderConfig
   74  

  162      ```python
  163:     >>> from transformers import PeAudioVideoModel, PeAudioVideoConfig
  164  

train_real_world/transformers_4573/src/transformers/models/pe_audio_video/convert_pe_audio_video_to_hf.py:
  19  
  20: from transformers.models.pe_audio_video.modeling_pe_audio_video import PeAudioVideoConfig, PeAudioVideoModel
  21: from transformers.utils import cached_file
  22  

train_real_world/transformers_4573/src/transformers/models/pe_video/configuration_pe_video.py:
   69      ```python
   70:     >>> from transformers import PeAudioEncoder, PeAudioEncoderConfig
   71  

  161      ```python
  162:     >>> from transformers import PeVideoModel, PeVideoConfig
  163  

train_real_world/transformers_4573/src/transformers/models/pegasus/configuration_pegasus.py:
  83      ```python
  84:     >>> from transformers import PegasusConfig, PegasusModel
  85  

train_real_world/transformers_4573/src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py:
  23  
  24: from transformers import PegasusConfig, PegasusForConditionalGeneration, PegasusTokenizer
  25: from transformers.models.pegasus.configuration_pegasus import DEFAULTS, task_specific_params
  26  

train_real_world/transformers_4573/src/transformers/models/pegasus/modeling_pegasus.py:
    54  
    55: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    56  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    71  
    72: # Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->Pegasus
    73  class PegasusSinusoidalPositionalEmbedding(nn.Embedding):

   106  
   107: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   108  def eager_attention_forward(

   136  
   137: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Pegasus
   138  class PegasusAttention(nn.Module):

   262  
   263: # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Pegasus, MBART->PEGASUS
   264  class PegasusEncoderLayer(GradientCheckpointingLayer):

   322  
   323: # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Pegasus, MBART->PEGASUS
   324  class PegasusDecoderLayer(GradientCheckpointingLayer):

   973          ```python
   974:         >>> from transformers import AutoTokenizer, PegasusModel
   975  

  1143          ```python
  1144:         >>> from transformers import AutoTokenizer, PegasusForConditionalGeneration
  1145  

  1214  
  1215: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Pegasus
  1216  class PegasusDecoderWrapper(PegasusPreTrainedModel):

  1276      @auto_docstring
  1277:     # Copied from transformers.models.bart.modeling_bart.BartForCausalLM.forward with Bart->Pegasus, facebook/bart-base->google/pegasus-large
  1278      def forward(

  1303          ```python
  1304:         >>> from transformers import AutoTokenizer, PegasusForCausalLM
  1305  

train_real_world/transformers_4573/src/transformers/models/pegasus_x/configuration_pegasus_x.py:
  88      ```python
  89:     >>> from transformers import PegasusXConfig, PegasusXModel
  90  

train_real_world/transformers_4573/src/transformers/models/pegasus_x/modeling_pegasus_x.py:
    62  
    63: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    64  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    79  
    80: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->PegasusX
    81  class PegasusXScaledWordEmbedding(nn.Embedding):

   123  
   124: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   125  def eager_attention_forward(

   153  
   154: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PegasusX
   155  class PegasusXAttention(nn.Module):

  1266          ```python
  1267:         >>> from transformers import AutoTokenizer, PegasusModel
  1268  

  1468  
  1469: # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->PegasusX
  1470  class PegasusXDecoderWrapper(PegasusXPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/perceiver/configuration_perceiver.py:
  96      ```python
  97:     >>> from transformers import PerceiverModel, PerceiverConfig
  98  

train_real_world/transformers_4573/src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py:
  29  
  30: from transformers import (
  31      PerceiverConfig,

  40  )
  41: from transformers.utils import logging
  42  

train_real_world/transformers_4573/src/transformers/models/perceiver/image_processing_perceiver.py:
  162  
  163:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC
  164      def resize(

train_real_world/transformers_4573/src/transformers/models/perceiver/modeling_perceiver.py:
   633          ```python
   634:         >>> from transformers import PerceiverConfig, PerceiverTokenizer, PerceiverImageProcessor, PerceiverModel
   635:         >>> from transformers.models.perceiver.modeling_perceiver import (
   636          ...     PerceiverTextPreprocessor,

   870          ```python
   871:         >>> from transformers import AutoTokenizer, PerceiverForMaskedLM
   872          >>> import torch

   996          ```python
   997:         >>> from transformers import AutoTokenizer, PerceiverForSequenceClassification
   998  

  1129          ```python
  1130:         >>> from transformers import AutoImageProcessor, PerceiverForImageClassificationLearned
  1131          >>> from PIL import Image

  1252          ```python
  1253:         >>> from transformers import AutoImageProcessor, PerceiverForImageClassificationFourier
  1254          >>> from PIL import Image

  1374          ```python
  1375:         >>> from transformers import AutoImageProcessor, PerceiverForImageClassificationConvProcessing
  1376          >>> from PIL import Image

  1510          ```python
  1511:         >>> from transformers import PerceiverForOpticalFlow
  1512          >>> import torch

  1724          ```python
  1725:         >>> from transformers import PerceiverForMultimodalAutoencoding
  1726          >>> import torch

train_real_world/transformers_4573/src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py:
  24  
  25: from transformers import (
  26      GenerationConfig,

  30  )
  31: from transformers.convert_slow_tokenizer import TikTokenConverter
  32: from transformers.models.auto.modeling_auto import AutoModel
  33: from transformers.models.perception_lm.configuration_perception_lm import (
  34      PerceptionLMConfig,
  35  )
  36: from transformers.models.perception_lm.image_processing_perception_lm_fast import (
  37      PerceptionLMImageProcessorFast,
  38  )
  39: from transformers.models.perception_lm.modeling_perception_lm import (
  40      PerceptionLMForConditionalGeneration,
  41  )
  42: from transformers.models.perception_lm.processing_perception_lm import (
  43      PerceptionLMProcessor,
  44  )
  45: from transformers.models.perception_lm.video_processing_perception_lm import (
  46      PerceptionLMVideoProcessor,
  47  )
  48: from transformers.models.timm_wrapper.configuration_timm_wrapper import TimmWrapperConfig
  49  

  51  try:
  52:     from transformers import LlamaTokenizerFast
  53  except ImportError as e:

  70  ```py
  71: from transformers import LlamaForCausalLM, LlamaTokenizer
  72  

train_real_world/transformers_4573/src/transformers/models/perception_lm/modeling_perception_lm.py:
  364          ```python
  365:         from transformers import AutoProcessor, AutoModelForImageTextToText
  366          from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/perception_lm/modular_perception_lm.py:
  348          ```python
  349:         from transformers import AutoProcessor, AutoModelForImageTextToText
  350          from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/persimmon/configuration_persimmon.py:
  76      ```python
  77:     >>> from transformers import PersimmonModel, PersimmonConfig
  78  

train_real_world/transformers_4573/src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py:
  20  
  21: from transformers import LlamaTokenizer, PersimmonConfig, PersimmonForCausalLM
  22  

  24  try:
  25:     from transformers import LlamaTokenizerFast
  26  

  47  ```py
  48: from transformers import PersimmonForCausalLM, PersimmonTokenizer
  49  

train_real_world/transformers_4573/src/transformers/models/persimmon/modeling_persimmon.py:
   61  
   62: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Persimmon
   63  class PersimmonRotaryEmbedding(nn.Module):

  130  
  131: # Copied from transformers.models.llama.modeling_llama.rotate_half
  132  def rotate_half(x):

  138  
  139: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
  140  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  166  
  167: # Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP with GPTNeoX->Persimmon
  168  class PersimmonMLP(nn.Module):

  546  
  547:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  548      def _update_causal_mask(

  616      @staticmethod
  617:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  618      def _prepare_4d_causal_attention_mask_with_cache_position(

  675  
  676:     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with LLAMA->PERSIMMON,Llama->Persimmon
  677      def __init__(self, config):

  711          ```python
  712:         >>> from transformers import AutoTokenizer, PersimmonForCausalLM
  713  

train_real_world/transformers_4573/src/transformers/models/phi/configuration_phi.py:
  91      ```python
  92:     >>> from transformers import PhiModel, PhiConfig
  93  

train_real_world/transformers_4573/src/transformers/models/phi/convert_phi_weights_to_hf.py:
  32  
  33: from transformers import PhiConfig, PhiForCausalLM
  34  

train_real_world/transformers_4573/src/transformers/models/phi/modeling_phi.py:
  495          ```python
  496:         >>> from transformers import AutoTokenizer, PhiForCausalLM
  497  

train_real_world/transformers_4573/src/transformers/models/phi3/configuration_phi3.py:
  95      ```python
  96:     >>> from transformers import Phi3Model, Phi3Config
  97  

train_real_world/transformers_4573/src/transformers/models/phi3/modeling_phi3.py:
   28  
   29: from transformers.utils.generic import check_model_inputs
   30  

  481          ```python
  482:         >>> from transformers import AutoTokenizer, Phi3ForCausalLM
  483  

train_real_world/transformers_4573/src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py:
   69      ```python
   70:     >>> from transformers import Phi4MultimodalVisionConfig
   71  

  175      ```python
  176:     >>> from transformers import Phi4MultimodalAudioConfig
  177  

  321      ```python
  322:     >>> from transformers import Phi4MultimodalModel, Phi4MultimodalConfig
  323  

train_real_world/transformers_4573/src/transformers/models/phi4_multimodal/convert_phi4_multimodal_weights_to_hf.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

train_real_world/transformers_4573/src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py:
  1691          ```python
  1692:         >>> from transformers import AutoTokenizer, Phi4MultimodalForCausalLM
  1693          >>> model = Phi4MultimodalForCausalLM.from_pretrained("TBA")

train_real_world/transformers_4573/src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py:
   106      ```python
   107:     >>> from transformers import Phi4MultimodalVisionConfig
   108  

   212      ```python
   213:     >>> from transformers import Phi4MultimodalAudioConfig
   214  

   358      ```python
   359:     >>> from transformers import Phi4MultimodalModel, Phi4MultimodalConfig
   360  

  1622          ```python
  1623:         >>> from transformers import AutoTokenizer, Phi4MultimodalForCausalLM
  1624          >>> model = Phi4MultimodalForCausalLM.from_pretrained("TBA")

train_real_world/transformers_4573/src/transformers/models/phimoe/configuration_phimoe.py:
  101      ```python
  102:     >>> from transformers import PhimoeModel, PhimoeConfig
  103      >>> # Initializing a Phi-3 style configuration

train_real_world/transformers_4573/src/transformers/models/phimoe/modeling_phimoe.py:
  844          ```python
  845:         >>> from transformers import AutoTokenizer, PhimoeForCausalLM
  846  

  905  
  906:     # Copied from transformers.models.phi3.modeling_phi3.Phi3ForCausalLM.prepare_inputs_for_generation
  907      def prepare_inputs_for_generation(

train_real_world/transformers_4573/src/transformers/models/phimoe/modular_phimoe.py:
  357  
  358:     # Copied from transformers.models.phi3.modeling_phi3.Phi3ForCausalLM.prepare_inputs_for_generation
  359      def prepare_inputs_for_generation(

train_real_world/transformers_4573/src/transformers/models/pix2struct/configuration_pix2struct.py:
   72      ```python
   73:     >>> from transformers import Pix2StructTextConfig, Pix2StructTextModel
   74  

  194      ```python
  195:     >>> from transformers import Pix2StructVisionConfig, Pix2StructVisionModel
  196  

  274      ```python
  275:     >>> from transformers import Pix2StructConfig, Pix2StructForConditionalGeneration
  276  

train_real_world/transformers_4573/src/transformers/models/pix2struct/convert_pix2struct_original_pytorch_to_hf.py:
  22  
  23: from transformers import (
  24      AutoTokenizer,

train_real_world/transformers_4573/src/transformers/models/pix2struct/modeling_pix2struct.py:
    58  
    59: # Adapted from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Pix2Struct
    60  class Pix2StructLayerNorm(nn.Module):

   223  
   224: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5DenseGatedActDense->Pix2StructVisionMlp,T5Config->Pix2StructVisionConfig,config.d_model->config.hidden_size,dropout_rate->dropout_rate
   225  class Pix2StructVisionMlp(nn.Module):

   429  
   430:     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->Pix2Struct
   431      def _shift_right(self, input_ids):

   496          >>> from PIL import Image
   497:         >>> from transformers import AutoProcessor, Pix2StructVisionModel
   498  

   549  
   550: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->Pix2StructText,d_model->hidden_size
   551  class Pix2StructTextDenseGatedActDense(nn.Module):

   587  
   588:     # Copied from transformers.models.t5.modeling_t5.T5LayerFF.forward
   589      def forward(self, hidden_states):

   627      @staticmethod
   628:     # Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket
   629      def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):

   675  
   676:     # Adapted from transformers.models.t5.modeling_t5.T5Attention.compute_bias
   677      def compute_bias(self, query_length, key_length, device=None, cache_position=None):

   696  
   697:     # Adapted from transformers.models.t5.modeling_t5.T5Attention.forward
   698      def forward(

   797  
   798: # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,T5LayerSelfAttention->Pix2StructTextLayerSelfAttention,self.SelfAttention->self.attention,config.d_model->config.hidden_size
   799  class Pix2StructTextLayerSelfAttention(nn.Module):

   832  
   833: # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,T5LayerCrossAttention->Pix2StructTextLayerCrossAttention,self.EncDecAttention->self.attention,config.d_model->config.hidden_size
   834  class Pix2StructTextLayerCrossAttention(nn.Module):

  1019          ```python
  1020:         >>> from transformers import AutoProcessor, Pix2StructTextModel
  1021  

  1188  
  1189:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  1190      def _update_causal_mask(

  1258      @staticmethod
  1259:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  1260      def _prepare_4d_causal_attention_mask_with_cache_position(

  1397          >>> import requests
  1398:         >>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration
  1399  

  1428          >>> import requests
  1429:         >>> from transformers import AutoProcessor, Pix2StructForConditionalGeneration
  1430  

train_real_world/transformers_4573/src/transformers/models/pixio/configuration_pixio.py:
  87      ```python
  88:     >>> from transformers import PixioConfig, PixioModel
  89  

train_real_world/transformers_4573/src/transformers/models/pixio/convert_pixio_to_pytorch.py:
  26  
  27: from transformers import BitImageProcessor, PixioConfig, PixioModel
  28: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/pixio/modeling_pixio.py:
  460          ```python
  461:         >>> from transformers import AutoImageProcessor, AutoBackbone
  462          >>> import torch

train_real_world/transformers_4573/src/transformers/models/pixio/modular_pixio.py:
   99      ```python
  100:     >>> from transformers import PixioConfig, PixioModel
  101  

  357          ```python
  358:         >>> from transformers import AutoImageProcessor, AutoBackbone
  359          >>> import torch

train_real_world/transformers_4573/src/transformers/models/pixtral/configuration_pixtral.py:
  63      ```python
  64:     >>> from transformers import PixtralVisionModel, PixtralVisionConfig
  65  

train_real_world/transformers_4573/src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py:
  22  
  23: from transformers import (
  24      LlavaConfig,

  87  def convert_mistral_tokenizer(model_file):
  88:     from transformers import LlamaTokenizer
  89  

train_real_world/transformers_4573/src/transformers/models/pixtral/image_processing_pixtral_fast.py:
  101  
  102:     # Adapted from transformers.models.pixtral.image_processing_pixtral.PixtralImageProcessor._pad_for_batching
  103      def _pad_for_batching(

train_real_world/transformers_4573/src/transformers/models/pixtral/modeling_pixtral.py:
  136  
  137: # Copied from transformers.models.llama.modeling_llama.rotate_half
  138  def rotate_half(x):

  171  
  172: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
  173  def eager_attention_forward(

  268  
  269: # Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Pixtral
  270  class PixtralMLP(nn.Module):

  285  
  286: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Pixtral
  287  class PixtralRMSNorm(nn.Module):

train_real_world/transformers_4573/src/transformers/models/pixtral/processing_pixtral.py:
  53  
  54: # Copied from transformers.models.idefics2.processing_idefics2.is_url
  55  def is_url(val) -> bool:

  58  
  59: # Copied from transformers.models.idefics2.processing_idefics2.is_image_or_image_url
  60  def is_image_or_image_url(elem):

train_real_world/transformers_4573/src/transformers/models/plbart/configuration_plbart.py:
  85      ```python
  86:     >>> from transformers import PLBartConfig, PLBartModel
  87  

train_real_world/transformers_4573/src/transformers/models/plbart/convert_plbart_original_checkpoint_to_torch.py:
  19  
  20: from transformers import PLBartConfig, PLBartForConditionalGeneration, PLBartForSequenceClassification
  21  

train_real_world/transformers_4573/src/transformers/models/plbart/modeling_plbart.py:
  1044          ```python
  1045:         >>> from transformers import AutoTokenizer, PLBartForConditionalGeneration
  1046  

  1341          ```python
  1342:         >>> from transformers import AutoTokenizer, PLBartForCausalLM
  1343  

train_real_world/transformers_4573/src/transformers/models/plbart/modular_plbart.py:
  283          ```python
  284:         >>> from transformers import AutoTokenizer, PLBartForConditionalGeneration
  285  

  396          ```python
  397:         >>> from transformers import AutoTokenizer, PLBartForCausalLM
  398  

train_real_world/transformers_4573/src/transformers/models/plbart/tokenization_plbart.py:
  102      ```python
  103:     >>> from transformers import PLBartTokenizer
  104  

train_real_world/transformers_4573/src/transformers/models/poolformer/configuration_poolformer.py:
  71      ```python
  72:     >>> from transformers import PoolFormerConfig, PoolFormerModel
  73  

train_real_world/transformers_4573/src/transformers/models/poolformer/convert_poolformer_original_to_pytorch.py:
  26  
  27: from transformers import PoolFormerConfig, PoolFormerForImageClassification, PoolFormerImageProcessor
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/poolformer/modeling_poolformer.py:
  33  
  34: # Copied from transformers.models.beit.modeling_beit.drop_path
  35  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  49  
  50: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->PoolFormer
  51  class PoolFormerDropPath(nn.Module):

train_real_world/transformers_4573/src/transformers/models/pop2piano/convert_pop2piano_weights_to_hf.py:
  21  
  22: from transformers import Pop2PianoConfig, Pop2PianoForConditionalGeneration
  23  

train_real_world/transformers_4573/src/transformers/models/pop2piano/modeling_pop2piano.py:
   24  
   25: from transformers.generation import GenerationConfig
   26  

   61  
   62: # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Pop2Piano
   63  class Pop2PianoLayerNorm(nn.Module):

   91  
   92: # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->Pop2Piano,t5->pop2piano
   93  class Pop2PianoDenseActDense(nn.Module):

  114  
  115: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->Pop2Piano
  116  class Pop2PianoDenseGatedActDense(nn.Module):

  144  
  145: # Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->Pop2Piano
  146  class Pop2PianoLayerFF(nn.Module):

  163  
  164: # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->Pop2Piano,t5->pop2piano
  165  class Pop2PianoAttention(nn.Module):

  368  
  369: # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->Pop2Piano,t5->pop2piano
  370  class Pop2PianoLayerSelfAttention(nn.Module):

  403  
  404: # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->Pop2Piano,t5->pop2piano
  405  class Pop2PianoLayerCrossAttention(nn.Module):

  440  
  441: # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Pop2Piano,t5->pop2piano
  442  class Pop2PianoBlock(GradientCheckpointingLayer):

  609  class Pop2PianoStack(Pop2PianoPreTrainedModel):
  610:     # Copied from transformers.models.t5.modeling_t5.T5Stack.__init__ with T5->Pop2Piano,t5->pop2piano
  611      def __init__(self, config):

  629  
  630:     # Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings
  631      def set_input_embeddings(self, new_embeddings):

  803  
  804:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  805      def _update_causal_mask(

  873      @staticmethod
  874:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  875      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py:
  78      ```python
  79:     >>> from transformers import PromptDepthAnythingConfig, PromptDepthAnythingForDepthEstimation
  80  

train_real_world/transformers_4573/src/transformers/models/prompt_depth_anything/convert_prompt_depth_anything_to_hf.py:
  27  
  28: from transformers import (
  29      Dinov2Config,

  33  )
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py:
  470  
  471:     # Copied from transformers.models.dpt.image_processing_dpt.DPTImageProcessor.post_process_depth_estimation with DPT->PromptDepthAnything
  472      def post_process_depth_estimation(

train_real_world/transformers_4573/src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py:
   23  
   24: from transformers.utils.generic import torch_int
   25  

  408          ```python
  409:         >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  410          >>> import torch

train_real_world/transformers_4573/src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py:
   17  
   18: from transformers.models.depth_anything.configuration_depth_anything import DepthAnythingConfig
   19: from transformers.models.depth_anything.modeling_depth_anything import (
   20      DepthAnythingDepthEstimationHead,

   26  )
   27: from transformers.utils.generic import torch_int
   28  

  251          ```python
  252:         >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
  253          >>> import torch

train_real_world/transformers_4573/src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py:
  22  # original prophetnet_checkpoints are saved under `patrickvonplaten/..._old` respectively
  23: from transformers_old.modeling_prophetnet import (
  24      ProphetNetForConditionalGeneration as ProphetNetForConditionalGenerationOld,
  25  )
  26: from transformers_old.modeling_xlm_prophetnet import (
  27      XLMProphetNetForConditionalGeneration as XLMProphetNetForConditionalGenerationOld,

  29  
  30: from transformers import ProphetNetForConditionalGeneration, XLMProphetNetForConditionalGeneration, logging
  31  

train_real_world/transformers_4573/src/transformers/models/prophetnet/modeling_prophetnet.py:
  1001          ```python
  1002:         >>> from transformers import AutoTokenizer, ProphetNetEncoder
  1003          >>> import torch

  1122          ```python
  1123:         >>> from transformers import AutoTokenizer, ProphetNetDecoder
  1124          >>> import torch

  1441          ```python
  1442:         >>> from transformers import AutoTokenizer, ProphetNetModel
  1443  

  1570          ```python
  1571:         >>> from transformers import AutoTokenizer, ProphetNetForConditionalGeneration
  1572  

  1738          ```python
  1739:         >>> from transformers import AutoTokenizer, ProphetNetForCausalLM
  1740          >>> import torch

  1750          >>> # Model can also be used with EncoderDecoder framework
  1751:         >>> from transformers import BertTokenizer, EncoderDecoderModel, AutoTokenizer
  1752          >>> import torch

train_real_world/transformers_4573/src/transformers/models/pvt/configuration_pvt.py:
  79      ```python
  80:     >>> from transformers import PvtModel, PvtConfig
  81  

train_real_world/transformers_4573/src/transformers/models/pvt/convert_pvt_to_pytorch.py:
  25  
  26: from transformers import PvtConfig, PvtForImageClassification, PvtImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/pvt/image_processing_pvt.py:
  98  
  99:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize
  100      def resize(

train_real_world/transformers_4573/src/transformers/models/pvt/modeling_pvt.py:
  38  
  39: # Copied from transformers.models.beit.modeling_beit.drop_path
  40  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  54  
  55: # Copied from transformers.models.convnext.modeling_convnext.ConvNextDropPath with ConvNext->Pvt
  56  class PvtDropPath(nn.Module):

train_real_world/transformers_4573/src/transformers/models/pvt_v2/configuration_pvt_v2.py:
  90      ```python
  91:     >>> from transformers import PvtV2Model, PvtV2Config
  92  

train_real_world/transformers_4573/src/transformers/models/pvt_v2/convert_pvt_v2_to_pytorch.py:
  25  
  26: from transformers import PvtImageProcessor, PvtV2Config, PvtV2ForImageClassification
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/pvt_v2/modeling_pvt_v2.py:
   37  
   38: # Copied from transformers.models.beit.modeling_beit.drop_path
   39  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   53  
   54: # Copied from transformers.models.convnext.modeling_convnext.ConvNextDropPath with ConvNext->Pvt
   55  class PvtV2DropPath(nn.Module):

  533          ```python
  534:         >>> from transformers import AutoImageProcessor, AutoBackbone
  535          >>> import torch

train_real_world/transformers_4573/src/transformers/models/qwen2/configuration_qwen2.py:
  86      ```python
  87:     >>> from transformers import Qwen2Model, Qwen2Config
  88  

train_real_world/transformers_4573/src/transformers/models/qwen2/modeling_qwen2.py:
  463          ```python
  464:         >>> from transformers import AutoTokenizer, Qwen2ForCausalLM
  465  

train_real_world/transformers_4573/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py:
   66      ```python
   67:     >>> from transformers import Qwen2_5OmniVisionEncoderConfig, Qwen2_5OmniVisionEncoder
   68  

  162      ```python
  163:     >>> from transformers import Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniAudioEncoder
  164  

  273      ```python
  274:     >>> from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
  275  

  412      ```python
  413:     >>> from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
  414  

  592      ```python
  593:     >>> from transformers import Qwen2_5OmniTalkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
  594  

  878      ```python
  879:     >>> from transformers import Qwen2_5OmniToken2WavModel, DiT_Args, BigVGAN_Args
  880  

  941      ```python
  942:     >>> from transformers import (
  943      ...     Qwen2_5OmniThinkerConfig,

train_real_world/transformers_4573/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:
  1963          >>> from qwen_vl_utils import process_vision_info
  1964:         >>> from transformers import Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration
  1965  

  2416          >>> import librosa
  2417:         >>> from transformers import AutoProcessor, Qwen2_5OmniTalkerForConditionalGeneration
  2418  

train_real_world/transformers_4573/src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py:
    99      ```python
   100:     >>> from transformers import Qwen2_5OmniVisionEncoderConfig, Qwen2_5OmniVisionEncoder
   101  

   196      ```python
   197:     >>> from transformers import Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniAudioEncoder
   198  

   308      ```python
   309:     >>> from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
   310  

   447      ```python
   448:     >>> from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
   449  

   627      ```python
   628:     >>> from transformers import Qwen2_5OmniTalkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig
   629  

   913      ```python
   914:     >>> from transformers import Qwen2_5OmniToken2WavModel, DiT_Args, BigVGAN_Args
   915  

   976      ```python
   977:     >>> from transformers import (
   978      ...     Qwen2_5OmniThinkerConfig,

  2266          >>> from qwen_vl_utils import process_vision_info
  2267:         >>> from transformers import Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration
  2268  

  2572          >>> import librosa
  2573:         >>> from transformers import AutoProcessor, Qwen2_5OmniTalkerForConditionalGeneration
  2574  

train_real_world/transformers_4573/src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py:
  138      ```python
  139:     >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig
  140  

  276      ```python
  277:     >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig
  278  

train_real_world/transformers_4573/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:
  1445          ```python
  1446:         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
  1447  

train_real_world/transformers_4573/src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py:
  687          ```python
  688:         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
  689  

train_real_world/transformers_4573/src/transformers/models/qwen2_audio/configuration_qwen2_audio.py:
   69      ```python
   70:     >>> from transformers import Qwen2AudioEncoderConfig, Qwen2AudioEncoder
   71  

  140      ```python
  141:     >>> from transformers import Qwen2AudioForConditionalGeneration, Qwen2AudioConfig, Qwen2AudioEncoderConfig, Qwen2Config
  142  

train_real_world/transformers_4573/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py:
   72  
   73: # Copied from transformers.models.whisper.modeling_whisper.eager_attention_forward
   74  def eager_attention_forward(

  102  
  103:     # Copied from transformers.models.whisper.modeling_whisper.WhisperAttention.__init__ with Whisper->Qwen2Audio
  104      def __init__(

  188  
  189: # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer with Whisper->Qwen2Audio, WHISPER->QWEN2AUDIO
  190  class Qwen2AudioEncoderLayer(GradientCheckpointingLayer):

  265  )
  266: # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoder with Whisper->Qwen2Audio
  267  class Qwen2AudioEncoder(Qwen2AudioPreTrainedModel):

  707          >>> import librosa
  708:         >>> from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
  709  

train_real_world/transformers_4573/src/transformers/models/qwen2_moe/configuration_qwen2_moe.py:
  107      ```python
  108:     >>> from transformers import Qwen2MoeModel, Qwen2MoeConfig
  109  

train_real_world/transformers_4573/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py:
  666          ```python
  667:         >>> from transformers import AutoTokenizer, Qwen2MoeForCausalLM
  668  

train_real_world/transformers_4573/src/transformers/models/qwen2_vl/configuration_qwen2_vl.py:
  126      ```python
  127:     >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig
  128  

  264      ```python
  265:     >>> from transformers import Qwen2VLForConditionalGeneration, Qwen2VLConfig
  266  

train_real_world/transformers_4573/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py:
   110  
   111: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2VL
   112  class Qwen2VLRotaryEmbedding(nn.Module):

   177  
   178: # Copied from transformers.models.llama.modeling_llama.rotate_half
   179  def rotate_half(x):

   313  
   314: # Copied from transformers.models.llama.modeling_llama.repeat_kv
   315  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   464  
   465: # Copied from transformers.models.qwen2.modeling_qwen2.Qwen2MLP
   466  class Qwen2MLP(nn.Module):

  1339          ```python
  1340:         >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
  1341  

train_real_world/transformers_4573/src/transformers/models/qwen3/configuration_qwen3.py:
  90      ```python
  91:     >>> from transformers import Qwen3Model, Qwen3Config
  92  

train_real_world/transformers_4573/src/transformers/models/qwen3/modeling_qwen3.py:
  494          ```python
  495:         >>> from transformers import AutoTokenizer, Qwen3ForCausalLM
  496  

train_real_world/transformers_4573/src/transformers/models/qwen3/modular_qwen3.py:
  128          ```python
  129:         >>> from transformers import AutoTokenizer, Qwen3ForCausalLM
  130  

train_real_world/transformers_4573/src/transformers/models/qwen3_moe/configuration_qwen3_moe.py:
  102      ```python
  103:     >>> from transformers import Qwen3MoeModel, Qwen3MoeConfig
  104  

train_real_world/transformers_4573/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py:
  657          ```python
  658:         >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM
  659  

train_real_world/transformers_4573/src/transformers/models/qwen3_moe/modular_qwen3_moe.py:
  128          ```python
  129:         >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM
  130  

train_real_world/transformers_4573/src/transformers/models/qwen3_next/configuration_qwen3_next.py:
  114      ```python
  115:     >>> from transformers import Qwen3NextModel, Qwen3NextConfig
  116  

train_real_world/transformers_4573/src/transformers/models/qwen3_next/modeling_qwen3_next.py:
   268  
   269: # Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb
   270  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  1220          ```python
  1221:         >>> from transformers import AutoTokenizer, Qwen3NextForCausalLM
  1222  

train_real_world/transformers_4573/src/transformers/models/qwen3_next/modular_qwen3_next.py:
  873          ```python
  874:         >>> from transformers import AutoTokenizer, Qwen3NextForCausalLM
  875  

train_real_world/transformers_4573/src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py:
    78      ```python
    79:     >>> from transformers import Qwen3OmniMoeAudioEncoderConfig, Qwen3OmniMoeAudioEncoder
    80  

   249      ```python
   250:     >>> from transformers import Qwen3OmniMoeTextModel, Qwen3OmniMoeTextConfig
   251  

   383      ```python
   384:     >>> from transformers import Qwen3OmniMoeThinkerModel, Qwen3OmniMoeThinkerConfig
   385  

   511      ```python
   512:     >>> from transformers import Qwen3OmniMoeTalkerCodePredictorModel, Qwen3OmniMoeTalkerCodePredictorConfig
   513  

   681      ```python
   682:     >>> from transformers import Qwen3OmniMoeTalkerTextModel, Qwen3OmniMoeTalkerTextConfig
   683  

   832      ```python
   833:     >>> from transformers import Qwen3OmniMoeTalkerConfig, Qwen3OmniMoeTalker
   834  

   967      ```python
   968:     >>> from transformers import Qwen3OmniMoeCode2WavConfig, Qwen3OmniMoeCode2WavModel
   969  

  1050      ```python
  1051:     >>> from transformers import (
  1052      ...     Qwen3OmniMoeThinkerConfig,

train_real_world/transformers_4573/src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py:
  2096          >>> from qwen_vl_utils import process_vision_info
  2097:         >>> from transformers import Qwen3OmniMoeProcessor, Qwen3OmniMoeThinkerForConditionalGeneration
  2098  

train_real_world/transformers_4573/src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py:
  239      ```python
  240:     >>> from transformers import Qwen3OmniMoeTextModel, Qwen3OmniMoeTextConfig
  241  

  373      ```python
  374:     >>> from transformers import Qwen3OmniMoeThinkerModel, Qwen3OmniMoeThinkerConfig
  375  

  598      ```python
  599:     >>> from transformers import Qwen3OmniMoeTalkerConfig, Qwen3OmniMoeTalker
  600  

  733      ```python
  734:     >>> from transformers import Qwen3OmniMoeCode2WavConfig, Qwen3OmniMoeCode2WavModel
  735  

  816      ```python
  817:     >>> from transformers import (
  818      ...     Qwen3OmniMoeThinkerConfig,

train_real_world/transformers_4573/src/transformers/models/qwen3_vl/configuration_qwen3_vl.py:
  118      ```python
  119:     >>> from transformers import Qwen3VLTextModel, Qwen3VLTextConfig
  120  

  210      ```python
  211:     >>> from transformers import Qwen3VLForConditionalGeneration, Qwen3VLConfig
  212  

train_real_world/transformers_4573/src/transformers/models/qwen3_vl/modeling_qwen3_vl.py:
  1340          ```python
  1341:         >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration
  1342  

train_real_world/transformers_4573/src/transformers/models/qwen3_vl/modular_qwen3_vl.py:
   160      ```python
   161:     >>> from transformers import Qwen3VLTextModel, Qwen3VLTextConfig
   162  

   252      ```python
   253:     >>> from transformers import Qwen3VLForConditionalGeneration, Qwen3VLConfig
   254  

  1121          ```python
  1122:         >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration
  1123  

train_real_world/transformers_4573/src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py:
   93      ```python
   94:     >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig
   95  

  251      ```python
  252:     >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig
  253  

train_real_world/transformers_4573/src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py:
  1547          >>> import requests
  1548:         >>> from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration
  1549  

train_real_world/transformers_4573/src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py:
  116      ```python
  117:     >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig
  118  

  240      ```python
  241:     >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig
  242  

  428          >>> import requests
  429:         >>> from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration
  430  

train_real_world/transformers_4573/src/transformers/models/rag/modeling_rag.py:
   286          ```python
   287:         >>> from transformers import RagModel
   288  

   487          ```python
   488:         >>> from transformers import AutoTokenizer, RagRetriever, RagModel
   489          >>> import torch

   773          ```python
   774:         >>> from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration
   775          >>> import torch

  1296          ```python
  1297:         >>> from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration
  1298          >>> import torch

train_real_world/transformers_4573/src/transformers/models/rag/retrieval_rag.py:
  370      >>> # To load the default "wiki_dpr" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')
  371:     >>> from transformers import RagRetriever
  372  

  377      >>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py
  378:     >>> from transformers import RagRetriever
  379  

  385      >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py
  386:     >>> from transformers import RagRetriever
  387  

  397      >>> # To load the legacy index built originally for Rag's paper
  398:     >>> from transformers import RagRetriever
  399  

train_real_world/transformers_4573/src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py:
  88      ```python
  89:     >>> from transformers import RecurrentGemmaModel, RecurrentGemmaConfig
  90  

train_real_world/transformers_4573/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py:
  19  
  20: from transformers import GemmaTokenizer, RecurrentGemmaConfig, RecurrentGemmaForCausalLM
  21  

  23  try:
  24:     from transformers import GemmaTokenizerFast
  25  except ImportError as e:

  45  ```py
  46: from transformers import GemmaForCausalLM, GemmaTokenizerFast
  47  

train_real_world/transformers_4573/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:
   42  
   43: # Copied from transformers.models.gemma.modeling_gemma.GemmaRMSNorm with Gemma->RecurrentGemma
   44  class RecurrentGemmaRMSNorm(nn.Module):

   63  
   64: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->RecurrentGemma
   65  class RecurrentGemmaRotaryEmbedding(nn.Module):

  133  
  134: # Copied from transformers.models.llama.modeling_llama.rotate_half
  135  def rotate_half(x):

  141  
  142: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
  143  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  169  
  170: # Copied from transformers.models.llama.modeling_llama.repeat_kv
  171  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

  745  
  746: # TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->RECURRENTGEMMA,Llama->RecurrentGemma,llama->gemma
  747  @auto_docstring

  784          ```python
  785:         >>> from transformers import AutoTokenizer, RecurrentGemmaForCausalLM
  786  

train_real_world/transformers_4573/src/transformers/models/reformer/configuration_reformer.py:
  141      ```python
  142:     >>> from transformers import ReformerConfig, ReformerModel
  143  

train_real_world/transformers_4573/src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py:
  24  
  25: from transformers import ReformerConfig, ReformerModelWithLMHead
  26: from transformers.utils import logging
  27  

train_real_world/transformers_4573/src/transformers/models/reformer/modeling_reformer.py:
  2341          >>> import torch
  2342:         >>> from transformers import AutoTokenizer, ReformerForMaskedLM
  2343  

  2466          >>> import torch
  2467:         >>> from transformers import AutoTokenizer, ReformerForSequenceClassification
  2468  

train_real_world/transformers_4573/src/transformers/models/regnet/configuration_regnet.py:
  54      ```python
  55:     >>> from transformers import RegNetConfig, RegNetModel
  56  

train_real_world/transformers_4573/src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py:
  36  
  37: from transformers import AutoImageProcessor, RegNetConfig, RegNetForImageClassification, RegNetModel
  38: from transformers.modeling_utils import _load_state_dict_into_meta_model, load_state_dict
  39: from transformers.utils import logging
  40  

train_real_world/transformers_4573/src/transformers/models/regnet/convert_regnet_to_pytorch.py:
  32  
  33: from transformers import AutoImageProcessor, RegNetConfig, RegNetForImageClassification, RegNetModel
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/regnet/modeling_regnet.py:
   89  
   90: # Copied from transformers.models.resnet.modeling_resnet.ResNetShortCut with ResNet->RegNet
   91  class RegNetShortCut(nn.Module):

  287  @auto_docstring
  288: # Copied from transformers.models.resnet.modeling_resnet.ResNetModel with RESNET->REGNET,ResNet->RegNet
  289  class RegNetModel(RegNetPreTrainedModel):

  337  )
  338: # Copied from transformers.models.resnet.modeling_resnet.ResNetForImageClassification with RESNET->REGNET,ResNet->RegNet,resnet->regnet
  339  class RegNetForImageClassification(RegNetPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/rembert/configuration_rembert.py:
  79      ```python
  80:     >>> from transformers import RemBertModel, RemBertConfig
  81  

train_real_world/transformers_4573/src/transformers/models/rembert/convert_rembert_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import RemBertConfig, RemBertModel
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/rembert/modeling_rembert.py:
   99  
  100: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->RemBert
  101  class RemBertPooler(nn.Module):

  217  
  218: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->RemBert
  219  class RemBertSelfOutput(nn.Module):

  238  
  239:     # copied from transformers.models.bert.modeling_bert.BertAttention.forward
  240      def forward(

  261  
  262: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->RemBert
  263  class RemBertIntermediate(nn.Module):

  277  
  278: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->RemBert
  279  class RemBertOutput(nn.Module):

  307  
  308:     # copied from transformers.models.bert.modeling_bert.BertLayer.forward
  309      def forward(

  353  
  354:     # Copied from transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk
  355      def feed_forward_chunk(self, attention_output):

  440  
  441: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->RemBert
  442  class RemBertPredictionHeadTransform(nn.Module):

  474  
  475: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->RemBert
  476  class RemBertOnlyMLMHead(nn.Module):

  785          ```python
  786:         >>> from transformers import AutoTokenizer, RemBertForCausalLM, RemBertConfig
  787          >>> import torch

train_real_world/transformers_4573/src/transformers/models/resnet/configuration_resnet.py:
  66      ```python
  67:     >>> from transformers import ResNetConfig, ResNetModel
  68  

train_real_world/transformers_4573/src/transformers/models/resnet/convert_resnet_to_pytorch.py:
  29  
  30: from transformers import AutoImageProcessor, ResNetConfig, ResNetForImageClassification
  31: from transformers.utils import logging
  32  

train_real_world/transformers_4573/src/transformers/models/resnet/modeling_resnet.py:
  402          ```python
  403:         >>> from transformers import AutoImageProcessor, AutoBackbone
  404          >>> import torch

train_real_world/transformers_4573/src/transformers/models/roberta/configuration_roberta.py:
  74      ```python
  75:     >>> from transformers import RobertaConfig, RobertaModel
  76  

train_real_world/transformers_4573/src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py:
  25  
  26: from transformers import RobertaConfig, RobertaForMaskedLM, RobertaForSequenceClassification
  27: from transformers.models.bert.modeling_bert import (
  28      BertIntermediate,

  33  )
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/roberta/modeling_roberta.py:
  774          ```python
  775:         >>> from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig
  776          >>> import torch

train_real_world/transformers_4573/src/transformers/models/roberta/modular_roberta.py:
  249          ```python
  250:         >>> from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig
  251          >>> import torch

train_real_world/transformers_4573/src/transformers/models/roberta/tokenization_roberta_old.py:
  41      ```python
  42:     >>> from transformers import RobertaTokenizerFast
  43  

train_real_world/transformers_4573/src/transformers/models/roberta/tokenization_roberta.py:
  38      ```python
  39:     >>> from transformers import RobertaTokenizer
  40  

train_real_world/transformers_4573/src/transformers/models/roberta_prelayernorm/configuration_roberta_prelayernorm.py:
  24  
  25: # Copied from transformers.models.roberta.configuration_roberta.RobertaConfig with FacebookAI/roberta-base->andreasmadsen/efficient_mlm_m0.40,RoBERTa->RoBERTa-PreLayerNorm,Roberta->RobertaPreLayerNorm,roberta->roberta-prelayernorm
  26  class RobertaPreLayerNormConfig(PreTrainedConfig):

  75      ```python
  76:     >>> from transformers import RobertaPreLayerNormConfig, RobertaPreLayerNormModel
  77  

train_real_world/transformers_4573/src/transformers/models/roberta_prelayernorm/convert_roberta_prelayernorm_original_pytorch_checkpoint_to_pytorch.py:
  21  
  22: from transformers import AutoTokenizer, RobertaPreLayerNormConfig, RobertaPreLayerNormForMaskedLM
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py:
    51  
    52: # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->RobertaPreLayerNorm
    53  class RobertaPreLayerNormEmbeddings(nn.Module):

   158  
   159: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   160  def eager_attention_forward(

   188  
   189: # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RobertaPreLayerNorm
   190  class RobertaPreLayerNormSelfAttention(nn.Module):

   262  
   263: # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->RobertaPreLayerNorm
   264  class RobertaPreLayerNormCrossAttention(nn.Module):

   416  
   417: # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RobertaPreLayerNorm
   418  class RobertaPreLayerNormLayer(GradientCheckpointingLayer):

   484  
   485: # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->RobertaPreLayerNorm
   486  class RobertaPreLayerNormEncoder(nn.Module):

   521  
   522: # Copied from transformers.models.bert.modeling_bert.BertPooler
   523  class RobertaPreLayerNormPooler(nn.Module):

   700  
   701:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
   702      def _create_attention_masks(

   741  )
   742: # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with FacebookAI/roberta-base->andreasmadsen/efficient_mlm_m0.40,ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm, RobertaPreLayerNormTokenizer->RobertaTokenizer
   743  class RobertaPreLayerNormForCausalLM(RobertaPreLayerNormPreTrainedModel, GenerationMixin):

   804          ```python
   805:         >>> from transformers import AutoTokenizer, RobertaPreLayerNormForCausalLM, AutoConfig
   806          >>> import torch

   865  
   866:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.__init__ with ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm
   867      def __init__(self, config):

   889      @auto_docstring
   890:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.forward with ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm
   891      def forward(

   946  
   947: # Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead with Roberta->RobertaPreLayerNorm
   948  class RobertaPreLayerNormLMHead(nn.Module):

   989      @auto_docstring
   990:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.forward with roberta->roberta_prelayernorm
   991      def forward(

  1061  @auto_docstring
  1062: # Copied from transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice with ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm
  1063  class RobertaPreLayerNormForMultipleChoice(RobertaPreLayerNormPreTrainedModel):

  1176      @auto_docstring
  1177:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.forward with roberta->roberta_prelayernorm
  1178      def forward(

  1230  
  1231: # Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->RobertaPreLayerNorm
  1232  class RobertaPreLayerNormClassificationHead(nn.Module):

  1267      @auto_docstring
  1268:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.forward with roberta->roberta_prelayernorm
  1269      def forward(

train_real_world/transformers_4573/src/transformers/models/roc_bert/configuration_roc_bert.py:
  90      ```python
  91:     >>> from transformers import RoCBertModel, RoCBertConfig
  92  

train_real_world/transformers_4573/src/transformers/models/roc_bert/modeling_roc_bert.py:
   177  
   178: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   179  def eager_attention_forward(

   207  
   208: # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RoCBert
   209  class RoCBertSelfAttention(nn.Module):

   281  
   282: # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->RoCBert
   283  class RoCBertCrossAttention(nn.Module):

   359  
   360: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->RoCBert
   361  class RoCBertSelfOutput(nn.Module):

   374  
   375: # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->RoCBert,BERT->ROC_BERT
   376  class RoCBertAttention(nn.Module):

   406  
   407: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->RoCBert
   408  class RoCBertIntermediate(nn.Module):

   422  
   423: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->RoCBert
   424  class RoCBertOutput(nn.Module):

   437  
   438: # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RoCBert
   439  class RoCBertLayer(GradientCheckpointingLayer):

   505  
   506: # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->RoCBert
   507  class RoCBertEncoder(nn.Module):

   540  
   541: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->RoCBert
   542  class RoCBertPooler(nn.Module):

   556  
   557: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->RoCBert
   558  class RoCBertPredictionHeadTransform(nn.Module):

   574  
   575: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->RoCBert
   576  class RoCBertLMPredictionHead(nn.Module):

   591  
   592: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->RoCBert
   593  class RoCBertOnlyMLMHead(nn.Module):

   659  
   660:     # Copied from transformers.models.bert.modeling_bert.BertModel.get_input_embeddings
   661      def get_input_embeddings(self):

   663  
   664:     # Copied from transformers.models.bert.modeling_bert.BertModel.set_input_embeddings
   665      def set_input_embeddings(self, value):

   775  
   776:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
   777      def _create_attention_masks(

   831  
   832:     # Copied from transformers.models.bert.modeling_bert.BertForPreTraining.get_output_embeddings
   833      def get_output_embeddings(self):

   835  
   836:     # Copied from transformers.models.bert.modeling_bert.BertForPreTraining.set_output_embeddings
   837      def set_output_embeddings(self, new_embeddings):

   919          ```python
   920:         >>> from transformers import AutoTokenizer, RoCBertForPreTraining
   921          >>> import torch

  1018  
  1019:     # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.__init__ with Bert->RoCBert,bert->roc_bert
  1020      def __init__(self, config):

  1034  
  1035:     # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.get_output_embeddings
  1036      def get_output_embeddings(self):

  1038  
  1039:     # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.set_output_embeddings
  1040      def set_output_embeddings(self, new_embeddings):

  1081          ```python
  1082:         >>> from transformers import AutoTokenizer, RoCBertForMaskedLM
  1083          >>> import torch

  1176  
  1177:     # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.__init__ with BertLMHeadModel->RoCBertForCausalLM,Bert->RoCBert,bert->roc_bert
  1178      def __init__(self, config):

  1189  
  1190:     # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.get_output_embeddings
  1191      def get_output_embeddings(self):

  1193  
  1194:     # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.set_output_embeddings
  1195      def set_output_embeddings(self, new_embeddings):

  1241          ```python
  1242:         >>> from transformers import AutoTokenizer, RoCBertForCausalLM, RoCBertConfig
  1243          >>> import torch

  1325  class RoCBertForSequenceClassification(RoCBertPreTrainedModel):
  1326:     # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification.__init__ with Bert->RoCBert,bert->roc_bert
  1327      def __init__(self, config):

  1425  class RoCBertForMultipleChoice(RoCBertPreTrainedModel):
  1426:     # Copied from transformers.models.bert.modeling_bert.BertForMultipleChoice.__init__ with Bert->RoCBert,bert->roc_bert
  1427      def __init__(self, config):

  1548  class RoCBertForTokenClassification(RoCBertPreTrainedModel):
  1549:     # Copied from transformers.models.bert.modeling_bert.BertForTokenClassification.__init__ with Bert->RoCBert,bert->roc_bert
  1550      def __init__(self, config):

  1627  class RoCBertForQuestionAnswering(RoCBertPreTrainedModel):
  1628:     # Copied from transformers.models.bert.modeling_bert.BertForQuestionAnswering.__init__ with Bert->RoCBert,bert->roc_bert
  1629      def __init__(self, config):

train_real_world/transformers_4573/src/transformers/models/roformer/configuration_roformer.py:
  75      ```python
  76:     >>> from transformers import RoFormerModel, RoFormerConfig
  77  

train_real_world/transformers_4573/src/transformers/models/roformer/convert_roformer_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import RoFormerConfig, RoFormerForMaskedLM
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/roformer/modeling_roformer.py:
   48  
   49: # Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->RoFormer
   50  class RoFormerSinusoidalPositionalEmbedding(nn.Embedding):

  260  
  261: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->RoFormer
  262  class RoFormerSelfOutput(nn.Module):

  305  
  306: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->RoFormer
  307  class RoFormerIntermediate(nn.Module):

  321  
  322: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->RoFormer
  323  class RoFormerOutput(nn.Module):

  487  
  488: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->RoFormer
  489  class RoFormerSequenceSummary(nn.Module):

  621  
  622: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->RoFormer
  623  class RoFormerOnlyMLMHead(nn.Module):

  938          ```python
  939:         >>> from transformers import AutoTokenizer, RoFormerForCausalLM, RoFormerConfig
  940          >>> import torch

train_real_world/transformers_4573/src/transformers/models/roformer/tokenization_roformer.py:
  41      ```python
  42:     >>> from transformers import RoFormerTokenizer
  43  

train_real_world/transformers_4573/src/transformers/models/rt_detr/configuration_rt_detr_resnet.py:
  66      ```python
  67:     >>> from transformers import RTDetrResNetConfig, RTDetrResnetBackbone
  68  

train_real_world/transformers_4573/src/transformers/models/rt_detr/configuration_rt_detr.py:
  164      ```python
  165:     >>> from transformers import RTDetrConfig, RTDetrModel
  166  

train_real_world/transformers_4573/src/transformers/models/rt_detr/convert_rt_detr_original_pytorch_checkpoint_to_hf.py:
  26  
  27: from transformers import RTDetrConfig, RTDetrForObjectDetection, RTDetrImageProcessor
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/rt_detr/image_processing_rt_detr.py:
  121  
  122: # Copied from transformers.models.detr.image_processing_detr.get_image_size_for_max_height_width
  123  def get_image_size_for_max_height_width(

  155  
  156: # Copied from transformers.models.detr.image_processing_detr.safe_squeeze
  157  def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:

  169  
  170: # Copied from transformers.models.detr.image_processing_detr.normalize_annotation
  171  def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:

  184  
  185: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
  186  def max_across_indices(values: Iterable[Any]) -> list[Any]:

  192  
  193: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
  194  def get_max_height_width(

  211  
  212: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
  213  def make_pixel_mask(

  285  
  286: # Copied from transformers.models.detr.image_processing_detr.resize_annotation
  287  def resize_annotation(

  453  
  454:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize
  455      def resize(

  514  
  515:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize_annotation
  516      def resize_annotation(

  528  
  529:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
  530      def rescale(

  557  
  558:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation
  559      def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:

  565  
  566:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image
  567      def _update_annotation_for_padded_image(

  609  
  610:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._pad_image
  611      def _pad_image(

  643  
  644:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.pad
  645      def pad(

train_real_world/transformers_4573/src/transformers/models/rt_detr/modeling_rt_detr_resnet.py:
   37  
   38: # Copied from transformers.models.resnet.modeling_resnet.ResNetConvLayer -> RTDetrResNetConvLayer
   39  class RTDetrResNetConvLayer(nn.Module):

  102  
  103: # Copied from transformers.models.resnet.modeling_resnet.ResNetShortCut -> RTDetrResNetChortCut
  104  class RTDetrResNetShortCut(nn.Module):

  256  
  257: # Copied from transformers.models.resnet.modeling_resnet.ResNetEncoder with ResNet->RTDetrResNet
  258  class RTDetrResNetEncoder(nn.Module):

  299  @auto_docstring
  300: # Copied from transformers.models.resnet.modeling_resnet.ResNetPreTrainedModel with ResNet->RTDetrResNet
  301  class RTDetrResNetPreTrainedModel(PreTrainedModel):

  359                          ```python
  360:                         >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone
  361                          >>> import torch

train_real_world/transformers_4573/src/transformers/models/rt_detr/modeling_rt_detr.py:
    49  @use_kernel_forward_from_hub("MultiScaleDeformableAttention")
    50: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention
    51  class MultiScaleDeformableAttention(nn.Module):

   282  
   283: # Copied from transformers.models.conditional_detr.modeling_conditional_detr.inverse_sigmoid
   284  def inverse_sigmoid(x, eps=1e-5):

   290  
   291: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->RTDetr
   292  class RTDetrFrozenBatchNorm2d(nn.Module):

   330  
   331: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->RTDetr
   332  def replace_batch_norm(model):

   662  
   663: # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->RTDetr
   664  class RTDetrMultiscaleDeformableAttention(nn.Module):

  1617          ```python
  1618:         >>> from transformers import AutoImageProcessor, RTDetrModel
  1619          >>> from PIL import Image

  1868          ```python
  1869:         >>> from transformers import RTDetrImageProcessor, RTDetrForObjectDetection
  1870          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/rt_detr/modular_rt_detr.py:
  6  
  7: from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast
  8  

train_real_world/transformers_4573/src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py:
  172      ```python
  173:     >>> from transformers import RTDetrV2Config, RTDetrV2Model
  174  

train_real_world/transformers_4573/src/transformers/models/rt_detr_v2/convert_rt_detr_v2_weights_to_hf.py:
  28  
  29: from transformers import RTDetrImageProcessor, RTDetrV2Config, RTDetrV2ForObjectDetection
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:
  1516          ```python
  1517:         >>> from transformers import AutoImageProcessor, RTDetrV2Model
  1518          >>> from PIL import Image

  1880          ```python
  1881:         >>> from transformers import RTDetrV2ImageProcessor, RTDetrV2ForObjectDetection
  1882          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py:
  184      ```python
  185:     >>> from transformers import RTDetrV2Config, RTDetrV2Model
  186  

train_real_world/transformers_4573/src/transformers/models/rwkv/configuration_rwkv.py:
  70      ```python
  71:     >>> from transformers import RwkvConfig, RwkvModel
  72  

train_real_world/transformers_4573/src/transformers/models/rwkv/convert_rwkv_checkpoint_to_hf.py:
  25  
  26: from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerFast, RwkvConfig
  27: from transformers.modeling_utils import WEIGHTS_INDEX_NAME
  28  

train_real_world/transformers_4573/src/transformers/models/sam/configuration_sam.py:
  189      ```python
  190:     >>> from transformers import (
  191      ...     SamVisionConfig,

  278      ```python
  279:     >>> from transformers import (
  280      ...     SamVisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam/convert_sam_to_hf.py:
  31  
  32: from transformers import (
  33      SamConfig,

train_real_world/transformers_4573/src/transformers/models/sam/modeling_sam.py:
    26  
    27: from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs
    28  

   147  
   148: # Copied from transformers.models.convnext.modeling_convnext.ConvNextLayerNorm with ConvNext->Sam
   149  class SamLayerNorm(nn.LayerNorm):

  1261          >>> import requests
  1262:         >>> from transformers import AutoModel, AutoProcessor
  1263  

train_real_world/transformers_4573/src/transformers/models/sam2/configuration_sam2.py:
  384      ```python
  385:     >>> from transformers import (
  386      ...     Sam2VisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam2/convert_sam2_to_hf.py:
  29  
  30: from transformers import (
  31      Sam2Config,

train_real_world/transformers_4573/src/transformers/models/sam2/modeling_sam2.py:
    32  
    33: from transformers.utils.generic import OutputRecorder
    34  

  1453          >>> import requests
  1454:         >>> from transformers import AutoModel, AutoProcessor
  1455  

train_real_world/transformers_4573/src/transformers/models/sam2/modular_sam2.py:
  1361          >>> import requests
  1362:         >>> from transformers import AutoModel, AutoProcessor
  1363  

train_real_world/transformers_4573/src/transformers/models/sam2_video/configuration_sam2_video.py:
  248      ```python
  249:     >>> from transformers import (
  250      ...     Sam2VisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam2_video/convert_sam2_video_to_hf.py:
  29  
  30: from transformers import (
  31      Sam2HieraDetConfig,

train_real_world/transformers_4573/src/transformers/models/sam2_video/modular_sam2_video.py:
  171      ```python
  172:     >>> from transformers import (
  173      ...     Sam2VisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam3/configuration_sam3.py:
   16  
   17: from transformers import CLIPTextConfig
   18  

  423      ```python
  424:     >>> from transformers import Sam3Config, Sam3Model
  425  

train_real_world/transformers_4573/src/transformers/models/sam3/convert_sam3_to_hf.py:
   28  
   29: from transformers import CLIPTokenizerFast, Sam3Config, Sam3ImageProcessorFast, Sam3Model, Sam3Processor
   30: from transformers.utils import logging
   31  

  422      print("\nTo test the model, you can run:")
  423:     print(">>> from transformers import Sam3Model")
  424      print(f">>> model = Sam3Model.from_pretrained('{output_path}')")

train_real_world/transformers_4573/src/transformers/models/sam3/modeling_sam3.py:
    28  
    29: from transformers import CLIPTextModelWithProjection
    30  

  2166          ```python
  2167:         >>> from transformers import Sam3Model, Sam3Processor
  2168          >>> from PIL import Image

  2204          ```python
  2205:         >>> from transformers import Sam3Model, Sam3Processor
  2206          >>> from PIL import Image

  2255          >>> import requests
  2256:         >>> from transformers import AutoModel, AutoProcessor
  2257  

train_real_world/transformers_4573/src/transformers/models/sam3/processing_sam3.py:
  591          ```python
  592:         >>> from transformers import AutoModel, AutoProcessor
  593          >>> from PIL import Image

  645          ```python
  646:         >>> from transformers import AutoModel, AutoProcessor
  647          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/sam3_tracker/configuration_sam3_tracker.py:
  174      ```python
  175:     >>> from transformers import (
  176      ...     Sam3TrackerVisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam3_tracker/modeling_sam3_tracker.py:
   32  
   33: from transformers.utils.generic import OutputRecorder
   34  

  953          >>> import requests
  954:         >>> from transformers import AutoModel, AutoProcessor
  955  

train_real_world/transformers_4573/src/transformers/models/sam3_tracker_video/configuration_sam3_tracker_video.py:
  250      ```python
  251:     >>> from transformers import (
  252      ...     Sam3VisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py:
  206      ```python
  207:     >>> from transformers import (
  208      ...     Sam3VisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam3_video/configuration_sam3_video.py:
  93      ```python
  94:     >>> from transformers import Sam3VideoConfig, Sam3VideoModel
  95  

train_real_world/transformers_4573/src/transformers/models/sam3_video/convert_sam3_video_to_hf.py:
   27  
   28: from transformers import CLIPTokenizerFast
   29: from transformers.models.sam2_video.video_processing_sam2_video import Sam2VideoVideoProcessor
   30: from transformers.models.sam3.image_processing_sam3_fast import Sam3ImageProcessorFast
   31: from transformers.models.sam3.modeling_sam3 import Sam3Model
   32: from transformers.models.sam3_tracker.modeling_sam3_tracker import Sam3TrackerModel
   33: from transformers.models.sam3_tracker_video.modeling_sam3_tracker_video import Sam3TrackerVideoModel
   34: from transformers.models.sam3_video.configuration_sam3_video import Sam3VideoConfig
   35: from transformers.models.sam3_video.modeling_sam3_video import Sam3VideoModel
   36: from transformers.models.sam3_video.processing_sam3_video import Sam3VideoProcessor
   37: from transformers.utils import logging
   38  

  739      print("\nTo test the model, you can run:")
  740:     print(">>> from transformers import Sam3Model")
  741      print(f">>> model = Sam3Model.from_pretrained('{output_path}')")

train_real_world/transformers_4573/src/transformers/models/sam3_video/modeling_sam3_video.py:
  26  
  27: from transformers.models.sam3.modeling_sam3 import Sam3VisionNeck
  28  

train_real_world/transformers_4573/src/transformers/models/sam_hq/configuration_sam_hq.py:
  128      ```python
  129:     >>> from transformers import (
  130      ...     SamHQVisionConfig,

train_real_world/transformers_4573/src/transformers/models/sam_hq/convert_samhq_to_hf.py:
  30  
  31: from transformers import SamHQConfig, SamHQModel, SamHQProcessor, SamHQVisionConfig, SamImageProcessor
  32  

train_real_world/transformers_4573/src/transformers/models/sam_hq/modeling_sam_hq.py:
    31  
    32: from transformers.modeling_outputs import ModelOutput
    33: from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs
    34  

  1398          >>> import requests
  1399:         >>> from transformers import AutoModel, AutoProcessor
  1400  

train_real_world/transformers_4573/src/transformers/models/sam_hq/modular_sam_hq.py:
   21  
   22: from transformers.modeling_outputs import ModelOutput
   23: from transformers.utils.generic import TransformersKwargs, check_model_inputs
   24  

  551          >>> import requests
  552:         >>> from transformers import AutoModel, AutoProcessor
  553  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t/configuration_seamless_m4t.py:
  225      ```python
  226:     >>> from transformers import SeamlessM4TModel, SeamlessM4TConfig
  227  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t/convert_fairseq2_to_hf.py:
  24  
  25: from transformers import (
  26      SeamlessM4TConfig,

  31  )
  32: from transformers.utils import logging
  33  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py:
  92      @staticmethod
  93:     # Copied from transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm
  94      def zero_mean_unit_var_norm(

train_real_world/transformers_4573/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:
   114  
   115: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
   116  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

   200  
   201: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->SeamlessM4TConformer, feat_extract_activation->speech_encoder_hidden_act
   202  class SeamlessM4TConformerPositionalConvEmbedding(nn.Module):

   246  
   247: # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRotaryPositionalEmbedding with Wav2Vec2->SeamlessM4T, num_attention_heads->speech_encoder_attention_heads
   248  class SeamlessM4TConformerRotaryPositionalEmbedding(nn.Module):

   281  
   282: # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRelPositionalEmbedding with Wav2Vec2->SeamlessM4T
   283  class SeamlessM4TConformerRelPositionalEmbedding(nn.Module):

   331  
   332: # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSamePadLayer with Wav2Vec2->SeamlessM4T
   333  class SeamlessM4TConformerSamePadLayer(nn.Module):

   474  
   475:     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention.forward
   476      def forward(

   537  
   538:     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_rotary_embedding
   539      def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):

   557  
   558:     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_relative_embeddings
   559      def _apply_relative_embeddings(self, query, key, relative_position_embeddings):

   601  
   602:     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4T, attention_dropout->speech_encoder_dropout, torch.nn->nn
   603      def __init__(self, config):

   871  
   872: # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100ScaledWordEmbedding with M2M100->SeamlessM4T
   873  class SeamlessM4TScaledWordEmbedding(nn.Embedding):

   885  
   886: # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding with M2M100->SeamlessM4T
   887  class SeamlessM4TSinusoidalPositionalEmbedding(nn.Module):

   971      @staticmethod
   972:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
   973      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

   991  
   992:     # Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart->SeamlessM4T
   993      def __init__(

  1141  
  1142: # Copied from transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense with NllbMoe->SeamlessM4T,DenseActDense->FeedForwardNetwork, d_model->hidden_size
  1143  class SeamlessM4TFeedForwardNetwork(nn.Module):

  2120  
  2121: # Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
  2122  class HifiGanResidualBlock(nn.Module):

train_real_world/transformers_4573/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py:
  49      ```python
  50:     >>> from transformers import SeamlessM4TTokenizer
  51  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py:
  228      ```python
  229:     >>> from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config
  230  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t_v2/convert_fairseq2_to_hf.py:
  24  
  25: from transformers import (
  26      SeamlessM4TFeatureExtractor,

  31  )
  32: from transformers.utils import logging
  33  

train_real_world/transformers_4573/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:
    86  )
    87: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput with SeamlessM4T->SeamlessM4Tv2
    88  class SeamlessM4Tv2GenerationOutput(ModelOutput):

   162  
   163: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
   164  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

   206  
   207: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.format_speech_generation_kwargs with SeamlessM4T->SeamlessM4Tv2
   208  def format_speech_generation_kwargs(kwargs):

   250  class SeamlessM4Tv2ConformerFeatureProjection(nn.Module):
   251:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerFeatureProjection.__init__
   252      def __init__(self, config):

   265  
   266: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerFeedForward with SeamlessM4T->SeamlessM4Tv2
   267  class SeamlessM4Tv2ConformerFeedForward(nn.Module):

   448  
   449:     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4Tv2, attention_dropout->speech_encoder_dropout, torch.nn->nn
   450      def __init__(self, config):

   623  
   624: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerAdapterLayer with SeamlessM4T->SeamlessM4Tv2
   625  class SeamlessM4Tv2ConformerAdapterLayer(nn.Module):

   722  
   723: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TConformerAdapter with SeamlessM4T->SeamlessM4Tv2
   724  class SeamlessM4Tv2ConformerAdapter(nn.Module):

   743  
   744: # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100ScaledWordEmbedding with M2M100->SeamlessM4Tv2
   745  class SeamlessM4Tv2ScaledWordEmbedding(nn.Embedding):

   757  
   758: # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding with M2M100->SeamlessM4Tv2
   759  class SeamlessM4Tv2SinusoidalPositionalEmbedding(nn.Module):

   843      @staticmethod
   844:     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids
   845      def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):

   863  
   864:     # Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart->SeamlessM4Tv2
   865      def __init__(

   971  
   972: # Copied from transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense with NllbMoe->SeamlessM4Tv2,DenseActDense->FeedForwardNetwork, d_model->hidden_size
   973  class SeamlessM4Tv2FeedForwardNetwork(nn.Module):

   994  
   995: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer with SeamlessM4T->SeamlessM4Tv2
   996  class SeamlessM4Tv2EncoderLayer(GradientCheckpointingLayer):

  1058  
  1059: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer with SeamlessM4T->SeamlessM4Tv2
  1060  class SeamlessM4Tv2DecoderLayer(GradientCheckpointingLayer):

  1301  
  1302:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TPreTrainedModel._compute_sub_sample_lengths_from_attention_mask
  1303      def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):

  1497  )
  1498: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSpeechEncoder with SeamlessM4T->SeamlessM4Tv2
  1499  class SeamlessM4Tv2SpeechEncoder(SeamlessM4Tv2PreTrainedModel):

  1571  )
  1572: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoder with SeamlessM4T->SeamlessM4Tv2
  1573  class SeamlessM4Tv2Encoder(SeamlessM4Tv2PreTrainedModel):

  1753  )
  1754: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoder with SeamlessM4T->SeamlessM4Tv2
  1755  class SeamlessM4Tv2Decoder(SeamlessM4Tv2PreTrainedModel):

  2103  class SeamlessM4Tv2TextToUnitModel(SeamlessM4Tv2PreTrainedModel):
  2104:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitModel.__init__ with SeamlessM4T->SeamlessM4Tv2, Decoder->TextToUnitDecoder
  2105      def __init__(

  2195  
  2196:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.__init__ with SeamlessM4T->SeamlessM4Tv2
  2197      def __init__(

  2219  
  2220:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.get_encoder
  2221      def get_encoder(self):

  2223  
  2224:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.get_decoder
  2225      def get_decoder(self):

  2227  
  2228:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.get_input_embeddings
  2229      def get_input_embeddings(self):

  2231  
  2232:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.set_input_embeddings
  2233      def set_input_embeddings(self, value):

  2305  
  2306: # Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
  2307  class HifiGanResidualBlock(nn.Module):

  2405  
  2406: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4THifiGan with SeamlessM4T->SeamlessM4Tv2
  2407  class SeamlessM4Tv2HifiGan(nn.Module):

  2507  
  2508:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan._get_dur_output_lengths
  2509      def _get_dur_output_lengths(self, input_ids, dur_out):

  2522  
  2523:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan._get_output_hifigan_lengths
  2524      def _get_output_hifigan_lengths(self, input_lengths: Union[torch.LongTensor, int]):

  2565  
  2566:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward with SeamlessM4T->SeamlessM4Tv2, spkr_id->speaker_id
  2567      def forward(

  2615  
  2616:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.apply_weight_norm
  2617      def apply_weight_norm(self):

  2628  
  2629:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.remove_weight_norm
  2630      def remove_weight_norm(self):

  2646  )
  2647: # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText with SeamlessM4T->SeamlessM4Tv2,SeamlessM4Tv2Tokenizer->SeamlessM4TTokenizer, SeamlessM4Tv2Processor->SeamlessM4TProcessor, SEAMLESS_M4T->SEAMLESS_M4T_V2
  2648  class SeamlessM4Tv2ForTextToText(SeamlessM4Tv2PreTrainedModel, GenerationMixin):

  2908  
  2909:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.__init__ with SeamlessM4T->SeamlessM4Tv2
  2910      def __init__(self, config: SeamlessM4Tv2Config):

  2920  
  2921:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_encoder
  2922      def get_encoder(self):

  2924  
  2925:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_decoder
  2926      def get_decoder(self):

  2928  
  2929:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_input_embeddings
  2930      def get_input_embeddings(self):

  2932  
  2933:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.set_input_embeddings
  2934      def set_input_embeddings(self, value):

  2937      @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)
  2938:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.forward
  2939      def forward(

  3042  
  3043:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.generate
  3044      def generate(

  3173  
  3174:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.__init__ with SeamlessM4T->SeamlessM4Tv2
  3175      def __init__(self, config: SeamlessM4Tv2Config):

  3189  
  3190:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_encoder
  3191      def get_encoder(self):

  3193  
  3194:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_decoder
  3195      def get_decoder(self):

  3197  
  3198:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_input_embeddings
  3199      def get_input_embeddings(self):

  3201  
  3202:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.set_input_embeddings
  3203      def set_input_embeddings(self, value):

  3208      @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)
  3209:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.forward with SeamlessM4T->SeamlessM4Tv2
  3210      def forward(

  3526  
  3527:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.__init__ with SeamlessM4T->SeamlessM4Tv2
  3528      def __init__(self, config):

  3539  
  3540:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_encoder
  3541      def get_encoder(self):

  3543  
  3544:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_decoder
  3545      def get_decoder(self):

  3547  
  3548:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_input_embeddings
  3549      def get_input_embeddings(self):

  3551  
  3552:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.set_input_embeddings
  3553      def set_input_embeddings(self, value):

  3556      @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)
  3557:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.forward with SeamlessM4T->SeamlessM4Tv2
  3558      def forward(

  3886  
  3887:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.__init__ with SeamlessM4T->SeamlessM4Tv2
  3888      def __init__(self, config, current_modality="text"):

  3912  
  3913:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_modality
  3914      def set_modality(self, modality="text"):

  3923  
  3924:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_encoder
  3925      def get_encoder(self):

  3930  
  3931:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_input_embeddings
  3932      def get_input_embeddings(self):

  3934  
  3935:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_input_embeddings
  3936      def set_input_embeddings(self, value):

  3941      @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)
  3942:     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.forward with SeamlessM4T->SeamlessM4Tv2
  3943      def forward(

train_real_world/transformers_4573/src/transformers/models/seed_oss/configuration_seed_oss.py:
  17  
  18: from transformers.configuration_utils import PreTrainedConfig
  19: from transformers.modeling_rope_utils import RopeParameters
  20  

  93      ```python
  94:     >>> from transformers import SeedOssModel, SeedOssConfig
  95  

train_real_world/transformers_4573/src/transformers/models/seed_oss/modeling_seed_oss.py:
  480          ```python
  481:         >>> from transformers import AutoTokenizer, SeedOssForCausalLM
  482  

train_real_world/transformers_4573/src/transformers/models/seed_oss/modular_seed_oss.py:
  169          ```python
  170:         >>> from transformers import AutoTokenizer, SeedOssForCausalLM
  171  

train_real_world/transformers_4573/src/transformers/models/segformer/configuration_segformer.py:
  79      ```python
  80:     >>> from transformers import SegformerModel, SegformerConfig
  81  

train_real_world/transformers_4573/src/transformers/models/segformer/convert_segformer_original_to_pytorch.py:
  26  
  27: from transformers import (
  28      SegformerConfig,

  32  )
  33: from transformers.utils import logging
  34  

train_real_world/transformers_4573/src/transformers/models/segformer/image_processing_segformer.py:
  135  
  136:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize
  137      def resize(

  184  
  185:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label
  186      def reduce_label(self, label: ImageInput) -> np.ndarray:

  433  
  434:     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->Segformer
  435      def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):

train_real_world/transformers_4573/src/transformers/models/segformer/modeling_segformer.py:
   60  
   61: # Copied from transformers.models.beit.modeling_beit.drop_path
   62  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   76  
   77: # Copied from transformers.models.convnext.modeling_convnext.ConvNextDropPath with ConvNext->Segformer
   78  class SegformerDropPath(nn.Module):

  641          ```python
  642:         >>> from transformers import AutoImageProcessor, SegformerForSemanticSegmentation
  643          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/segformer/modular_segformer.py:
  21  
  22: from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast
  23  

train_real_world/transformers_4573/src/transformers/models/seggpt/configuration_seggpt.py:
  78      ```python
  79:     >>> from transformers import SegGptConfig, SegGptModel
  80  

train_real_world/transformers_4573/src/transformers/models/seggpt/convert_seggpt_to_hf.py:
  25  
  26: from transformers import SegGptConfig, SegGptForImageSegmentation, SegGptImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/seggpt/image_processing_seggpt.py:
  193  
  194:     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC
  195      def resize(

train_real_world/transformers_4573/src/transformers/models/seggpt/modeling_seggpt.py:
   89  
   90: # Copied from transformers.models.sam.modeling_sam.SamPatchEmbeddings with Sam->SegGpt
   91  class SegGptPatchEmbeddings(nn.Module):

  352  
  353: # Copied from transformers.models.sam.modeling_sam.SamMLPBlock with SamMLPBlock->SegGptMlp
  354  class SegGptMlp(nn.Module):

  367  
  368: # Copied from transformers.models.beit.modeling_beit.drop_path
  369  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  383  
  384: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->SegGpt
  385  class SegGptDropPath(nn.Module):

  505  
  506: # Copied from transformers.models.convnext.modeling_convnext.ConvNextLayerNorm with ConvNext->SegGpt
  507  class SegGptLayerNorm(nn.LayerNorm):

  673          ```python
  674:         >>> from transformers import SegGptImageProcessor, SegGptModel
  675          >>> from PIL import Image

  870          ```python
  871:         >>> from transformers import SegGptImageProcessor, SegGptForImageSegmentation
  872          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/sew/configuration_sew.py:
  140      ```python
  141:     >>> from transformers import SEWConfig, SEWModel
  142  

train_real_world/transformers_4573/src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py:
  27  
  28: from transformers import (
  29      SEWConfig,

train_real_world/transformers_4573/src/transformers/models/sew/modeling_sew.py:
  719  
  720:     # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states
  721      def _mask_hidden_states(

train_real_world/transformers_4573/src/transformers/models/sew/modular_sew.py:
  338  
  339:     # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states
  340      def _mask_hidden_states(

train_real_world/transformers_4573/src/transformers/models/sew_d/configuration_sew_d.py:
  155      ```python
  156:     >>> from transformers import SEWDConfig, SEWDModel
  157  

train_real_world/transformers_4573/src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py:
  27  
  28: from transformers import (
  29      SEWDConfig,

train_real_world/transformers_4573/src/transformers/models/sew_d/modeling_sew_d.py:
    41  
    42: # Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices
    43  def _compute_mask_indices(

   207  @torch.jit.script
   208: # Copied from transformers.models.deberta.modeling_deberta.c2p_dynamic_expand
   209  def c2p_dynamic_expand(c2p_pos, query_layer, relative_pos):

   213  @torch.jit.script
   214: # Copied from transformers.models.deberta.modeling_deberta.p2c_dynamic_expand
   215  def p2c_dynamic_expand(c2p_pos, query_layer, key_layer):

   219  @torch.jit.script
   220: # Copied from transformers.models.deberta.modeling_deberta.pos_dynamic_expand
   221  def pos_dynamic_expand(pos_index, p2c_att, key_layer):

   243  
   244: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SEWD
   245  class SEWDNoLayerNormConvLayer(GradientCheckpointingLayer):

   265  
   266: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SEWD
   267  class SEWDLayerNormConvLayer(GradientCheckpointingLayer):

   293  
   294: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SEWD
   295  class SEWDGroupNormConvLayer(GradientCheckpointingLayer):

   318  
   319: # Copied from transformers.models.sew.modeling_sew.SEWPositionalConvEmbedding with SEW->SEWD
   320  class SEWDPositionalConvEmbedding(nn.Module):

   362  
   363: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->SEW
   364  class SEWDSamePadLayer(nn.Module):

   374  
   375: # Copied from transformers.models.sew.modeling_sew.SEWUpsampling with SEW->SEWD
   376  class SEWDUpsampling(nn.Module):

   397  
   398: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->SEWD
   399  class SEWDFeatureEncoder(nn.Module):

   472      >>> import torch
   473:     >>> from transformers.models.deberta_v2.modeling_deberta_v2 import XSoftmax
   474  

   883  
   884: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->SEWD
   885  class SEWDIntermediate(nn.Module):

  1243  @auto_docstring
  1244: # Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps
  1245  class SEWDModel(SEWDPreTrainedModel):

  1264  
  1265:     # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states
  1266      def _mask_hidden_states(

  1372  )
  1373: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV2VEC2->SEWD
  1374  class SEWDForCTC(SEWDPreTrainedModel):

  1518  )
  1519: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV2VEC2->SEWD
  1520  class SEWDForSequenceClassification(SEWDPreTrainedModel):

train_real_world/transformers_4573/src/transformers/models/shieldgemma2/configuration_shieldgemma2.py:
  55      ```python
  56:     >>> from transformers import ShieldGemma2ForConditionalGeneration, ShieldGemma2Config, SiglipVisionConfig, ShieldGemma2TextConfig
  57  

train_real_world/transformers_4573/src/transformers/models/siglip/configuration_siglip.py:
   67      ```python
   68:     >>> from transformers import SiglipTextConfig, SiglipTextModel
   69  

  151      ```python
  152:     >>> from transformers import SiglipVisionConfig, SiglipVisionModel
  153  

  215      ```python
  216:     >>> from transformers import SiglipConfig, SiglipModel
  217  

  227      >>> # We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig
  228:     >>> from transformers import SiglipTextConfig, SiglipVisionConfig
  229  

train_real_world/transformers_4573/src/transformers/models/siglip/convert_siglip_to_hf.py:
  30  
  31: from transformers import (
  32      GemmaTokenizerFast,

  38  )
  39: from transformers.utils import logging
  40  

train_real_world/transformers_4573/src/transformers/models/siglip/modeling_siglip.py:
    81  )
    82: # Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Siglip
    83  class SiglipVisionModelOutput(ModelOutput):

   100  )
   101: # Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Siglip
   102  class SiglipTextModelOutput(ModelOutput):

   115  @auto_docstring
   116: # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->Siglip
   117  class SiglipOutput(ModelOutput):

   223  
   224: # Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->Siglip
   225  class SiglipTextEmbeddings(nn.Module):

   349  
   350: # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Siglip
   351  class SiglipMLP(nn.Module):

   473  
   474: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->Siglip
   475  class SiglipEncoder(nn.Module):

   601          ```python
   602:         >>> from transformers import AutoTokenizer, SiglipTextModel
   603  

   728          >>> import requests
   729:         >>> from transformers import AutoProcessor, SiglipVisionModel
   730  

   808          ```python
   809:         >>> from transformers import AutoTokenizer, AutoModel
   810          >>> import torch

   845          >>> import torch
   846:         >>> from transformers import AutoProcessor, AutoModel
   847:         >>> from transformers.image_utils import load_image
   848  

   890          >>> import requests
   891:         >>> from transformers import AutoProcessor, AutoModel
   892          >>> import torch

  1011          ```python
  1012:         >>> from transformers import AutoImageProcessor, SiglipForImageClassification
  1013          >>> import torch

train_real_world/transformers_4573/src/transformers/models/siglip2/configuration_siglip2.py:
   72      ```python
   73:     >>> from transformers import Siglip2TextConfig, Siglip2TextModel
   74  

  159      ```python
  160:     >>> from transformers import Siglip2VisionConfig, Siglip2VisionModel
  161  

  223      ```python
  224:     >>> from transformers import Siglip2Config, Siglip2Model
  225  

  235      >>> # We can also initialize a Siglip2Config from a Siglip2TextConfig and a Siglip2VisionConfig
  236:     >>> from transformers import Siglip2TextConfig, Siglip2VisionConfig
  237  

train_real_world/transformers_4573/src/transformers/models/siglip2/convert_siglip2_to_hf.py:
  29  
  30: from transformers import GemmaTokenizerFast, Siglip2Config, Siglip2ImageProcessorFast, Siglip2Model, Siglip2Processor
  31: from transformers.utils import logging
  32  

train_real_world/transformers_4573/src/transformers/models/siglip2/modeling_siglip2.py:
   691          ```python
   692:         >>> from transformers import AutoTokenizer, Siglip2TextModel
   693  

   786          >>> import requests
   787:         >>> from transformers import AutoProcessor, Siglip2VisionModel
   788  

   867          ```python
   868:         >>> from transformers import AutoTokenizer, AutoModel
   869          >>> import torch

   909          >>> import torch
   910:         >>> from transformers import AutoProcessor, AutoModel
   911:         >>> from transformers.image_utils import load_image
   912  

   962          >>> import requests
   963:         >>> from transformers import AutoProcessor, AutoModel
   964          >>> import torch

  1100          ```python
  1101:         >>> from transformers import AutoImageProcessor, Siglip2ForImageClassification
  1102          >>> import torch

train_real_world/transformers_4573/src/transformers/models/siglip2/modular_siglip2.py:
   20  
   21: from transformers.models.siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig
   22: from transformers.models.siglip.modeling_siglip import (
   23      BaseModelOutput,

   85      ```python
   86:     >>> from transformers import Siglip2VisionConfig, Siglip2VisionModel
   87  

  340          >>> import requests
  341:         >>> from transformers import AutoProcessor, Siglip2VisionModel
  342  

  387          >>> import torch
  388:         >>> from transformers import AutoProcessor, AutoModel
  389:         >>> from transformers.image_utils import load_image
  390  

  438          >>> import requests
  439:         >>> from transformers import AutoProcessor, AutoModel
  440          >>> import torch

  542          ```python
  543:         >>> from transformers import AutoImageProcessor, Siglip2ForImageClassification
  544          >>> import torch

train_real_world/transformers_4573/src/transformers/models/smollm3/configuration_smollm3.py:
  96      ```python
  97:     >>> from transformers import SmolLM3Model, SmolLM3Config
  98  

train_real_world/transformers_4573/src/transformers/models/smollm3/modeling_smollm3.py:
  492          ```python
  493:         >>> from transformers import AutoTokenizer, SmolLM3ForCausalLM
  494  

train_real_world/transformers_4573/src/transformers/models/smollm3/modular_smollm3.py:
  113      ```python
  114:     >>> from transformers import SmolLM3Model, SmolLM3Config
  115  

train_real_world/transformers_4573/src/transformers/models/smolvlm/configuration_smolvlm.py:
   68      ```python
   69:     >>> from transformers.models.smolvlm.modeling_smolvlm import SmolVLMVisionTransformer
   70:     >>> from transformers.models.smolvlm.configuration_smolvlm import SmolVLMVisionConfig
   71  

  143      ```python
  144:     >>> from transformers import SmolVLMModel, SmolVLMConfig
  145      >>> # Initializing configuration

train_real_world/transformers_4573/src/transformers/models/smolvlm/modeling_smolvlm.py:
  772  
  773:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  774:         >>> from transformers.image_utils import load_image
  775  

train_real_world/transformers_4573/src/transformers/models/smolvlm/modular_smolvlm.py:
   79      ```python
   80:     >>> from transformers.models.smolvlm.modeling_smolvlm import SmolVLMVisionTransformer
   81:     >>> from transformers.models.smolvlm.configuration_smolvlm import SmolVLMVisionConfig
   82  

  132      ```python
  133:     >>> from transformers import SmolVLMModel, SmolVLMConfig
  134      >>> # Initializing configuration

  368  
  369:         >>> from transformers import AutoProcessor, AutoModelForImageTextToText
  370:         >>> from transformers.image_utils import load_image
  371  

train_real_world/transformers_4573/src/transformers/models/smolvlm/processing_smolvlm.py:
  239          >>> import requests
  240:         >>> from transformers import SmolVLMProcessor
  241:         >>> from transformers.image_utils import load_image
  242  

train_real_world/transformers_4573/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py:
  46      ```python
  47:     >>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel
  48  

train_real_world/transformers_4573/src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py:
  22  
  23: from transformers import (
  24      MBart50Tokenizer,

train_real_world/transformers_4573/src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py:
  24  
  25: from transformers import (
  26      Speech2Text2Config,

train_real_world/transformers_4573/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:
   36  
   37: # Copied from transformers.models.encoder_decoder.modeling_encoder_decoder.shift_tokens_right
   38  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

  212          ```python
  213:         >>> from transformers import SpeechEncoderDecoderModel
  214  

  369          ```python
  370:         >>> from transformers import SpeechEncoderDecoderModel, AutoProcessor
  371          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/speech_to_text/configuration_speech_to_text.py:
  105      ```python
  106:     >>> from transformers import Speech2TextConfig, Speech2TextModel
  107  

train_real_world/transformers_4573/src/transformers/models/speech_to_text/convert_s2t_fairseq_to_tfms.py:
  19  
  20: from transformers import Speech2TextConfig, Speech2TextForConditionalGeneration
  21  

train_real_world/transformers_4573/src/transformers/models/speech_to_text/modeling_speech_to_text.py:
    50  
    51: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    52  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

   171  
   172: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   173  def eager_attention_forward(

   201  
   202: # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenAttention with Musicgen->Speech2Text
   203  class Speech2TextAttention(nn.Module):

   319  
   320: # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT
   321  class Speech2TextEncoderLayer(GradientCheckpointingLayer):

   379  
   380: # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT
   381  # TODO: change copy when applying cache class

   413  
   414:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward
   415      def forward(

   939           >>> import torch
   940:          >>> from transformers import Speech2TextModel, AutoFeatureExtractor
   941           >>> from datasets import load_dataset

  1080          >>> import torch
  1081:         >>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
  1082          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/speecht5/configuration_speecht5.py:
  175      ```python
  176:     >>> from transformers import SpeechT5Model, SpeechT5Config
  177  

  380      ```python
  381:     >>> from transformers import SpeechT5HifiGan, SpeechT5HifiGanConfig
  382  

train_real_world/transformers_4573/src/transformers/models/speecht5/convert_hifigan.py:
  21  
  22: from transformers import SpeechT5HifiGan, SpeechT5HifiGanConfig, logging
  23  

train_real_world/transformers_4573/src/transformers/models/speecht5/convert_speecht5_original_pytorch_checkpoint_to_pytorch.py:
  20  
  21: from transformers import (
  22      SpeechT5Config,

  30  )
  31: from transformers.tokenization_python import AddedToken
  32  

train_real_world/transformers_4573/src/transformers/models/speecht5/feature_extraction_speecht5.py:
  138      @staticmethod
  139:     # Copied from transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm
  140      def zero_mean_unit_var_norm(

train_real_world/transformers_4573/src/transformers/models/speecht5/modeling_speecht5.py:
    52  
    53: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    54  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    90  
    91: # Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices
    92  def _compute_mask_indices(

   210  
   211: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SpeechT5
   212  class SpeechT5NoLayerNormConvLayer(GradientCheckpointingLayer):

   232  
   233: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SpeechT5
   234  class SpeechT5LayerNormConvLayer(GradientCheckpointingLayer):

   260  
   261: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SpeechT5
   262  class SpeechT5GroupNormConvLayer(GradientCheckpointingLayer):

   285  
   286: # Copied from transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding with Speech2Text->SpeechT5
   287  class SpeechT5SinusoidalPositionalEmbedding(nn.Module):

   355  
   356: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->SpeechT5
   357  class SpeechT5PositionalConvEmbedding(nn.Module):

   445  
   446: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->SpeechT5
   447  class SpeechT5SamePadLayer(nn.Module):

   457  
   458: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->SpeechT5
   459  class SpeechT5FeatureEncoder(nn.Module):

   498  
   499: # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->SpeechT5
   500  class SpeechT5FeatureProjection(nn.Module):

   569  
   570:     # Copied from transformers.models.unispeech.modeling_unispeech.UniSpeechPreTrainedModel._get_feature_vector_attention_mask
   571      def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):

   585  
   586:     # Copied from transformers.models.unispeech.modeling_unispeech.UniSpeechPreTrainedModel._get_feat_extract_output_lengths
   587      def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):

   601  
   602:     # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states
   603      def _mask_hidden_states(

  2117          ```python
  2118:         >>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToText
  2119          >>> from datasets import load_dataset

  2425          ```python
  2426:         >>> from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, set_seed
  2427          >>> import torch

  2762          ```python
  2763:         >>> from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan, set_seed
  2764          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/splinter/configuration_splinter.py:
  71      ```python
  72:     >>> from transformers import SplinterModel, SplinterConfig
  73  

train_real_world/transformers_4573/src/transformers/models/splinter/modeling_splinter.py:
   91  
   92: # Copied from transformers.models.align.modeling_align.eager_attention_forward
   93  def eager_attention_forward(

  115  
  116: # Copied from transformers.models.align.modeling_align.AlignTextSelfAttention with AlignText->Splinter
  117  class SplinterSelfAttention(nn.Module):

  172  
  173: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Splinter
  174  class SplinterSelfOutput(nn.Module):

  187  
  188: # Copied from transformers.models.align.modeling_align.AlignTextAttention with AlignText->Splinter
  189  class SplinterAttention(nn.Module):

  212  
  213: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Splinter
  214  class SplinterIntermediate(nn.Module):

  228  
  229: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Splinter
  230  class SplinterOutput(nn.Module):

  243  
  244: # Copied from transformers.models.align.modeling_align.AlignTextLayer with AlignText->Splinter
  245  class SplinterLayer(GradientCheckpointingLayer):

  282  
  283: # Copied from transformers.models.align.modeling_align.AlignTextEncoder with AlignText->Splinter
  284  class SplinterEncoder(nn.Module):

train_real_world/transformers_4573/src/transformers/models/squeezebert/configuration_squeezebert.py:
  83      ```python
  84:     >>> from transformers import SqueezeBertConfig, SqueezeBertModel
  85  

train_real_world/transformers_4573/src/transformers/models/stablelm/configuration_stablelm.py:
  96      ```python
  97:     >>> from transformers import StableLmModel, StableLmConfig
  98  

train_real_world/transformers_4573/src/transformers/models/stablelm/modeling_stablelm.py:
   60  
   61: # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->StableLm
   62  class StableLmRotaryEmbedding(nn.Module):

  129  
  130: # Copied from transformers.models.llama.modeling_llama.rotate_half
  131  def rotate_half(x):

  137  
  138: # Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
  139  def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):

  165  
  166: # Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->StableLm
  167  class StableLmMLP(nn.Module):

  197  
  198: # Copied from transformers.models.llama.modeling_llama.repeat_kv
  199  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

  210  
  211: # Copied from transformers.models.llama.modeling_llama.eager_attention_forward
  212  def eager_attention_forward(

  419  
  420:         # copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.forward
  421          if self.use_parallel_residual:

  571  
  572:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  573      def _update_causal_mask(

  641      @staticmethod
  642:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  643      def _prepare_4d_causal_attention_mask_with_cache_position(

  697  
  698: # Copied from transformers.models.persimmon.modeling_persimmon.PersimmonForCausalLM with PERSIMMON->STABLELM,Persimmon->StableLm
  699  class StableLmForCausalLM(StableLmPreTrainedModel, GenerationMixin):

  701  
  702:     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with LLAMA->STABLELM,Llama->StableLm
  703      def __init__(self, config):

  738          ```python
  739:         >>> from transformers import AutoTokenizer, StableLmForCausalLM
  740  

train_real_world/transformers_4573/src/transformers/models/starcoder2/configuration_starcoder2.py:
  89      ```python
  90:     >>> from transformers import Starcoder2Model, Starcoder2Config
  91  

train_real_world/transformers_4573/src/transformers/models/starcoder2/modeling_starcoder2.py:
   32  
   33: from transformers.utils.generic import check_model_inputs
   34  

  458          ```python
  459:         >>> from transformers import AutoTokenizer, Starcoder2ForCausalLM
  460  

train_real_world/transformers_4573/src/transformers/models/starcoder2/modular_starcoder2.py:
  27  
  28: from transformers.utils.generic import check_model_inputs
  29  

train_real_world/transformers_4573/src/transformers/models/superglue/configuration_superglue.py:
  56          ```python
  57:         >>> from transformers import SuperGlueConfig, SuperGlueModel
  58  

train_real_world/transformers_4573/src/transformers/models/superglue/convert_superglue_to_hf.py:
  21  
  22: from transformers import (
  23      AutoModelForKeypointDetection,

train_real_world/transformers_4573/src/transformers/models/superglue/image_processing_superglue.py:
   55  
   56: # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale
   57  def is_grayscale(

   70  
   71: # Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale
   72  def convert_to_grayscale(

  192  
  193:     # Copied from transformers.models.superpoint.image_processing_superpoint.SuperPointImageProcessor.resize
  194      def resize(

  413  
  414:     # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor.visualize_keypoint_matching with EfficientLoFTR->SuperGlue
  415      def visualize_keypoint_matching(

  468  
  469:     # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor._get_color
  470      def _get_color(self, score):

train_real_world/transformers_4573/src/transformers/models/superglue/modeling_superglue.py:
   22  
   23: from transformers import PreTrainedModel
   24: from transformers.models.superglue.configuration_superglue import SuperGlueConfig
   25  

  678          ```python
  679:         >>> from transformers import AutoImageProcessor, AutoModel
  680          >>> import torch

train_real_world/transformers_4573/src/transformers/models/superpoint/configuration_superpoint.py:
  50      ```python
  51:     >>> from transformers import SuperPointConfig, SuperPointForKeypointDetection
  52  

train_real_world/transformers_4573/src/transformers/models/superpoint/convert_superpoint_to_pytorch.py:
  20  
  21: from transformers import SuperPointConfig, SuperPointForKeypointDetection, SuperPointImageProcessor
  22  

train_real_world/transformers_4573/src/transformers/models/superpoint/modeling_superpoint.py:
   21  
   22: from transformers import PreTrainedModel
   23: from transformers.modeling_outputs import (
   24      BaseModelOutputWithNoAttention,
   25  )
   26: from transformers.models.superpoint.configuration_superpoint import SuperPointConfig
   27  

  386          ```python
  387:         >>> from transformers import AutoImageProcessor, SuperPointForKeypointDetection
  388          >>> import torch

train_real_world/transformers_4573/src/transformers/models/swiftformer/configuration_swiftformer.py:
  72      ```python
  73:     >>> from transformers import SwiftFormerConfig, SwiftFormerModel
  74  

train_real_world/transformers_4573/src/transformers/models/swiftformer/convert_swiftformer_original_to_hf.py:
  25  
  26: from transformers import (
  27      SwiftFormerConfig,

  30  )
  31: from transformers.utils import logging
  32  

train_real_world/transformers_4573/src/transformers/models/swiftformer/modeling_swiftformer.py:
  61  
  62: # Copied from transformers.models.beit.modeling_beit.drop_path
  63  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

train_real_world/transformers_4573/src/transformers/models/swin/configuration_swin.py:
  85      ```python
  86:     >>> from transformers import SwinConfig, SwinModel
  87  

train_real_world/transformers_4573/src/transformers/models/swin/convert_swin_simmim_to_pytorch.py:
  24  
  25: from transformers import SwinConfig, SwinForMaskedImageModeling, ViTImageProcessor
  26  

train_real_world/transformers_4573/src/transformers/models/swin/convert_swin_timm_to_pytorch.py:
   9  
  10: from transformers import AutoImageProcessor, SwinConfig, SwinForImageClassification
  11  

train_real_world/transformers_4573/src/transformers/models/swin/modeling_swin.py:
   197  
   198:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   199      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   363  
   364: # Copied from transformers.models.beit.modeling_beit.drop_path
   365  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   379  
   380: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Swin
   381  class SwinDropPath(nn.Module):

   961          ```python
   962:         >>> from transformers import AutoImageProcessor, SwinForMaskedImageModeling
   963          >>> import torch

  1146          ```python
  1147:         >>> from transformers import AutoImageProcessor, AutoBackbone
  1148          >>> import torch

train_real_world/transformers_4573/src/transformers/models/swin2sr/configuration_swin2sr.py:
  82      ```python
  83:     >>> from transformers import Swin2SRConfig, Swin2SRModel
  84  

train_real_world/transformers_4573/src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py:
  23  
  24: from transformers import Swin2SRConfig, Swin2SRForImageSuperResolution, Swin2SRImageProcessor
  25  

train_real_world/transformers_4573/src/transformers/models/swin2sr/modeling_swin2sr.py:
    49  
    50: # Copied from transformers.models.swin.modeling_swin.window_partition
    51  def window_partition(input_feature, window_size):

    62  
    63: # Copied from transformers.models.swin.modeling_swin.window_reverse
    64  def window_reverse(windows, window_size, height, width):

    73  
    74: # Copied from transformers.models.beit.modeling_beit.drop_path
    75  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

    89  
    90: # Copied from transformers.models.swin.modeling_swin.SwinDropPath with Swin->Swin2SR
    91  class Swin2SRDropPath(nn.Module):

   175  
   176: # Copied from transformers.models.swinv2.modeling_swinv2.Swinv2PatchMerging with Swinv2->Swin2SR
   177  class Swin2SRPatchMerging(nn.Module):

   230  
   231: # Copied from transformers.models.swinv2.modeling_swinv2.Swinv2SelfAttention with Swinv2->Swin2SR
   232  class Swin2SRSelfAttention(nn.Module):

   367  
   368: # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->Swin2SR
   369  class Swin2SRSelfOutput(nn.Module):

   381  
   382: # Copied from transformers.models.swinv2.modeling_swinv2.Swinv2Attention with Swinv2->Swin2SR
   383  class Swin2SRAttention(nn.Module):

   408  
   409: # Copied from transformers.models.swin.modeling_swin.SwinIntermediate with Swin->Swin2SR
   410  class Swin2SRIntermediate(nn.Module):

   424  
   425: # Copied from transformers.models.swin.modeling_swin.SwinOutput with Swin->Swin2SR
   426  class Swin2SROutput(nn.Module):

   437  
   438: # Copied from transformers.models.swinv2.modeling_swinv2.Swinv2Layer with Swinv2->Swin2SR
   439  class Swin2SRLayer(nn.Module):

  1001  
  1002:          >>> from transformers import AutoImageProcessor, Swin2SRForImageSuperResolution
  1003  

train_real_world/transformers_4573/src/transformers/models/swinv2/configuration_swinv2.py:
  85      ```python
  86:     >>> from transformers import Swinv2Config, Swinv2Model
  87  

train_real_world/transformers_4573/src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py:
  25  
  26: from transformers import AutoImageProcessor, Swinv2Config, Swinv2ForImageClassification
  27  

train_real_world/transformers_4573/src/transformers/models/swinv2/modeling_swinv2.py:
    48  )
    49: # Copied from transformers.models.swin.modeling_swin.SwinEncoderOutput with Swin->Swinv2
    50  class Swinv2EncoderOutput(ModelOutput):

    71  )
    72: # Copied from transformers.models.swin.modeling_swin.SwinModelOutput with Swin->Swinv2
    73  class Swinv2ModelOutput(ModelOutput):

    97  )
    98: # Copied from transformers.models.swin.modeling_swin.SwinMaskedImageModelingOutput with Swin->Swinv2
    99  class Swinv2MaskedImageModelingOutput(ModelOutput):

   134  )
   135: # Copied from transformers.models.swin.modeling_swin.SwinImageClassifierOutput with Swin->Swinv2
   136  class Swinv2ImageClassifierOutput(ModelOutput):

   156  
   157: # Copied from transformers.models.swin.modeling_swin.window_partition
   158  def window_partition(input_feature, window_size):

   169  
   170: # Copied from transformers.models.swin.modeling_swin.window_reverse
   171  def window_reverse(windows, window_size, height, width):

   180  
   181: # Copied from transformers.models.swin.modeling_swin.drop_path
   182  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   196  
   197: # Copied from transformers.models.swin.modeling_swin.SwinDropPath with Swin->Swinv2
   198  class Swinv2DropPath(nn.Module):

   211  
   212: # Copied from transformers.models.swin.modeling_swin.SwinEmbeddings with Swin->Swinv2
   213  class Swinv2Embeddings(nn.Module):

   235  
   236:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   237      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

   304  
   305: # Copied from transformers.models.swin.modeling_swin.SwinPatchEmbeddings with Swin->Swinv2
   306  class Swinv2PatchEmbeddings(nn.Module):

   538  
   539: # Copied from transformers.models.swin.modeling_swin.SwinSelfOutput with Swin->Swinv2
   540  class Swinv2SelfOutput(nn.Module):

   578  
   579: # Copied from transformers.models.swin.modeling_swin.SwinIntermediate with Swin->Swinv2
   580  class Swinv2Intermediate(nn.Module):

   594  
   595: # Copied from transformers.models.swin.modeling_swin.SwinOutput with Swin->Swinv2
   596  class Swinv2Output(nn.Module):

   917  @auto_docstring
   918: # Copied from transformers.models.swin.modeling_swin.SwinModel with SWIN->SWINV2,Swin->Swinv2
   919  class Swinv2Model(Swinv2PreTrainedModel):

  1014  )
  1015: # Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling with swin->swinv2, base-simmim-window6-192->tiny-patch4-window8-256,SWIN->SWINV2,Swin->Swinv2,192->256
  1016  class Swinv2ForMaskedImageModeling(Swinv2PreTrainedModel):

  1049          ```python
  1050:         >>> from transformers import AutoImageProcessor, Swinv2ForMaskedImageModeling
  1051          >>> import torch

  1131  )
  1132: # Copied from transformers.models.swin.modeling_swin.SwinForImageClassification with SWIN->SWINV2,Swin->Swinv2,swin->swinv2
  1133  class Swinv2ForImageClassification(Swinv2PreTrainedModel):

  1228          ```python
  1229:         >>> from transformers import AutoImageProcessor, AutoBackbone
  1230          >>> import torch

train_real_world/transformers_4573/src/transformers/models/switch_transformers/convert_big_switch.py:
   10  
   11: from transformers.models.switch_transformers.convert_switch_transformers_original_flax_checkpoint_to_pytorch import (
   12      rename_keys,
   13  )
   14: from transformers.utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME
   15: from transformers.utils.hub import convert_file_size_to_int
   16  

  180  def sanity_check():
  181:     from transformers import SwitchTransformersConfig, SwitchTransformersForConditionalGeneration, T5Tokenizer
  182  

train_real_world/transformers_4573/src/transformers/models/switch_transformers/convert_switch_transformers_original_flax_checkpoint_to_pytorch.py:
  26  
  27: from transformers import SwitchTransformersConfig, SwitchTransformersForConditionalGeneration
  28: from transformers.utils import logging
  29  

train_real_world/transformers_4573/src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import T5Config, T5ForConditionalGeneration
  23: from transformers.utils import logging
  24  

train_real_world/transformers_4573/src/transformers/models/t5/convert_t5x_checkpoint_to_pytorch.py:
  37  
  38: from transformers import T5Config, T5EncoderModel, T5ForConditionalGeneration
  39: from transformers.utils import logging
  40  

train_real_world/transformers_4573/src/transformers/models/t5/modeling_t5.py:
   913          ```python
   914:         >>> from transformers import AutoTokenizer, T5Model
   915  

  1081          ```python
  1082:         >>> from transformers import AutoTokenizer, T5ForConditionalGeneration
  1083  

  1225          ```python
  1226:         >>> from transformers import AutoTokenizer, T5EncoderModel
  1227  

train_real_world/transformers_4573/src/transformers/models/t5gemma/configuration_t5gemma.py:
   98      ```python
   99:     >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig
  100      >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration

  196      ```python
  197:     >>> from transformers import T5GemmaConfig, T5GemmaModel
  198      >>> t5gemma_config = T5GemmaConfig.from_pretrained("google/t5gemma-2b-2b-prefixlm-it")

train_real_world/transformers_4573/src/transformers/models/t5gemma/modular_t5gemma.py:
  135      ```python
  136:     >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig
  137      >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration

  210      ```python
  211:     >>> from transformers import T5GemmaConfig, T5GemmaModel
  212      >>> t5gemma_config = T5GemmaConfig.from_pretrained("google/t5gemma-2b-2b-prefixlm-it")

train_real_world/transformers_4573/src/transformers/models/t5gemma2/configuration_t5gemma2.py:
  244      ```python
  245:     >>> from transformers import T5Gemma2EncoderForConditionalGeneration, T5Gemma2EncoderConfig, SiglipVisionConfig, T5Gemma2EncoderTextConfig
  246  

  517      ```python
  518:     >>> from transformers import T5Gemma2Config, T5Gemma2Model
  519      >>> t5gemma2_config = T5Gemma2Config.from_pretrained("google/t5gemma-270m-270m")

train_real_world/transformers_4573/src/transformers/models/t5gemma2/modular_t5gemma2.py:
  392      ```python
  393:     >>> from transformers import T5Gemma2Config, T5Gemma2Model
  394      >>> t5gemma2_config = T5Gemma2Config.from_pretrained("google/t5gemma-270m-270m")

train_real_world/transformers_4573/src/transformers/models/table_transformer/configuration_table_transformer.py:
  116      ```python
  117:     >>> from transformers import TableTransformerModel, TableTransformerConfig
  118  

  136  
  137:     # Copied from transformers.models.detr.configuration_detr.DetrConfig.__init__
  138      def __init__(

train_real_world/transformers_4573/src/transformers/models/table_transformer/convert_table_transformer_to_hf_no_timm.py:
  27  
  28: from transformers import DetrImageProcessor, ResNetConfig, TableTransformerConfig, TableTransformerForObjectDetection
  29: from transformers.utils import logging
  30  

train_real_world/transformers_4573/src/transformers/models/table_transformer/convert_table_transformer_to_hf.py:
  28  
  29: from transformers import DetrImageProcessor, TableTransformerConfig, TableTransformerForObjectDetection
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/table_transformer/modeling_table_transformer.py:
    55  )
    56: # Copied from transformers.models.detr.modeling_detr.DetrDecoderOutput with DETR->TABLE_TRANSFORMER,Detr->TableTransformer
    57  class TableTransformerDecoderOutput(BaseModelOutputWithCrossAttentions):

    78  )
    79: # Copied from transformers.models.detr.modeling_detr.DetrModelOutput with DETR->TABLE_TRANSFORMER,Detr->TableTransformer
    80  class TableTransformerModelOutput(Seq2SeqModelOutput):

    97  )
    98: # Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->TableTransformer,DetrImageProcessor->DetrImageProcessor
    99  class TableTransformerObjectDetectionOutput(ModelOutput):

   135  
   136: # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->TableTransformer
   137  class TableTransformerFrozenBatchNorm2d(nn.Module):

   175  
   176: # Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->TableTransformer
   177  def replace_batch_norm(model):

   200  
   201: # Copied from transformers.models.detr.modeling_detr.DetrConvEncoder with Detr->TableTransformer
   202  class TableTransformerConvEncoder(nn.Module):

   273  
   274: # Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->TableTransformer
   275  class TableTransformerConvModel(nn.Module):

   295  
   296: # Copied from transformers.models.detr.modeling_detr.DetrSinePositionEmbedding with Detr->TableTransformer
   297  class TableTransformerSinePositionEmbedding(nn.Module):

   333  
   334: # Copied from transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding with Detr->TableTransformer
   335  class TableTransformerLearnedPositionEmbedding(nn.Module):

   357  
   358: # Copied from transformers.models.detr.modeling_detr.build_position_encoding with Detr->TableTransformer
   359  def build_position_encoding(config):

   371  
   372: # Copied from transformers.models.detr.modeling_detr.DetrAttention with DETR->TABLE_TRANSFORMER,Detr->TableTransformer
   373  class TableTransformerAttention(nn.Module):

   506  class TableTransformerEncoderLayer(nn.Module):
   507:     # Copied from transformers.models.detr.modeling_detr.DetrEncoderLayer.__init__ with Detr->TableTransformer
   508      def __init__(self, config: TableTransformerConfig):

   579  class TableTransformerDecoderLayer(GradientCheckpointingLayer):
   580:     # Copied from transformers.models.detr.modeling_detr.DetrDecoderLayer.__init__ with Detr->TableTransformer
   581      def __init__(self, config: TableTransformerConfig):

   832  
   833: # Copied from transformers.models.detr.modeling_detr.DetrDecoder with DETR->TABLE_TRANSFORMER,Detr->TableTransformer
   834  class TableTransformerDecoder(TableTransformerPreTrainedModel):

  1007  class TableTransformerModel(TableTransformerPreTrainedModel):
  1008:     # Copied from transformers.models.detr.modeling_detr.DetrModel.__init__ with Detr->TableTransformer
  1009      def __init__(self, config: TableTransformerConfig):

  1062          ```python
  1063:         >>> from transformers import AutoImageProcessor, TableTransformerModel
  1064          >>> from huggingface_hub import hf_hub_download

  1176  class TableTransformerForObjectDetection(TableTransformerPreTrainedModel):
  1177:     # Copied from transformers.models.detr.modeling_detr.DetrForObjectDetection.__init__ with Detr->TableTransformer
  1178      def __init__(self, config: TableTransformerConfig):

  1228          >>> from huggingface_hub import hf_hub_download
  1229:         >>> from transformers import AutoImageProcessor, TableTransformerForObjectDetection
  1230          >>> import torch

  1310  
  1311: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->TableTransformer,detr->table_transformer
  1312  class TableTransformerMLPPredictionHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/tapas/configuration_tapas.py:
  127      ```python
  128:     >>> from transformers import TapasModel, TapasConfig
  129  

train_real_world/transformers_4573/src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import (
  23      TapasConfig,

  29  )
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/tapas/modeling_tapas.py:
   247  
   248: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
   249  class TapasSelfOutput(nn.Module):

   268  
   269:     # Copied from transformers.models.rembert.modeling_rembert.RemBertAttention.forward
   270      def forward(

   291  
   292: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
   293  class TapasIntermediate(nn.Module):

   307  
   308: # Copied from transformers.models.bert.modeling_bert.BertOutput
   309  class TapasOutput(nn.Module):

   337  
   338:     # Copied from transformers.models.rembert.modeling_rembert.RemBertLayer.forward
   339      def forward(

   383  
   384:     # Copied from transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk
   385      def feed_forward_chunk(self, attention_output):

   442  
   443: # Copied from transformers.models.bert.modeling_bert.BertPooler
   444  class TapasPooler(nn.Module):

   458  
   459: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->Tapas
   460  class TapasPredictionHeadTransform(nn.Module):

   476  
   477: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Tapas
   478  class TapasLMPredictionHead(nn.Module):

   493  
   494: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Tapas
   495  class TapasOnlyMLMHead(nn.Module):

   583          ```python
   584:         >>> from transformers import AutoTokenizer, TapasModel
   585          >>> import pandas as pd

   729          ```python
   730:         >>> from transformers import AutoTokenizer, TapasForMaskedLM
   731          >>> import pandas as pd

   887          ```python
   888:         >>> from transformers import AutoTokenizer, TapasForQuestionAnswering
   889          >>> import pandas as pd

  1191          ```python
  1192:         >>> from transformers import AutoTokenizer, TapasForSequenceClassification
  1193          >>> import torch

train_real_world/transformers_4573/src/transformers/models/textnet/configuration_textnet.py:
  16  
  17: from transformers import PreTrainedConfig
  18: from transformers.utils import logging
  19: from transformers.utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices
  20  

  70      ```python
  71:     >>> from transformers import TextNetConfig, TextNetBackbone
  72  

train_real_world/transformers_4573/src/transformers/models/textnet/convert_textnet_to_hf.py:
  26  
  27: from transformers import TextNetBackbone, TextNetConfig, TextNetImageProcessor
  28  

train_real_world/transformers_4573/src/transformers/models/textnet/modeling_textnet.py:
   22  
   23: from transformers import PreTrainedModel
   24: from transformers.activations import ACT2CLS
   25: from transformers.modeling_outputs import (
   26      BackboneOutput,

   30  )
   31: from transformers.models.textnet.configuration_textnet import TextNetConfig
   32: from transformers.utils import logging
   33: from transformers.utils.backbone_utils import BackboneMixin
   34  

  306          >>> import requests
  307:         >>> from transformers import TextNetForImageClassification, TextNetImageProcessor
  308          >>> from PIL import Image

  373          >>> from PIL import Image
  374:         >>> from transformers import AutoImageProcessor, AutoBackbone
  375  

train_real_world/transformers_4573/src/transformers/models/time_series_transformer/configuration_time_series_transformer.py:
  111      ```python
  112:     >>> from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel
  113  

train_real_world/transformers_4573/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py:
   228  
   229: # Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->TimeSeries
   230  class TimeSeriesSinusoidalPositionalEmbedding(nn.Embedding):

   272  
   273: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   274  def eager_attention_forward(

   302  
   303: # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->TimeSeriesTransformer
   304  class TimeSeriesTransformerAttention(nn.Module):

   428  
   429: # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->TimeSeriesTransformer, BART->TIME_SERIES_TRANSFORMER
   430  class TimeSeriesTransformerEncoderLayer(GradientCheckpointingLayer):

   496  
   497: # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->TimeSeriesTransformer, with BART->TIME_SERIES_TRANSFORMER
   498  class TimeSeriesTransformerDecoderLayer(GradientCheckpointingLayer):

  1175          >>> import torch
  1176:         >>> from transformers import TimeSeriesTransformerModel
  1177  

  1429          >>> import torch
  1430:         >>> from transformers import TimeSeriesTransformerForPrediction
  1431  

train_real_world/transformers_4573/src/transformers/models/timesfm/convert_timesfm_orignal_to_hf.py:
   9  
  10: from transformers import TimesFmConfig, TimesFmModelForPrediction
  11  

train_real_world/transformers_4573/src/transformers/models/timesfm/modeling_timesfm.py:
  709          ```python
  710:         >>> from transformers import TimesFmModelForPrediction
  711  

train_real_world/transformers_4573/src/transformers/models/timesfm/modular_timesfm.py:
  665          ```python
  666:         >>> from transformers import TimesFmModelForPrediction
  667  

train_real_world/transformers_4573/src/transformers/models/timesformer/configuration_timesformer.py:
  72      ```python
  73:     >>> from transformers import TimesformerConfig, TimesformerModel
  74  

train_real_world/transformers_4573/src/transformers/models/timesformer/convert_timesformer_to_pytorch.py:
  24  
  25: from transformers import TimesformerConfig, TimesformerForVideoClassification, VideoMAEImageProcessor
  26  

train_real_world/transformers_4573/src/transformers/models/timesformer/modeling_timesformer.py:
  149  
  150: # Copied from transformers.models.beit.modeling_beit.drop_path
  151  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  165  
  166: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->TimeSformer
  167  class TimeSformerDropPath(nn.Module):

  505  
  506:         >>> from transformers import AutoImageProcessor, TimesformerModel
  507          >>> from huggingface_hub import hf_hub_download

  642  
  643:         >>> from transformers import AutoImageProcessor, TimesformerForVideoClassification
  644          >>> from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/timm_backbone/configuration_timm_backbone.py:
  50      ```python
  51:     >>> from transformers import TimmBackboneConfig, TimmBackbone
  52  

train_real_world/transformers_4573/src/transformers/models/timm_wrapper/configuration_timm_wrapper.py:
  55      ```python
  56:     >>> from transformers import TimmWrapperModel
  57  

train_real_world/transformers_4573/src/transformers/models/timm_wrapper/modeling_timm_wrapper.py:
  185          >>> from urllib.request import urlopen
  186:         >>> from transformers import AutoModel, AutoImageProcessor
  187  

  315          >>> from urllib.request import urlopen
  316:         >>> from transformers import AutoModelForImageClassification, AutoImageProcessor
  317  

train_real_world/transformers_4573/src/transformers/models/trocr/configuration_trocr.py:
  75      ```python
  76:     >>> from transformers import TrOCRConfig, TrOCRForCausalLM
  77  

train_real_world/transformers_4573/src/transformers/models/trocr/convert_trocr_unilm_to_pytorch.py:
  23  
  24: from transformers import (
  25      RobertaTokenizer,

  33  )
  34: from transformers.utils import logging
  35  

train_real_world/transformers_4573/src/transformers/models/trocr/modeling_trocr.py:
   40  
   41: # Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->TrOCR
   42  class TrOCRLearnedPositionalEmbedding(nn.Embedding):

   68  
   69: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->TrOCR
   70  class TrOCRScaledWordEmbedding(nn.Embedding):

  699          ```python
  700:         >>> from transformers import (
  701          ...     TrOCRConfig,

train_real_world/transformers_4573/src/transformers/models/tvp/image_processing_tvp.py:
  67  
  68: # Copied from transformers.models.vivit.image_processing_vivit.make_batched
  69  def make_batched(videos) -> list[list[ImageInput]]:

train_real_world/transformers_4573/src/transformers/models/tvp/modeling_tvp.py:
  394  
  395: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Tvp
  396  class TvpIntermediate(nn.Module):

  502  
  503: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->Tvp
  504  class TvpPooler(nn.Module):

  729          >>> import torch
  730:         >>> from transformers import AutoConfig, AutoTokenizer, TvpModel
  731  

  834          >>> import torch
  835:         >>> from transformers import AutoConfig, AutoTokenizer, TvpForVideoGrounding
  836  

train_real_world/transformers_4573/src/transformers/models/udop/convert_udop_to_hf.py:
  23  
  24: from transformers import (
  25      LayoutLMv3ImageProcessor,

  30  )
  31: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
  32  

train_real_world/transformers_4573/src/transformers/models/udop/modeling_udop.py:
    30  
    31: from transformers import UdopConfig
    32: from transformers.modeling_outputs import (
    33      Seq2SeqLMOutput,

   312  
   313:     # Copied from transformers.models.prophetnet.modeling_prophetnet.ProphetNetPreTrainedModel._shift_right with ProphetNet->Udop
   314      def _shift_right(self, input_ids):

   336  
   337: # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Udop
   338  class UdopLayerNorm(nn.Module):

   362  
   363: # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->Udop
   364  class UdopDenseActDense(nn.Module):

   385  
   386: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->Udop
   387  class UdopDenseGatedActDense(nn.Module):

   415  
   416: # Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->Udop
   417  class UdopLayerFF(nn.Module):

   434  
   435: # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->Udop
   436  class UdopAttention(nn.Module):

   639  
   640: # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->Udop
   641  class UdopLayerSelfAttention(nn.Module):

   674  
   675: # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->Udop
   676  class UdopLayerCrossAttention(nn.Module):

   711  
   712: # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Udop
   713  class UdopBlock(GradientCheckpointingLayer):

  1293  
  1294:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
  1295      def _update_causal_mask(

  1363      @staticmethod
  1364:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  1365      def _prepare_4d_causal_attention_mask_with_cache_position(

  1502          ```python
  1503:         >>> from transformers import AutoProcessor, AutoModel
  1504          >>> from datasets import load_dataset

  1683          ```python
  1684:         >>> from transformers import AutoProcessor, UdopForConditionalGeneration
  1685          >>> from datasets import load_dataset

  1847          ```python
  1848:         >>> from transformers import AutoProcessor, UdopEncoderModel
  1849          >>> from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/udop/processing_udop.py:
   20  
   21: from transformers import logging
   22  

  163  
  164:     # Copied from transformers.models.layoutlmv3.processing_layoutlmv3.LayoutLMv3Processor.get_overflowing_images
  165      def get_overflowing_images(self, images, overflow_to_sample_mapping):

train_real_world/transformers_4573/src/transformers/models/umt5/convert_umt5_checkpoint_to_pytorch.py:
  38  
  39: from transformers import MT5Config, UMT5EncoderModel, UMT5ForConditionalGeneration
  40: from transformers.utils import logging
  41  

train_real_world/transformers_4573/src/transformers/models/umt5/modeling_umt5.py:
    59  
    60: # Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->UMT5
    61  class UMT5LayerNorm(nn.Module):

    85  
    86: # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->UMT5
    87  class UMT5DenseActDense(nn.Module):

   108  
   109: # Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->UMT5
   110  class UMT5DenseGatedActDense(nn.Module):

   138  
   139: # Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->UMT5
   140  class UMT5LayerFF(nn.Module):

   465  
   466: # Copied from transformers.models.t5.modeling_t5.T5ClassificationHead with T5->UMT5
   467  class UMT5ClassificationHead(nn.Module):

   772  
   773:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask
   774      def _update_causal_mask(

   842      @staticmethod
   843:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
   844      def _prepare_4d_causal_attention_mask_with_cache_position(

   904      ```python
   905:     >>> from transformers import UMT5Model, AutoTokenizer
   906  

   941  
   942:     # Copied from transformers.models.t5.modeling_t5.T5Model.get_input_embeddings
   943      def get_input_embeddings(self):

   945  
   946:     # Copied from transformers.models.t5.modeling_t5.T5Model.set_input_embeddings
   947      def set_input_embeddings(self, new_embeddings):

  1000          ```python
  1001:         >>> from transformers import AutoTokenizer, UMT5Model
  1002  

  1080      ```python
  1081:     >>> from transformers import UMT5ForConditionalGeneration, AutoTokenizer
  1082  

  1120  
  1121:     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_input_embeddings
  1122      def get_input_embeddings(self):

  1124  
  1125:     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_input_embeddings
  1126      def set_input_embeddings(self, new_embeddings):

  1184          ```python
  1185:         >>> from transformers import AutoTokenizer, UMT5ForConditionalGeneration
  1186  

  1275  
  1276:     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels
  1277      def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):

  1286      ```python
  1287:     >>> from transformers import UMT5EncoderModel, AutoTokenizer
  1288  

  1314  
  1315:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings
  1316      def get_input_embeddings(self):

  1318  
  1319:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.set_input_embeddings
  1320      def set_input_embeddings(self, new_embeddings):

  1324      @auto_docstring
  1325:     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with T5->UMT5, google-t5/t5-small->google/umt5-small, t5#training->umt5#training
  1326      def forward(

  1348          ```python
  1349:         >>> from transformers import AutoTokenizer, UMT5EncoderModel
  1350  

  1381  
  1382:     # Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.__init__ with T5->UMT5
  1383      def __init__(self, config: UMT5Config):

  1525  
  1526:     # Copied from transformers.models.t5.modeling_t5.T5ForTokenClassification.__init__ with T5->UMT5
  1527      def __init__(self, config: UMT5Config):

  1538      @auto_docstring
  1539:     # Copied from transformers.models.t5.modeling_t5.T5ForTokenClassification.forward with T5->UMT5, t5->umt5
  1540      def forward(

  1625  
  1626:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_input_embeddings
  1627      def get_input_embeddings(self):

  1629  
  1630:     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.set_input_embeddings
  1631      def set_input_embeddings(self, new_embeddings):

train_real_world/transformers_4573/src/transformers/models/unispeech/configuration_unispeech.py:
  171      ```python
  172:     >>> from transformers import UniSpeechConfig, UniSpeechModel
  173  

train_real_world/transformers_4573/src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import (
  26      UniSpeechConfig,

train_real_world/transformers_4573/src/transformers/models/unispeech/modeling_unispeech.py:
  1118          >>> import torch
  1119:         >>> from transformers import AutoFeatureExtractor, UniSpeechForPreTraining
  1120  

train_real_world/transformers_4573/src/transformers/models/unispeech/modular_unispeech.py:
  361          >>> import torch
  362:         >>> from transformers import AutoFeatureExtractor, UniSpeechForPreTraining
  363  

train_real_world/transformers_4573/src/transformers/models/unispeech_sat/configuration_unispeech_sat.py:
  181      ```python
  182:     >>> from transformers import UniSpeechSatModel, UniSpeechSatConfig
  183  

train_real_world/transformers_4573/src/transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py:
  20  
  21: from transformers import (
  22      UniSpeechSatConfig,

train_real_world/transformers_4573/src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py:
  21  
  22: from transformers import UniSpeechSatConfig, UniSpeechSatForCTC, UniSpeechSatForPreTraining, logging
  23  

train_real_world/transformers_4573/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py:
  1130          >>> import torch
  1131:         >>> from transformers import AutoFeatureExtractor, UniSpeechSatForPreTraining
  1132:         >>> from transformers.models.unispeech_sat.modeling_unispeech_sat import _compute_mask_indices
  1133  

train_real_world/transformers_4573/src/transformers/models/unispeech_sat/modular_unispeech_sat.py:
  379          >>> import torch
  380:         >>> from transformers import AutoFeatureExtractor, UniSpeechSatForPreTraining
  381:         >>> from transformers.models.unispeech_sat.modeling_unispeech_sat import _compute_mask_indices
  382  

train_real_world/transformers_4573/src/transformers/models/univnet/configuration_univnet.py:
  72      ```python
  73:     >>> from transformers import UnivNetModel, UnivNetConfig
  74  

train_real_world/transformers_4573/src/transformers/models/univnet/convert_univnet.py:
  18  
  19: from transformers import UnivNetConfig, UnivNetModel, logging
  20  

train_real_world/transformers_4573/src/transformers/models/univnet/modeling_univnet.py:
  502           ```python
  503:          >>> from transformers import UnivNetFeatureExtractor, UnivNetModel
  504           >>> from datasets import load_dataset, Audio

train_real_world/transformers_4573/src/transformers/models/upernet/configuration_upernet.py:
  72      ```python
  73:     >>> from transformers import UperNetConfig, UperNetForSemanticSegmentation
  74  

train_real_world/transformers_4573/src/transformers/models/upernet/convert_convnext_upernet_to_pytorch.py:
  24  
  25: from transformers import ConvNextConfig, SegformerImageProcessor, UperNetConfig, UperNetForSemanticSegmentation
  26  

train_real_world/transformers_4573/src/transformers/models/upernet/convert_swin_upernet_to_pytorch.py:
  27  
  28: from transformers import SegformerImageProcessor, SwinConfig, UperNetConfig, UperNetForSemanticSegmentation
  29  

train_real_world/transformers_4573/src/transformers/models/upernet/modeling_upernet.py:
  312          ```python
  313:         >>> from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
  314          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/vaultgemma/configuration_vaultgemma.py:
  98      ```python
  99:     >>> from transformers import VaultGemmaModel, VaultGemmaConfig
  100      >>> # Initializing a VaultGemma vaultgemma-7b style configuration

train_real_world/transformers_4573/src/transformers/models/vaultgemma/modeling_vaultgemma.py:
  502          ```python
  503:         >>> from transformers import AutoTokenizer, VaultGemmaForCausalLM
  504  

train_real_world/transformers_4573/src/transformers/models/vaultgemma/modular_vaultgemma.py:
  96      ```python
  97:     >>> from transformers import VaultGemmaModel, VaultGemmaConfig
  98      >>> # Initializing a VaultGemma vaultgemma-7b style configuration

train_real_world/transformers_4573/src/transformers/models/video_llava/configuration_video_llava.py:
  63      ```python
  64:     >>> from transformers import VideoLlavaForConditionalGeneration, VideoLlavaConfig, CLIPVisionConfig, LlamaConfig
  65  

train_real_world/transformers_4573/src/transformers/models/video_llava/convert_video_llava_weights_to_hf.py:
  18  
  19: from transformers import (
  20      AddedToken,

train_real_world/transformers_4573/src/transformers/models/video_llava/modeling_video_llava.py:
  101  
  102: # Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->VideoLlava
  103  class VideoLlavaMultiModalProjector(nn.Module):

  480          >>> from huggingface_hub import hf_hub_download
  481:         >>> from transformers import VideoLlavaProcessor, VideoLlavaForConditionalGeneration
  482  

  628      @staticmethod
  629:     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position
  630      def _prepare_4d_causal_attention_mask_with_cache_position(

train_real_world/transformers_4573/src/transformers/models/videomae/configuration_videomae.py:
  81      ```python
  82:     >>> from transformers import VideoMAEConfig, VideoMAEModel
  83  

train_real_world/transformers_4573/src/transformers/models/videomae/convert_videomae_to_pytorch.py:
  24  
  25: from transformers import (
  26      VideoMAEConfig,

train_real_world/transformers_4573/src/transformers/models/videomae/modeling_videomae.py:
  180  
  181: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  182  def eager_attention_forward(

  270  
  271: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->VideoMAE
  272  class VideoMAESelfOutput(nn.Module):

  288  
  289: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->VideoMAE
  290  class VideoMAEAttention(nn.Module):

  301  
  302: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate ViT->VideoMAE
  303  class VideoMAEIntermediate(nn.Module):

  317  
  318: # Copied from transformers.models.vit.modeling_vit.ViTOutput ViT->VideoMAE
  319  class VideoMAEOutput(nn.Module):

  331  
  332: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->VideoMAE,VIT->VIDEOMAE
  333  class VideoMAELayer(GradientCheckpointingLayer):

  362  
  363: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->VideoMAE
  364  class VideoMAEEncoder(nn.Module):

  433          >>> import torch
  434:         >>> from transformers import VideoMAEVideoProcessor, VideoMAEModel
  435          >>> from huggingface_hub import hf_hub_download

  543          ```python
  544:         >>> from transformers import AutoImageProcessor, VideoMAEForPreTraining
  545          >>> import numpy as np

  710          >>> import torch
  711:         >>> from transformers import VideoMAEVideoProcessor, VideoMAEForVideoClassification
  712          >>> from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/vilt/configuration_vilt.py:
  83      ```python
  84:     >>> from transformers import ViLTModel, ViLTConfig
  85  

train_real_world/transformers_4573/src/transformers/models/vilt/convert_vilt_original_to_pytorch.py:
  25  
  26: from transformers import (
  27      BertTokenizer,

  35  )
  36: from transformers.utils import logging
  37  

train_real_world/transformers_4573/src/transformers/models/vilt/modeling_vilt.py:
   370  
   371: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Vilt
   372  class ViltSelfOutput(nn.Module):

   403  
   404: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->Vilt
   405  class ViltIntermediate(nn.Module):

   419  
   420: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViT->Vilt
   421  class ViltOutput(nn.Module):

   577          ```python
   578:         >>> from transformers import ViltProcessor, ViltModel
   579          >>> from PIL import Image

   732          ```python
   733:         >>> from transformers import ViltProcessor, ViltForMaskedLM
   734          >>> import requests

   900          ```python
   901:         >>> from transformers import ViltProcessor, ViltForQuestionAnswering
   902          >>> import requests

  1003          ```python
  1004:         >>> from transformers import ViltProcessor, ViltForImageAndTextRetrieval
  1005          >>> import requests

  1107          ```python
  1108:         >>> from transformers import ViltProcessor, ViltForImagesAndTextClassification
  1109          >>> import requests

train_real_world/transformers_4573/src/transformers/models/vipllava/configuration_vipllava.py:
  53      ```python
  54:     >>> from transformers import VipLlavaForConditionalGeneration, VipLlavaConfig, CLIPVisionConfig, LlamaConfig
  55  

train_real_world/transformers_4573/src/transformers/models/vipllava/convert_vipllava_weights_to_hf.py:
  18  
  19: from transformers import (
  20      AddedToken,

  44  
  45: # Copied from transformers.models.llava.convert_llava_weights_to_hf.convert_state_dict_to_hf
  46  def convert_state_dict_to_hf(state_dict):

train_real_world/transformers_4573/src/transformers/models/vipllava/modeling_vipllava.py:
  346          >>> import requests
  347:         >>> from transformers import AutoProcessor, VipLlavaForConditionalGeneration
  348  

train_real_world/transformers_4573/src/transformers/models/vipllava/modular_vipllava.py:
   20  
   21: from transformers.models.llava.modeling_llava import (
   22      LlavaCausalLMOutputWithPast,

  214          >>> import requests
  215:         >>> from transformers import AutoProcessor, VipLlavaForConditionalGeneration
  216  

train_real_world/transformers_4573/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py:
  45      ```python
  46:     >>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel
  47  

train_real_world/transformers_4573/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:
   33  
   34: # Copied from transformers.models.encoder_decoder.modeling_encoder_decoder.shift_tokens_right
   35  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

  205          ```python
  206:         >>> from transformers import VisionEncoderDecoderModel
  207  

  347          ```python
  348:         >>> from transformers import AutoProcessor, VisionEncoderDecoderModel
  349          >>> import requests

train_real_world/transformers_4573/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py:
  53      ```python
  54:     >>> from transformers import ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel
  55  

train_real_world/transformers_4573/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:
   33  
   34: # Copied from transformers.models.clip.modeling_clip.contrastive_loss
   35  def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:

   38  
   39: # Copied from transformers.models.clip.modeling_clip.clip_loss
   40  def clip_loss(similarity: torch.Tensor) -> torch.Tensor:

  124          >>> import torch
  125:         >>> from transformers import VisionTextDualEncoderModel, AutoTokenizer
  126  

  155          >>> import torch
  156:         >>> from transformers import VisionTextDualEncoderModel, AutoImageProcessor
  157:         >>> from transformers.image_utils import load_image
  158  

  197          >>> import requests
  198:         >>> from transformers import (
  199          ...     VisionTextDualEncoderModel,

  333          ```python
  334:         >>> from transformers import VisionTextDualEncoderModel
  335  

train_real_world/transformers_4573/src/transformers/models/visual_bert/configuration_visual_bert.py:
  79      ```python
  80:     >>> from transformers import VisualBertConfig, VisualBertModel
  81  

train_real_world/transformers_4573/src/transformers/models/visual_bert/convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py:
  22  
  23: from transformers import (
  24      VisualBertConfig,

  29  )
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/visual_bert/modeling_visual_bert.py:
   241  
   242: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->VisualBert
   243  class VisualBertSelfOutput(nn.Module):

   278  
   279: # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->VisualBert
   280  class VisualBertIntermediate(nn.Module):

   294  
   295: # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->VisualBert
   296  class VisualBertOutput(nn.Module):

   392  
   393: # Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->VisualBert
   394  class VisualBertPooler(nn.Module):

   408  
   409: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->VisualBert
   410  class VisualBertPredictionHeadTransform(nn.Module):

   426  
   427: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->VisualBert
   428  class VisualBertLMPredictionHead(nn.Module):

   443  
   444: # Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->VisualBert
   445  class VisualBertPreTrainingHeads(nn.Module):

   578          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image.
   579:         from transformers import AutoTokenizer, VisualBertModel
   580          import torch

   774          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
   775:         from transformers import AutoTokenizer, VisualBertForPreTraining
   776  

   935          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
   936:         from transformers import AutoTokenizer, VisualBertForMultipleChoice
   937          import torch

  1097          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
  1098:         from transformers import AutoTokenizer, VisualBertForQuestionAnswering
  1099          import torch

  1234          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
  1235:         from transformers import AutoTokenizer, VisualBertForVisualReasoning
  1236          import torch

  1410          # Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
  1411:         from transformers import AutoTokenizer, VisualBertForRegionToPhraseAlignment
  1412          import torch

train_real_world/transformers_4573/src/transformers/models/vit/configuration_vit.py:
  72      ```python
  73:     >>> from transformers import ViTConfig, ViTModel
  74  

train_real_world/transformers_4573/src/transformers/models/vit/convert_dino_to_pytorch.py:
  25  
  26: from transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor, ViTModel
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/vit/convert_vit_timm_to_pytorch.py:
  25  
  26: from transformers import DeiTImageProcessor, ViTConfig, ViTForImageClassification, ViTImageProcessor, ViTModel
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/vit/modeling_vit.py:
  170  
  171: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  172  def eager_attention_forward(

  504          ```python
  505:         >>> from transformers import AutoImageProcessor, ViTForMaskedImageModeling
  506          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vit_mae/configuration_vit_mae.py:
  79      ```python
  80:     >>> from transformers import ViTMAEConfig, ViTMAEModel
  81  

train_real_world/transformers_4573/src/transformers/models/vit_mae/convert_vit_mae_to_pytorch.py:
  22  
  23: from transformers import ViTMAEConfig, ViTMAEForPreTraining, ViTMAEImageProcessor
  24  

train_real_world/transformers_4573/src/transformers/models/vit_mae/modeling_vit_mae.py:
  200  
  201:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
  202      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

  331  
  332: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  333  def eager_attention_forward(

  361  
  362: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention ViT->ViTMAE
  363  class ViTMAESelfAttention(nn.Module):

  412  
  413: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->ViTMAE
  414  class ViTMAESelfOutput(nn.Module):

  430  
  431: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->ViTMAE
  432  class ViTMAEAttention(nn.Module):

  443  
  444: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate ViT->ViTMAE
  445  class ViTMAEIntermediate(nn.Module):

  459  
  460: # Copied from transformers.models.vit.modeling_vit.ViTOutput ViT->ViTMAE
  461  class ViTMAEOutput(nn.Module):

  473  
  474: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMAE,VIT->VITMAE
  475  class ViTMAELayer(GradientCheckpointingLayer):

  504  
  505: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->ViTMAE
  506  class ViTMAEEncoder(nn.Module):

  588          ```python
  589:         >>> from transformers import AutoImageProcessor, ViTMAEModel
  590          >>> from PIL import Image

  880          ```python
  881:         >>> from transformers import AutoImageProcessor, ViTMAEForPreTraining
  882          >>> from PIL import Image

train_real_world/transformers_4573/src/transformers/models/vit_msn/configuration_vit_msn.py:
  66      ```python
  67:     >>> from transformers import ViTMSNModel, ViTMSNConfig
  68  

train_real_world/transformers_4573/src/transformers/models/vit_msn/convert_msn_to_pytorch.py:
  24  
  25: from transformers import ViTImageProcessor, ViTMSNConfig, ViTMSNModel
  26: from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
  27  

train_real_world/transformers_4573/src/transformers/models/vit_msn/modeling_vit_msn.py:
   54  
   55:     # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
   56      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

  126  
  127: # Copied from transformers.models.vit.modeling_vit.ViTPatchEmbeddings with ViT->ViTMSN
  128  class ViTMSNPatchEmbeddings(nn.Module):

  166  
  167: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  168  def eager_attention_forward(

  196  
  197: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->ViTMSN
  198  class ViTMSNSelfAttention(nn.Module):

  247  
  248: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->ViTMSN
  249  class ViTMSNSelfOutput(nn.Module):

  265  
  266: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->ViTMSN
  267  class ViTMSNAttention(nn.Module):

  278  
  279: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->ViTMSN
  280  class ViTMSNIntermediate(nn.Module):

  294  
  295: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViT->ViTMSN
  296  class ViTMSNOutput(nn.Module):

  308  
  309: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMSN, VIT->VITMSN
  310  class ViTMSNLayer(GradientCheckpointingLayer):

  339  
  340: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->ViTMSN
  341  class ViTMSNEncoder(nn.Module):

  427          ```python
  428:         >>> from transformers import AutoImageProcessor, ViTMSNModel
  429          >>> import torch

  485          ```python
  486:         >>> from transformers import AutoImageProcessor, ViTMSNForImageClassification
  487          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vitdet/configuration_vitdet.py:
  88      ```python
  89:     >>> from transformers import VitDetConfig, VitDetModel
  90  

train_real_world/transformers_4573/src/transformers/models/vitdet/modeling_vitdet.py:
  264  
  265: # Copied from transformers.models.beit.modeling_beit.drop_path
  266  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

  280  
  281: # Copied from transformers.models.beit.modeling_beit.BeitDropPath
  282  class VitDetDropPath(nn.Module):

  638          ```python
  639:         >>> from transformers import VitDetConfig, VitDetModel
  640          >>> import torch

  715          ```python
  716:         >>> from transformers import VitDetConfig, VitDetBackbone
  717          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vitmatte/configuration_vitmatte.py:
  66      ```python
  67:     >>> from transformers import VitMatteConfig, VitMatteForImageMatting
  68  

train_real_world/transformers_4573/src/transformers/models/vitmatte/convert_vitmatte_to_hf.py:
  26  
  27: from transformers import VitDetConfig, VitMatteConfig, VitMatteForImageMatting, VitMatteImageProcessor
  28  

train_real_world/transformers_4573/src/transformers/models/vitmatte/modeling_vitmatte.py:
  249          ```python
  250:         >>> from transformers import VitMatteImageProcessor, VitMatteForImageMatting
  251          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vitpose/configuration_vitpose.py:
  63      ```python
  64:     >>> from transformers import VitPoseConfig, VitPoseForPoseEstimation
  65  

train_real_world/transformers_4573/src/transformers/models/vitpose/convert_vitpose_to_hf.py:
  31  
  32: from transformers import VitPoseBackboneConfig, VitPoseConfig, VitPoseForPoseEstimation, VitPoseImageProcessor
  33  

train_real_world/transformers_4573/src/transformers/models/vitpose/modeling_vitpose.py:
  231          ```python
  232:         >>> from transformers import AutoImageProcessor, VitPoseForPoseEstimation
  233          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vitpose_backbone/configuration_vitpose_backbone.py:
  80      ```python
  81:     >>> from transformers import VitPoseBackboneConfig, VitPoseBackbone
  82  

train_real_world/transformers_4573/src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py:
   99  
  100: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  101  def eager_attention_forward(

  129  
  130: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->VitPoseBackbone
  131  class VitPoseBackboneSelfAttention(nn.Module):

  180  
  181: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->VitPoseBackbone
  182  class VitPoseBackboneSelfOutput(nn.Module):

  198  
  199: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->VitPoseBackbone
  200  class VitPoseBackboneAttention(nn.Module):

  319  
  320: # Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->VitPoseBackbone
  321  class VitPoseBackboneEncoder(nn.Module):

  411          ```python
  412:         >>> from transformers import VitPoseBackboneConfig, VitPoseBackbone
  413          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vits/configuration_vits.py:
  136      ```python
  137:     >>> from transformers import VitsModel, VitsConfig
  138  

train_real_world/transformers_4573/src/transformers/models/vits/convert_original_checkpoint.py:
  23  
  24: from transformers import VitsConfig, VitsModel, VitsTokenizer, logging
  25  

train_real_world/transformers_4573/src/transformers/models/vits/modeling_vits.py:
   401  
   402: # Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock
   403  class HifiGanResidualBlock(nn.Module):

  1289          ```python
  1290:         >>> from transformers import VitsTokenizer, VitsModel, set_seed
  1291          >>> import torch

train_real_world/transformers_4573/src/transformers/models/vivit/configuration_vivit.py:
  67      ```python
  68:     >>> from transformers import VivitConfig, VivitModel
  69  

train_real_world/transformers_4573/src/transformers/models/vivit/convert_vivit_flax_to_pytorch.py:
  29  
  30: from transformers import VivitConfig, VivitForVideoClassification, VivitImageProcessor
  31: from transformers.image_utils import PILImageResampling
  32  

train_real_world/transformers_4573/src/transformers/models/vivit/image_processing_vivit.py:
   20  
   21: from transformers.utils import is_vision_available
   22: from transformers.utils.generic import TensorType
   23  

  185  
  186:     # Copied from transformers.models.efficientnet.image_processing_efficientnet.EfficientNetImageProcessor.rescale
  187      def rescale(

train_real_world/transformers_4573/src/transformers/models/vivit/modeling_vivit.py:
  100  
  101:     # Adapted from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
  102      def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:

  159  
  160: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  161  def eager_attention_forward(

  189  
  190: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Vivit
  191  class VivitSelfAttention(nn.Module):

  240  
  241: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Vivit
  242  class VivitSelfOutput(nn.Module):

  258  
  259: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->Vivit
  260  class VivitAttention(nn.Module):

  424  
  425:         >>> from transformers import VivitImageProcessor, VivitModel
  426          >>> from huggingface_hub import hf_hub_download

  553  
  554:         >>> from transformers import VivitImageProcessor, VivitForVideoClassification
  555          >>> from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/vjepa2/configuration_vjepa2.py:
  82      ```python
  83:     >>> from transformers import VJEPA2Config, VJEPA2Model
  84  

train_real_world/transformers_4573/src/transformers/models/vjepa2/convert_vjepa2_classifier_to_hf.py:
  25  
  26: from transformers import VJEPA2ForVideoClassification, VJEPA2VideoProcessor
  27  

train_real_world/transformers_4573/src/transformers/models/vjepa2/convert_vjepa2_to_hf.py:
  26  
  27: from transformers import VJEPA2Config, VJEPA2Model, VJEPA2VideoProcessor
  28: from transformers.models.vjepa2.modeling_vjepa2 import apply_masks
  29  

train_real_world/transformers_4573/src/transformers/models/vjepa2/modeling_vjepa2.py:
   151  
   152: # Adapted from transformers.models.vit.modeling_vit.eager_attention_forward
   153  def eager_attention_forward(

   344  
   345: # Adapted from transformers.models.beit.modeling_dinov2.drop_path
   346  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   360  
   361: # Adapted from transformers.models.beit.modeling_beit.BeitDropPath
   362  class VJEPA2DropPath(nn.Module):

  1103          >>> import numpy as np
  1104:         >>> from transformers import AutoVideoProcessor, VJEPA2ForVideoClassification
  1105  

train_real_world/transformers_4573/src/transformers/models/voxtral/configuration_voxtral.py:
   57      ```python
   58:     >>> from transformers import VoxtralEncoderConfig, VoxtralEncoder
   59  

  140      ```python
  141:     >>> from transformers import VoxtralForConditionalGeneration, VoxtralConfig
  142  

train_real_world/transformers_4573/src/transformers/models/voxtral/convert_voxtral_weights_to_hf.py:
  24  
  25: from transformers import (
  26      MistralCommonBackend,

  31  )
  32: from transformers.models.whisper.modeling_whisper import sinusoids
  33: from transformers.utils.hub import cached_file
  34  

train_real_world/transformers_4573/src/transformers/models/voxtral/modeling_voxtral.py:
  449          ```python
  450:         >>> from transformers import VoxtralForConditionalGeneration, AutoProcessor
  451          >>> import torch

train_real_world/transformers_4573/src/transformers/models/voxtral/modular_voxtral.py:
  211          ```python
  212:         >>> from transformers import VoxtralForConditionalGeneration, AutoProcessor
  213          >>> import torch

train_real_world/transformers_4573/src/transformers/models/voxtral/processing_voxtral.py:
  121          from huggingface_hub import hf_hub_download
  122:         from transformers.audio_utils import load_audio_as
  123  

  287          ```python
  288:         from transformers import VoxtralProcessor
  289  

train_real_world/transformers_4573/src/transformers/models/wav2vec2/configuration_wav2vec2.py:
  189      ```python
  190:     >>> from transformers import Wav2Vec2Config, Wav2Vec2Model
  191  

train_real_world/transformers_4573/src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import (
  26      Wav2Vec2Config,

  33  )
  34: from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2ForSequenceClassification
  35  

train_real_world/transformers_4573/src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py:
  20  
  21: from transformers import (
  22      Wav2Vec2Config,

train_real_world/transformers_4573/src/transformers/models/wav2vec2/modeling_wav2vec2.py:
   441  
   442: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   443  def eager_attention_forward(

  1132          ```python
  1133:         >>> from transformers import Wav2Vec2ForCTC, AutoProcessor
  1134  

  1470          >>> import torch
  1471:         >>> from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining
  1472:         >>> from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
  1473          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/wav2vec2/tokenization_wav2vec2.py:
  623          >>> # Let's see how to retrieve time steps for a model
  624:         >>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
  625          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/wav2vec2_bert/configuration_wav2vec2_bert.py:
  167      ```python
  168:     >>> from transformers import Wav2Vec2BertConfig, Wav2Vec2BertModel
  169  

train_real_world/transformers_4573/src/transformers/models/wav2vec2_bert/convert_wav2vec2_seamless_checkpoint.py:
  25  
  26: from transformers import (
  27      SeamlessM4TFeatureExtractor,

train_real_world/transformers_4573/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py:
  591  
  592: # Copied from transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2._compute_new_attention_mask
  593  def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):

train_real_world/transformers_4573/src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py:
  41  
  42: # Copied from transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2._compute_new_attention_mask
  43  def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):

train_real_world/transformers_4573/src/transformers/models/wav2vec2_conformer/configuration_wav2vec2_conformer.py:
  194      ```python
  195:     >>> from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel
  196  

train_real_world/transformers_4573/src/transformers/models/wav2vec2_conformer/convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py:
   24  
   25: from transformers import (
   26      Wav2Vec2ConformerConfig,

  173  
  174: # Copied from transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.load_conv_layer
  175  def load_conv_layer(full_name, value, feature_extractor, unused_weights, use_group_norm):

train_real_world/transformers_4573/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py:
  1282          >>> import torch
  1283:         >>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForPreTraining
  1284:         >>> from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import _compute_mask_indices, _sample_negative_indices
  1285          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:
  525          >>> # Let's see how to retrieve time steps for a model
  526:         >>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
  527          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/wavlm/configuration_wavlm.py:
  178      ```python
  179:     >>> from transformers import WavLMConfig, WavLMModel
  180  

train_real_world/transformers_4573/src/transformers/models/wavlm/convert_wavlm_original_pytorch_checkpoint_to_pytorch.py:
  28  
  29: from transformers import WavLMConfig, WavLMModel, logging
  30  

train_real_world/transformers_4573/src/transformers/models/wavlm/convert_wavlm_original_s3prl_checkpoint_to_pytorch.py:
  20  
  21: from transformers import (
  22      Wav2Vec2FeatureExtractor,

train_real_world/transformers_4573/src/transformers/models/whisper/configuration_whisper.py:
  168      ```python
  169:     >>> from transformers import WhisperConfig, WhisperModel
  170  

train_real_world/transformers_4573/src/transformers/models/whisper/convert_openai_to_hf.py:
  30  
  31: from transformers import (
  32      GenerationConfig,

  39  )
  40: from transformers.models.whisper.tokenization_whisper import LANGUAGES, bytes_to_unicode
  41: from transformers.utils.import_utils import _is_package_available
  42  

train_real_world/transformers_4573/src/transformers/models/whisper/feature_extraction_whisper.py:
  169      @staticmethod
  170:     # Copied from transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm
  171      def zero_mean_unit_var_norm(

train_real_world/transformers_4573/src/transformers/models/whisper/generation_whisper.py:
   26  
   27: from transformers.cache_utils import EncoderDecoderCache
   28  

  575          >>> import torch
  576:         >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
  577          >>> from datasets import load_dataset, Audio

  632          >>> import torch
  633:         >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
  634          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/whisper/modeling_whisper.py:
    66  
    67: # Copied from transformers.models.bart.modeling_bart.shift_tokens_right
    68  def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):

    83  
    84: # Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices
    85  def _compute_mask_indices(

   364  
   365: # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER
   366  class WhisperEncoderLayer(GradientCheckpointingLayer):

  1017           >>> import torch
  1018:          >>> from transformers import AutoFeatureExtractor, WhisperModel
  1019           >>> from datasets import load_dataset

  1170          >>> import torch
  1171:         >>> from transformers import AutoProcessor, WhisperForConditionalGeneration
  1172          >>> from datasets import load_dataset

  1321          ```python
  1322:         >>> from transformers import WhisperForCausalLM, WhisperForConditionalGeneration, WhisperProcessor
  1323          >>> import torch

  1443          >>> import torch
  1444:         >>> from transformers import AutoFeatureExtractor, WhisperForAudioClassification
  1445          >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/models/whisper/tokenization_whisper.py:
  280  
  281:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._decode_with_timestamps
  282      def _decode_with_timestamps(

  329  
  330:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._compute_offsets
  331      def _compute_offsets(self, token_ids, time_precision=0.02, segment_size=1500):

  399      @lru_cache
  400:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.timestamp_ids
  401      def timestamp_ids(self, time_precision=0.02):

  410  
  411:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._preprocess_token_ids
  412      def _preprocess_token_ids(self, token_ids, skip_special_tokens: bool = False):

  429  
  430:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._filter_timestamp_ids
  431      def _filter_timestamp_ids(self, text):

  433  
  434:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.decode
  435      def decode(

  530  
  531:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._normalize
  532      def _normalize(self, text):

  538  
  539:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._basic_normalize
  540      def _basic_normalize(self, text, remove_diacritics=False):

  546  
  547:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.normalize
  548      def normalize(self, text):

  556      @staticmethod
  557:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.basic_normalize
  558      def basic_normalize(text, remove_diacritics=False):

  636      @property
  637:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.prefix_tokens
  638      def prefix_tokens(self) -> list[int]:

  670  
  671:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.build_inputs_with_special_tokens
  672      def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None) -> list[int]:

  678  
  679:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.get_special_tokens_mask
  680      def get_special_tokens_mask(

  709  
  710:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.get_decoder_prompt_ids
  711      def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):

  729  
  730:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.get_prompt_ids
  731      def get_prompt_ids(self, text: str, return_tensors="np"):

  744  
  745:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._strip_prompt
  746      def _strip_prompt(self, token_ids: list[int], prompt_token_id: int, decoder_start_token_id: int):

  764      @staticmethod
  765:     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._convert_to_list
  766      def _convert_to_list(token_ids):

train_real_world/transformers_4573/src/transformers/models/x_clip/configuration_x_clip.py:
   65      ```python
   66:     >>> from transformers import XCLIPTextModel, XCLIPTextConfig
   67  

  164      ```python
  165:     >>> from transformers import XCLIPVisionModel, XCLIPVisionConfig
  166  

train_real_world/transformers_4573/src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py:
  22  
  23: from transformers import (
  24      CLIPTokenizer,

train_real_world/transformers_4573/src/transformers/models/x_clip/modeling_x_clip.py:
    50  
    51: # Copied from transformers.models.clip.modeling_clip.clip_loss with clip->x_clip
    52  def x_clip_loss(similarity: torch.Tensor) -> torch.Tensor:

   100  
   101: # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->XCLIP
   102  class XCLIPVisionEmbeddings(nn.Module):

   184  
   185: # Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->XCLIP
   186  class XCLIPTextEmbeddings(nn.Module):

   225  
   226: # Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward
   227  def eager_attention_forward(

   338  
   339: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->XCLIP
   340  class XCLIPEncoderLayer(GradientCheckpointingLayer):

   389  
   390: # Copied from transformers.models.beit.modeling_beit.drop_path
   391  def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:

   405  
   406: # Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->XCLIP
   407  class XCLIPDropPath(nn.Module):

   558  
   559: # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->XCLIP
   560  class XCLIPEncoder(nn.Module):

   747          ```python
   748:         >>> from transformers import AutoTokenizer, XCLIPTextModel
   749  

   942  
   943:         >>> from transformers import AutoProcessor, XCLIPVisionModel
   944          >>> from huggingface_hub import hf_hub_download

  1221          >>> import torch
  1222:         >>> from transformers import AutoTokenizer, AutoModel
  1223  

  1257  
  1258:         >>> from transformers import AutoProcessor, AutoModel
  1259          >>> from huggingface_hub import hf_hub_download

  1359  
  1360:         >>> from transformers import AutoProcessor, AutoModel
  1361          >>> from huggingface_hub import hf_hub_download

train_real_world/transformers_4573/src/transformers/models/xcodec/configuration_xcodec.py:
  21  
  22: from transformers import AutoConfig, DacConfig, HubertConfig, WavLMConfig
  23  

  69      ```python
  70:     >>> from transformers import XcodecModel, XcodecConfig
  71  

train_real_world/transformers_4573/src/transformers/models/xcodec/convert_xcodec_weights_to_hf.py:
  21  
  22: from transformers import (
  23      AutoConfig,

train_real_world/transformers_4573/src/transformers/models/xcodec/modeling_xcodec.py:
  228  
  229:     # Copied from transformers.models.encodec.modeling_encodec.EncodecEuclideanCodebook.quantize
  230      def quantize(self, hidden_states):

  257  
  258:     # Copied from transformers.models.encodec.modeling_encodec.EncodecVectorQuantization.encode
  259      def encode(self, hidden_states):

  263  
  264:     # Copied from transformers.models.encodec.modeling_encodec.EncodecVectorQuantization.decode
  265      def decode(self, embed_ind):

  594          >>> from datasets import load_dataset
  595:         >>> from transformers import AutoFeatureExtractor, XcodecModel
  596  

train_real_world/transformers_4573/src/transformers/models/xglm/configuration_xglm.py:
  71      ```python
  72:     >>> from transformers import XGLMModel, XGLMConfig
  73  

train_real_world/transformers_4573/src/transformers/models/xglm/convert_xglm_original_ckpt_to_trfms.py:
  6  
  7: from transformers import XGLMConfig, XGLMForCausalLM
  8  

train_real_world/transformers_4573/src/transformers/models/xglm/modeling_xglm.py:
   37  
   38: # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->XGLM
   39  class XGLMScaledWordEmbedding(nn.Embedding):

  284  
  285:     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward
  286      def forward(

train_real_world/transformers_4573/src/transformers/models/xlm/configuration_xlm.py:
  123      ```python
  124:     >>> from transformers import XLMConfig, XLMModel
  125  

train_real_world/transformers_4573/src/transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py:
  22  
  23: from transformers.models.xlm.tokenization_xlm import VOCAB_FILES_NAMES
  24: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, logging
  25  

train_real_world/transformers_4573/src/transformers/models/xlm/modeling_xlm.py:
  1324          ```python
  1325:         >>> from transformers import AutoTokenizer, XLMForQuestionAnswering
  1326          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlm_roberta/configuration_xlm_roberta.py:
  74      ```python
  75:     >>> from transformers import XLMRobertaConfig, XLMRobertaModel
  76  

train_real_world/transformers_4573/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:
  783          ```python
  784:         >>> from transformers import AutoTokenizer, XLMRobertaForCausalLM, AutoConfig
  785          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlm_roberta/modular_xlm_roberta.py:
  107          ```python
  108:         >>> from transformers import AutoTokenizer, XLMRobertaForCausalLM, AutoConfig
  109          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py:
  71      ```python
  72:     >>> from transformers import XLMRobertaXLConfig, XLMRobertaXLModel
  73  

train_real_world/transformers_4573/src/transformers/models/xlm_roberta_xl/convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py:
  25  
  26: from transformers import XLMRobertaConfig, XLMRobertaXLForMaskedLM, XLMRobertaXLForSequenceClassification
  27: from transformers.models.bert.modeling_bert import (
  28      BertIntermediate,

  33  )
  34: from transformers.models.roberta.modeling_roberta import RobertaAttention
  35: from transformers.utils import logging
  36  

train_real_world/transformers_4573/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py:
  816          ```python
  817:         >>> from transformers import AutoTokenizer, RobertaForCausalLM, RobertaConfig
  818          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py:
  318          ```python
  319:         >>> from transformers import AutoTokenizer, RobertaForCausalLM, RobertaConfig
  320          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlnet/configuration_xlnet.py:
  121      ```python
  122:     >>> from transformers import XLNetConfig, XLNetModel
  123  

train_real_world/transformers_4573/src/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py:
  21  
  22: from transformers import (
  23      XLNetConfig,

  27  )
  28: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, logging
  29  

train_real_world/transformers_4573/src/transformers/models/xlnet/modeling_xlnet.py:
   358  
   359: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerStartLogits with XLM->XLNet
   360  class XLNetPoolerStartLogits(nn.Module):

   397  
   398: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerEndLogits with XLM->XLNet
   399  class XLNetPoolerEndLogits(nn.Module):

   467  
   468: # Copied from transformers.models.xlm.modeling_xlm.XLMPoolerAnswerClass with XLM->XLNet
   469  class XLNetPoolerAnswerClass(nn.Module):

   533  
   534: # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->XLNet
   535  class XLNetSequenceSummary(nn.Module):

  1367          ```python
  1368:         >>> from transformers import AutoTokenizer, XLNetLMHeadModel
  1369          >>> import torch

  2057          ```python
  2058:         >>> from transformers import AutoTokenizer, XLNetForQuestionAnswering
  2059          >>> import torch

train_real_world/transformers_4573/src/transformers/models/xlstm/configuration_xlstm.py:
  139      ```python
  140:     >>> from transformers import xLSTMConfig, xLSTMModel
  141  

train_real_world/transformers_4573/src/transformers/models/xlstm/modeling_xlstm.py:
  1327          ```python
  1328:         >>> from transformers import AutoTokenizer, xLSTMForCausalLM, xLSTMCache
  1329  

train_real_world/transformers_4573/src/transformers/models/xmod/configuration_xmod.py:
  89      ```python
  90:     >>> from transformers import XmodConfig, XmodModel
  91  

train_real_world/transformers_4573/src/transformers/models/xmod/convert_xmod_original_pytorch_checkpoint_to_pytorch.py:
  24  
  25: from transformers import XmodConfig, XmodForMaskedLM, XmodForSequenceClassification
  26: from transformers.utils import logging
  27  

train_real_world/transformers_4573/src/transformers/models/xmod/modeling_xmod.py:
    50  
    51: # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Xmod
    52  class XmodEmbeddings(nn.Module):

   157  
   158: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
   159  def eager_attention_forward(

   187  
   188: # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Xmod
   189  class XmodSelfAttention(nn.Module):

   261  
   262: # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->Xmod
   263  class XmodCrossAttention(nn.Module):

   340  class XmodSelfOutput(nn.Module):
   341:     # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput.__init__
   342      def __init__(self, config):

   395  
   396: # Copied from transformers.models.roberta.modeling_roberta.RobertaIntermediate
   397  class XmodIntermediate(nn.Module):

   598  
   599: # Copied from transformers.models.roberta.modeling_roberta.RobertaPooler
   600  class XmodPooler(nn.Module):

   702  
   703:     # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.get_input_embeddings
   704      def get_input_embeddings(self):

   706  
   707:     # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.set_input_embeddings
   708      def set_input_embeddings(self, value):

   806  
   807:     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks
   808      def _create_attention_masks(

   853  
   854:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM.__init__ with Roberta->Xmod
   855      def __init__(self, config):

   866  
   867:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM.get_output_embeddings
   868      def get_output_embeddings(self):

   870  
   871:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM.set_output_embeddings
   872      def set_output_embeddings(self, new_embeddings):

   905          ```python
   906:         >>> from transformers import AutoTokenizer, XmodForCausalLM, AutoConfig
   907          >>> import torch

   964  
   965:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.__init__ with Roberta->Xmod
   966      def __init__(self, config):

   980  
   981:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.get_output_embeddings
   982      def get_output_embeddings(self):

   984  
   985:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.set_output_embeddings
   986      def set_output_embeddings(self, new_embeddings):

  1040  
  1041: # Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead
  1042  class XmodLMHead(nn.Module):

  1070  class XmodForSequenceClassification(XmodPreTrainedModel):
  1071:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.__init__ with Roberta->Xmod
  1072      def __init__(self, config):

  1150  class XmodForMultipleChoice(XmodPreTrainedModel):
  1151:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice.__init__ with Roberta->Xmod
  1152      def __init__(self, config):

  1251  class XmodForTokenClassification(XmodPreTrainedModel):
  1252:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.__init__ with Roberta->Xmod
  1253      def __init__(self, config):

  1315  
  1316: # Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead
  1317  class XmodClassificationHead(nn.Module):

  1340  class XmodForQuestionAnswering(XmodPreTrainedModel):
  1341:     # Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.__init__ with Roberta->Xmod
  1342      def __init__(self, config):

train_real_world/transformers_4573/src/transformers/models/yolos/configuration_yolos.py:
  83      ```python
  84:     >>> from transformers import YolosConfig, YolosModel
  85  

train_real_world/transformers_4573/src/transformers/models/yolos/convert_yolos_to_pytorch.py:
  25  
  26: from transformers import YolosConfig, YolosForObjectDetection, YolosImageProcessor
  27: from transformers.utils import logging
  28  

train_real_world/transformers_4573/src/transformers/models/yolos/image_processing_yolos.py:
   106  
   107: # Copied from transformers.models.detr.image_processing_detr.get_max_height_width
   108  def get_max_height_width(

   174  
   175: # Copied from transformers.models.detr.image_processing_detr.get_image_size_for_max_height_width
   176  def get_image_size_for_max_height_width(

   210  
   211: # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size
   212  def get_resize_output_image_size(

   239  
   240: # Copied from transformers.models.detr.image_processing_detr.safe_squeeze
   241  def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:

   253  
   254: # Copied from transformers.models.detr.image_processing_detr.normalize_annotation
   255  def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:

   268  
   269: # Copied from transformers.models.detr.image_processing_detr.max_across_indices
   270  def max_across_indices(values: Iterable[Any]) -> list[Any]:

   276  
   277: # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask
   278  def make_pixel_mask(

   295  
   296: # Copied from transformers.models.detr.image_processing_detr.convert_coco_poly_to_mask
   297  def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndarray:

   330  
   331: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_detection_annotation
   332  def prepare_coco_detection_annotation(

   391  
   392: # Copied from transformers.models.detr.image_processing_detr.masks_to_boxes
   393  def masks_to_boxes(masks: np.ndarray) -> np.ndarray:

   426  
   427: # Copied from transformers.models.detr.image_processing_detr.prepare_coco_panoptic_annotation with DETR->YOLOS
   428  def prepare_coco_panoptic_annotation(

   468  
   469: # Copied from transformers.models.detr.image_processing_detr.get_segmentation_image
   470  def get_segmentation_image(

   494  
   495: # Copied from transformers.models.detr.image_processing_detr.get_mask_area
   496  def get_mask_area(seg_img: np.ndarray, target_size: tuple[int, int], n_classes: int) -> np.ndarray:

   504  
   505: # Copied from transformers.models.detr.image_processing_detr.score_labels_from_class_probabilities
   506  def score_labels_from_class_probabilities(logits: np.ndarray) -> tuple[np.ndarray, np.ndarray]:

   513  
   514: # Copied from transformers.models.detr.image_processing_detr.resize_annotation
   515  def resize_annotation(

   565  
   566: # Copied from transformers.models.detr.image_processing_detr.binary_mask_to_rle
   567  def binary_mask_to_rle(mask):

   588  
   589: # Copied from transformers.models.detr.image_processing_detr.convert_segmentation_to_rle
   590  def convert_segmentation_to_rle(segmentation):

   610  
   611: # Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects
   612  def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):

   639  
   640: # Copied from transformers.models.detr.image_processing_detr.check_segment_validity
   641  def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):

   658  
   659: # Copied from transformers.models.detr.image_processing_detr.compute_segments
   660  def compute_segments(

   831  
   832:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation
   833      def prepare_annotation(

   864  
   865:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize
   866      def resize(

   925  
   926:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.resize_annotation
   927      def resize_annotation(

   939  
   940:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.rescale
   941      def rescale(

   968  
   969:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation
   970      def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:

   976  
   977:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image
   978      def _update_annotation_for_padded_image(

  1020  
  1021:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._pad_image
  1022      def _pad_image(

  1381  
  1382:     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection with Detr->Yolos
  1383      def post_process_object_detection(

train_real_world/transformers_4573/src/transformers/models/yolos/modeling_yolos.py:
  213  
  214: # Copied from transformers.models.bert.modeling_bert.eager_attention_forward
  215  def eager_attention_forward(

  243  
  244: # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Yolos
  245  class YolosSelfAttention(nn.Module):

  294  
  295: # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Yolos
  296  class YolosSelfOutput(nn.Module):

  312  
  313: # Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->Yolos
  314  class YolosAttention(nn.Module):

  325  
  326: # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->Yolos
  327  class YolosIntermediate(nn.Module):

  341  
  342: # Copied from transformers.models.vit.modeling_vit.ViTOutput with ViT->Yolos
  343  class YolosOutput(nn.Module):

  355  
  356: # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->Yolos,VIT->YOLOS
  357  class YolosLayer(GradientCheckpointingLayer):

  506  
  507: # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->Yolos
  508  class YolosMLPPredictionHead(nn.Module):

  575          ```python
  576:         >>> from transformers import AutoImageProcessor, AutoModelForObjectDetection
  577          >>> import torch

train_real_world/transformers_4573/src/transformers/models/yolos/modular_yolos.py:
  4  
  5: from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast
  6  

train_real_world/transformers_4573/src/transformers/models/yoso/configuration_yoso.py:
  78      ```python
  79:     >>> from transformers import YosoConfig, YosoModel
  80  

train_real_world/transformers_4573/src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py:
  20  
  21: from transformers import YosoConfig, YosoForMaskedLM
  22  

train_real_world/transformers_4573/src/transformers/models/yoso/modeling_yoso.py:
  225  
  226: # Copied from transformers.models.nystromformer.modeling_nystromformer.NystromformerEmbeddings
  227  class YosoEmbeddings(nn.Module):

  427  
  428: # Copied from transformers.models.bert.modeling_bert.BertSelfOutput
  429  class YosoSelfOutput(nn.Module):

  455  
  456: # Copied from transformers.models.bert.modeling_bert.BertIntermediate
  457  class YosoIntermediate(nn.Module):

  471  
  472: # Copied from transformers.models.bert.modeling_bert.BertOutput
  473  class YosoOutput(nn.Module):

  555  
  556: # Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform
  557  class YosoPredictionHeadTransform(nn.Module):

  573  
  574: # Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Yoso
  575  class YosoLMPredictionHead(nn.Module):

  590  
  591: # Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Yoso
  592  class YosoOnlyMLMHead(nn.Module):

train_real_world/transformers_4573/src/transformers/models/zamba/modeling_zamba.py:
    63  
    64: # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Zamba
    65  class ZambaRMSNorm(nn.Module):

    84  
    85: # Copied from transformers.models.llama.modeling_llama.repeat_kv
    86  def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

   148  
   149:     # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.update
   150      def update(

   166  
   167:     # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.reorder_cache
   168      def reorder_cache(self, beam_idx: torch.LongTensor):

   181  
   182:     # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.get_seq_length
   183      def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:

   222  
   223:     Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:
   224      The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.

   563  
   564: # Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Zamba
   565  class ZambaMLP(nn.Module):

   997  
   998: # Adapted from transformers.models.jamba.modeling_jamba.JambaForCausalLM with Jamba->Zamba, JAMBA->ZAMBA
   999  class ZambaForCausalLM(ZambaPreTrainedModel, GenerationMixin):

  1036          ```python
  1037:         >>> from transformers import AutoTokenizer, ZambaForCausalLM
  1038  

train_real_world/transformers_4573/src/transformers/models/zamba2/configuration_zamba2.py:
  122      ```python
  123:     >>> from transformers import Zamba2Model, Zamba2Config
  124      >>> # Initializing a Zamba2-2.7B style configuration

train_real_world/transformers_4573/src/transformers/models/zamba2/modeling_zamba2.py:
   352  
   353:     Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:
   354      The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.

  1443  
  1444: # Adapted from transformers.models.jamba.modeling_jamba.JambaForCausalLM with Jamba->Zamba2, JAMBA->ZAMBA2
  1445  class Zamba2ForCausalLM(Zamba2PreTrainedModel, GenerationMixin):

  1482          ```python
  1483:         >>> from transformers import AutoTokenizer, Zamba2ForCausalLM
  1484  

train_real_world/transformers_4573/src/transformers/models/zamba2/modular_zamba2.py:
  174  
  175:     Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:
  176      The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.

train_real_world/transformers_4573/src/transformers/models/zoedepth/configuration_zoedepth.py:
  122      ```python
  123:     >>> from transformers import ZoeDepthConfig, ZoeDepthForDepthEstimation
  124  

train_real_world/transformers_4573/src/transformers/models/zoedepth/convert_zoedepth_to_hf.py:
  28  
  29: from transformers import BeitConfig, ZoeDepthConfig, ZoeDepthForDepthEstimation, ZoeDepthImageProcessor
  30: from transformers.utils import logging
  31  

train_real_world/transformers_4573/src/transformers/models/zoedepth/modeling_zoedepth.py:
   146  
   147:     # Copied from transformers.models.dpt.modeling_dpt.DPTReassembleLayer.forward with DPT->ZoeDepth
   148      def forward(self, hidden_state):

   153  
   154: # Copied from transformers.models.dpt.modeling_dpt.DPTFeatureFusionStage with DPT->ZoeDepth
   155  class ZoeDepthFeatureFusionStage(nn.Module):

   178  
   179: # Copied from transformers.models.dpt.modeling_dpt.DPTPreActResidualLayer with DPT->ZoeDepth
   180  class ZoeDepthPreActResidualLayer(nn.Module):

   241  
   242: # Copied from transformers.models.dpt.modeling_dpt.DPTFeatureFusionLayer with DPT->ZoeDepth
   243  class ZoeDepthFeatureFusionLayer(nn.Module):

   291  
   292:     # Copied from transformers.models.dpt.modeling_dpt.DPTNeck.__init__ with DPT->ZoeDepth
   293      def __init__(self, config: ZoeDepthConfig):

   776  
   777: # Copied from transformers.models.grounding_dino.modeling_grounding_dino.GroundingDinoMultiheadAttention with GroundingDino->ZoeDepth
   778  class ZoeDepthMultiheadAttention(nn.Module):

  1204  
  1205: # Modified from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->ZoeDepth,dpt->zoedepth
  1206  # avoiding sdpa and flash_attn_2 support, it's done int the backend

  1268          ```python
  1269:         >>> from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation
  1270          >>> import torch

train_real_world/transformers_4573/src/transformers/pipelines/__init__.py:
  682      ```python
  683:     >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer
  684  

train_real_world/transformers_4573/src/transformers/pipelines/any_to_any.py:
  81      ```python
  82:     >>> from transformers import pipeline
  83  

  89      ```python
  90:     >>> from transformers import pipeline
  91  

train_real_world/transformers_4573/src/transformers/pipelines/audio_classification.py:
  75      ```python
  76:     >>> from transformers import pipeline
  77  

train_real_world/transformers_4573/src/transformers/pipelines/automatic_speech_recognition.py:
  126      ```python
  127:     >>> from transformers import pipeline
  128  

train_real_world/transformers_4573/src/transformers/pipelines/base.py:
  732  if is_torch_available():
  733:     from transformers.pipelines.pt_utils import (
  734          PipelineChunkIterator,

train_real_world/transformers_4573/src/transformers/pipelines/depth_estimation.py:
  31      ```python
  32:     >>> from transformers import pipeline
  33  

train_real_world/transformers_4573/src/transformers/pipelines/document_question_answering.py:
  116      ```python
  117:     >>> from transformers import pipeline
  118  

train_real_world/transformers_4573/src/transformers/pipelines/feature_extraction.py:
  22      ```python
  23:     >>> from transformers import pipeline
  24  

train_real_world/transformers_4573/src/transformers/pipelines/fill_mask.py:
  40      ```python
  41:     >>> from transformers import pipeline
  42  

  69      ```python
  70:     >>> from transformers import pipeline
  71  

train_real_world/transformers_4573/src/transformers/pipelines/image_classification.py:
  41  
  42: # Copied from transformers.pipelines.text_classification.sigmoid
  43  def sigmoid(_outputs):

  46  
  47: # Copied from transformers.pipelines.text_classification.softmax
  48  def softmax(_outputs):

  53  
  54: # Copied from transformers.pipelines.text_classification.ClassificationFunction
  55  class ClassificationFunction(ExplicitEnum):

  80      ```python
  81:     >>> from transformers import pipeline
  82  

train_real_world/transformers_4573/src/transformers/pipelines/image_feature_extraction.py:
  30      ```python
  31:     >>> from transformers import pipeline
  32  

train_real_world/transformers_4573/src/transformers/pipelines/image_segmentation.py:
  34      ```python
  35:     >>> from transformers import pipeline
  36  

train_real_world/transformers_4573/src/transformers/pipelines/image_text_to_text.py:
  67      ```python
  68:     >>> from transformers import pipeline
  69  

  75      ```python
  76:     >>> from transformers import pipeline
  77  

train_real_world/transformers_4573/src/transformers/pipelines/image_to_image.py:
  51  
  52:     >>> from transformers import pipeline
  53  

train_real_world/transformers_4573/src/transformers/pipelines/image_to_text.py:
  53      ```python
  54:     >>> from transformers import pipeline
  55  

train_real_world/transformers_4573/src/transformers/pipelines/mask_generation.py:
  66      ```python
  67:     >>> from transformers import pipeline
  68  

train_real_world/transformers_4573/src/transformers/pipelines/object_detection.py:
  33      ```python
  34:     >>> from transformers import pipeline
  35  

train_real_world/transformers_4573/src/transformers/pipelines/question_answering.py:
  237      ```python
  238:     >>> from transformers import pipeline
  239  

train_real_world/transformers_4573/src/transformers/pipelines/table_question_answering.py:
  89      ```python
  90:     >>> from transformers import pipeline
  91  

train_real_world/transformers_4573/src/transformers/pipelines/text_classification.py:
  53      ```python
  54:     >>> from transformers import pipeline
  55  

train_real_world/transformers_4573/src/transformers/pipelines/text_generation.py:
  38      ```python
  39:     >>> from transformers import pipeline
  40  

  49      ```python
  50:     >>> from transformers import pipeline
  51  

train_real_world/transformers_4573/src/transformers/pipelines/text_to_audio.py:
  56      ```python
  57:     >>> from transformers import pipeline
  58  

  75      ```python
  76:     >>> from transformers import pipeline
  77  

train_real_world/transformers_4573/src/transformers/pipelines/token_classification.py:
  102      ```python
  103:     >>> from transformers import pipeline
  104  

train_real_world/transformers_4573/src/transformers/pipelines/visual_question_answering.py:
   32      ```python
   33:     >>> from transformers import pipeline
   34  

  122                  ```python
  123:                 >>> from transformers.pipelines.pt_utils import KeyDataset
  124                  >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/pipelines/zero_shot_audio_classification.py:
  45      ```python
  46:     >>> from transformers import pipeline
  47      >>> from datasets import load_dataset

train_real_world/transformers_4573/src/transformers/pipelines/zero_shot_classification.py:
  58      ```python
  59:     >>> from transformers import pipeline
  60  

train_real_world/transformers_4573/src/transformers/pipelines/zero_shot_image_classification.py:
  37      ```python
  38:     >>> from transformers import pipeline
  39  

train_real_world/transformers_4573/src/transformers/pipelines/zero_shot_object_detection.py:
  14  
  15:     from transformers.modeling_outputs import BaseModelOutput
  16  

  30      ```python
  31:     >>> from transformers import pipeline
  32  

  94                  ```python
  95:                 >>> from transformers import pipeline
  96  

train_real_world/transformers_4573/src/transformers/pipelines/deprecated/text2text_generation.py:
  34      ```python
  35:     >>> from transformers import pipeline
  36  

train_real_world/transformers_4573/src/transformers/quantizers/base.py:
  290      def __init__(self, config):
  291:         from transformers.models.llama4.modeling_llama4 import Llama4TextMLP
  292  

train_real_world/transformers_4573/src/transformers/utils/auto_docstring.py:
  1139      # import here to avoid circular import
  1140:     from transformers.models import auto as auto_module
  1141  

  1244      # import here to avoid circular import
  1245:     from transformers.models import auto as auto_module
  1246  

  1654      # Import here to avoid circular import
  1655:     from transformers.models import auto as auto_module
  1656  

  1790      # import here to avoid circular import
  1791:     from transformers.models import auto as auto_module
  1792  

train_real_world/transformers_4573/src/transformers/utils/backbone_utils.py:
  315      """
  316:     from transformers import AutoBackbone, AutoConfig
  317  

train_real_world/transformers_4573/src/transformers/utils/chat_parsing_utils.py:
  5  
  6: from transformers.utils import is_jmespath_available
  7  

train_real_world/transformers_4573/src/transformers/utils/chat_template_utils.py:
  282      ```python
  283:     >>> from transformers import AutoTokenizer
  284:     >>> from transformers.utils import get_json_schema
  285      >>>

train_real_world/transformers_4573/src/transformers/utils/doc.py:
  196      ```python
  197:     >>> from transformers import AutoTokenizer, {model_class}
  198      >>> import torch

  229      ```python
  230:     >>> from transformers import AutoTokenizer, {model_class}
  231      >>> import torch

  264      >>> import torch
  265:     >>> from transformers import AutoTokenizer, {model_class}
  266  

  292      >>> import torch
  293:     >>> from transformers import AutoTokenizer, {model_class}
  294  

  321      ```python
  322:     >>> from transformers import AutoTokenizer, {model_class}
  323      >>> import torch

  353      ```python
  354:     >>> from transformers import AutoTokenizer, {model_class}
  355      >>> import torch

  370      ```python
  371:     >>> from transformers import AutoTokenizer, {model_class}
  372      >>> import torch

  395      >>> import torch
  396:     >>> from transformers import AutoTokenizer, {model_class}
  397  

  411      ```python
  412:     >>> from transformers import AutoProcessor, {model_class}
  413      >>> import torch

  437      ```python
  438:     >>> from transformers import AutoProcessor, {model_class}
  439      >>> from datasets import load_dataset

  472      ```python
  473:     >>> from transformers import AutoFeatureExtractor, {model_class}
  474      >>> from datasets import load_dataset

  508      ```python
  509:     >>> from transformers import AutoFeatureExtractor, {model_class}
  510      >>> from datasets import load_dataset

  537      ```python
  538:     >>> from transformers import AutoFeatureExtractor, {model_class}
  539      >>> from datasets import load_dataset

  572      ```python
  573:     >>> from transformers import AutoImageProcessor, {model_class}
  574      >>> import torch

  597      ```python
  598:     >>> from transformers import AutoImageProcessor, {model_class}
  599      >>> import torch

  642      ```python
  643:     >>> from transformers import AutoProcessor, {model_class}, SpeechT5HifiGan
  644  

  660      ```python
  661:     >>> from transformers import AutoProcessor, {model_class}
  662  

  685      >>> import requests
  686:     >>> from transformers import AutoProcessor, {model_class}
  687  

  704      ```python
  705:     >>> from transformers import AutoImageProcessor, {model_class}
  706      >>> import torch

  886      >>> import requests
  887:     >>> from transformers import AutoProcessor, {model_class}
  888  

train_real_world/transformers_4573/src/transformers/utils/hub.py:
  756          ```python
  757:         from transformers import {object_class}
  758  

train_real_world/transformers_4573/src/transformers/utils/import_utils.py:
  2192                                  try:
  2193:                                     # Try to get it from transformers module to trigger lazy loading
  2194                                      transformers_module = sys.modules.get("transformers")

train_real_world/transformers_4573/templates/adding_a_missing_tokenization_test/cookiecutter-template-{{cookiecutter.modelname}}/test_tokenization_{{cookiecutter.lowercase_modelname}}.py:
  20  {% if cookiecutter.has_slow_class == "True" and  cookiecutter.has_fast_class == "True" -%}
  21: from transformers import {{cookiecutter.camelcase_modelname}}Tokenizer, {{cookiecutter.camelcase_modelname}}TokenizerFast
  22  {% elif  cookiecutter.has_slow_class == "True" -%}
  23: from transformers import {{cookiecutter.camelcase_modelname}}Tokenizer
  24  {% elif  cookiecutter.has_fast_class == "True" -%}
  25: from transformers import {{cookiecutter.camelcase_modelname}}TokenizerFast
  26  {% endif -%}
  27  {% if cookiecutter.has_fast_class == "True" and  cookiecutter.slow_tokenizer_use_sentencepiece == "True" -%}
  28: from transformers.testing_utils import require_sentencepiece, require_tokenizers
  29  from ...test_tokenization_common import TokenizerTesterMixin

  34  {% elif  cookiecutter.slow_tokenizer_use_sentencepiece == "True" -%}
  35: from transformers.testing_utils import require_sentencepiece
  36  from ...test_tokenization_common import TokenizerTesterMixin

  40  {% elif  cookiecutter.has_fast_class == "True" -%}
  41: from transformers.testing_utils import require_tokenizers
  42  from ...test_tokenization_common import TokenizerTesterMixin

train_real_world/transformers_4573/templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py:
   34  import transformers
   35: from transformers import (
   36      CONFIG_MAPPING,

   47  )
   48: from transformers.trainer_utils import get_last_checkpoint
   49  

  510  from accelerate import Accelerator
  511: from transformers import (
  512      CONFIG_MAPPING,

train_real_world/transformers_4573/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md:
   674  ```python
   675: from transformers import [camelcase name of model]Model, [camelcase name of model]Config
   676  model = [camelcase name of model]Model([camelcase name of model]Config())

  1014  ```python
  1015: from transformers import [camelcase name of model]Tokenizer
  1016  input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

train_real_world/transformers_4573/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md:
   684  ```python
   685: from transformers import BigBirdModel, BigBirdConfig
   686  model = BigBirdModel(BigBirdConfig())

  1003  ```python
  1004: from transformers import BertGenerationTokenizer
  1005  tokenizer = BertGenerationTokenizer("/path/to/gpt2.model/file")

  1025  ```python
  1026: from transformers import BertGenerationTokenizer
  1027  input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

train_real_world/transformers_4573/tests/causal_lm_tester.py:
  20  
  21: from transformers import AutoModelForCausalLM, PreTrainedConfig, set_seed
  22: from transformers.models.auto.auto_factory import getattribute_from_module
  23: from transformers.testing_utils import (
  24      _COMMON_MODEL_NAMES_MAP,

train_real_world/transformers_4573/tests/test_backbone_common.py:
  18  
  19: from transformers.testing_utils import require_torch, torch_device
  20: from transformers.utils.backbone_utils import BackboneType
  21  

train_real_world/transformers_4573/tests/test_configuration_common.py:
  20  
  21: from transformers import is_torch_available
  22: from transformers.utils import direct_transformers_import
  23  

train_real_world/transformers_4573/tests/test_executorch.py:
  18  
  19: from transformers import AutoModelForCausalLM, set_seed
  20: from transformers.generation.configuration_utils import GenerationConfig
  21: from transformers.integrations.executorch import (
  22      TorchExportableModuleForDecoderOnlyLM,

  25  )
  26: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3
  27: from transformers.testing_utils import require_torch
  28  

train_real_world/transformers_4573/tests/test_feature_extraction_common.py:
  19  
  20: from transformers.testing_utils import check_json_file_has_correct_format
  21  

train_real_world/transformers_4573/tests/test_image_processing_common.py:
  31  
  32: from transformers import AutoImageProcessor, BatchFeature
  33: from transformers.image_utils import AnnotationFormat, AnnotionFormat
  34: from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES
  35: from transformers.testing_utils import (
  36      check_json_file_has_correct_format,

  42  )
  43: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  44  

  46  if is_torchvision_available():
  47:     from transformers.image_processing_utils_fast import BaseImageProcessorFast
  48  

train_real_world/transformers_4573/tests/test_image_transforms.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils.import_utils import is_torch_available, is_vision_available
  22  

  29  
  30:     from transformers.image_transforms import (
  31          center_crop,

train_real_world/transformers_4573/tests/test_modeling_common.py:
   35  
   36: from transformers import (
   37      AutoModel,

   45  )
   46: from transformers.conversion_mapping import get_model_conversion_mapping
   47: from transformers.core_model_loading import WeightRenaming
   48: from transformers.integrations import HfDeepSpeedConfig
   49: from transformers.integrations.deepspeed import (
   50      is_deepspeed_available,

   53  )
   54: from transformers.modeling_layers import GradientCheckpointingLayer
   55: from transformers.modeling_utils import FLASH_ATTN_KERNEL_FALLBACK, _get_tied_weight_keys
   56: from transformers.models.auto import get_values
   57: from transformers.models.auto.modeling_auto import (
   58      MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,

   80  )
   81: from transformers.testing_utils import (
   82      CaptureLogger,

  107  )
  108: from transformers.utils import (
  109      CONFIG_NAME,

  125  
  126:     from transformers import MODEL_MAPPING
  127:     from transformers.integrations.accelerate import compute_module_sizes
  128:     from transformers.integrations.tensor_parallel import _get_parameter_tp_plan
  129:     from transformers.modeling_utils import load_state_dict
  130:     from transformers.pytorch_utils import id_tensor_storage
  131  

train_real_world/transformers_4573/tests/test_pipeline_mixin.py:
  39  
  40: from transformers.models.auto.processing_auto import PROCESSOR_MAPPING_NAMES
  41: from transformers.pipelines import (
  42      AudioClassificationPipeline,

  52  )
  53: from transformers.testing_utils import (
  54      is_pipeline_test,

  60  )
  61: from transformers.utils import direct_transformers_import, logging
  62  

train_real_world/transformers_4573/tests/test_processing_common.py:
   28  
   29: from transformers.processing_utils import (
   30      MODALITY_TO_AUTOPROCESSOR_MAPPING,

   32  )
   33: from transformers.testing_utils import (
   34      check_json_file_has_correct_format,

   39  )
   40: from transformers.utils import is_torch_available, is_vision_available
   41  

  214                  f"        # Create your custom {component_type}\n"
  215:                 f"        from transformers import {component_class}\n"
  216                  f"        component = {component_class}(...)\n"

  232  
  233:         from transformers.models.auto.configuration_auto import (
  234              CONFIG_MAPPING,

  278          if mapping_name == "tokenizer":
  279:             from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING
  280  

  282          elif mapping_name == "image_processor":
  283:             from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING
  284  

  286          elif mapping_name == "feature_extractor":
  287:             from transformers.models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING
  288  

  290          elif mapping_name == "video_processor":
  291:             from transformers.models.auto.video_processing_auto import VIDEO_PROCESSOR_MAPPING
  292  

train_real_world/transformers_4573/tests/test_sentencepiece_backend_mixin.py:
  6  
  7: from transformers import AutoTokenizer, PythonBackend, TokenizersBackend
  8: from transformers.tokenization_python import AddedToken
  9  

train_real_world/transformers_4573/tests/test_sequence_feature_extraction_common.py:
  17  
  18: from transformers import BatchFeature
  19: from transformers.testing_utils import require_torch
  20  

train_real_world/transformers_4573/tests/test_tokenization_common.py:
    31  
    32: from transformers import (
    33      AutoTokenizer,

    44  )
    45: from transformers.testing_utils import (
    46      get_tests_dir,

    51  )
    52: from transformers.tokenization_python import AddedToken
    53  

    84  if TYPE_CHECKING:
    85:     from transformers import PretrainedConfig, PreTrainedModel
    86  

   470          """
   471:         from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
   472  

  2703      def test_local_files_only(self):
  2704:         from transformers import AutoTokenizer
  2705  

  2809      def test_local_files_only(self):
  2810:         from transformers import AutoTokenizer
  2811  

train_real_world/transformers_4573/tests/test_tokenization_mistral_common.py:
  24  
  25: from transformers.image_utils import load_image
  26: from transformers.models.auto.tokenization_auto import AutoTokenizer
  27: from transformers.testing_utils import (
  28      is_flaky,

  30  )
  31: from transformers.tokenization_mistral_common import MistralCommonBackend
  32: from transformers.tokenization_utils_base import BatchEncoding, TruncationStrategy
  33: from transformers.utils import PaddingStrategy, is_mistral_common_available
  34  

train_real_world/transformers_4573/tests/test_tokenizers_backend_mixin.py:
    9  
   10: from transformers import TokenizersBackend
   11: from transformers.tokenization_utils_base import PreTrainedTokenizerBase
   12  

   53              try:
   54:                 from transformers import AutoTokenizer
   55  

  440      def test_local_files_only(self):
  441:         from transformers import AutoTokenizer
  442  

train_real_world/transformers_4573/tests/test_training_args.py:
  4  
  5: from transformers import TrainingArguments
  6  

train_real_world/transformers_4573/tests/test_training_mixin.py:
  23  
  24: from transformers import set_seed
  25: from transformers.testing_utils import Colors, build_cpu_memory_monitor, init_test_logger, is_training_test
  26  

train_real_world/transformers_4573/tests/test_video_processing_common.py:
  26  
  27: from transformers import AutoVideoProcessor
  28: from transformers.testing_utils import (
  29      check_json_file_has_correct_format,

  35  )
  36: from transformers.utils import is_torch_available, is_vision_available
  37: from transformers.video_utils import VideoMetadata
  38  

train_real_world/transformers_4573/tests/cli/test_chat.py:
  17  
  18: from transformers.cli.chat import new_chat_history, parse_generate_flags, save_chat
  19  

train_real_world/transformers_4573/tests/cli/test_download.py:
  16  
  17: from transformers.testing_utils import require_torch
  18  

train_real_world/transformers_4573/tests/cli/test_serve.py:
  24  
  25: from transformers import GenerationConfig
  26: from transformers.cli.serve import Modality, Serve
  27: from transformers.testing_utils import require_openai, slow
  28: from transformers.utils.import_utils import is_openai_available
  29  

train_real_world/transformers_4573/tests/cli/test_system.py:
  14  
  15: from transformers import __version__
  16  

train_real_world/transformers_4573/tests/deepspeed/test_alst_ulysses_sp.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      TestCasePlus,

  32  
  33:     from transformers import (
  34          AutoModelForCausalLM,

train_real_world/transformers_4573/tests/deepspeed/test_deepspeed.py:
    28  from tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa
    29: from transformers import AutoModel, TrainingArguments, is_torch_available, logging
    30: from transformers.integrations.deepspeed import (
    31      HfDeepSpeedConfig,

    34  )
    35: from transformers.testing_utils import (
    36      CaptureLogger,

    52  )
    53: from transformers.trainer_utils import get_last_checkpoint, set_seed
    54: from transformers.utils import SAFE_WEIGHTS_NAME, is_torch_bf16_available_on_device, is_torch_fp16_available_on_device
    55  

   126      from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint
   127:     from transformers.integrations.deepspeed import deepspeed_config, is_deepspeed_zero3_enabled  # noqa
   128  

   279  
   280:         from transformers.models.gpt2.modeling_gpt2 import GPT2Model, GPT2PreTrainedModel
   281  

   746      # def test_missed_zero3_init(self):
   747:     #     from transformers import Trainer  # noqa
   748  

  1000  
  1001:         from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer  # noqa
  1002  

train_real_world/transformers_4573/tests/deepspeed/test_model_zoo.py:
  22  from tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa
  23: from transformers import is_torch_available
  24: from transformers.testing_utils import (
  25      TestCasePlus,

  34  )
  35: from transformers.trainer_utils import set_seed
  36  

train_real_world/transformers_4573/tests/extended/test_trainer_ext.py:
   23  
   24: from transformers.testing_utils import (
   25      CaptureStderr,

   37  )
   38: from transformers.trainer_callback import TrainerState
   39: from transformers.trainer_utils import set_seed
   40  

  165      def test_run_seq2seq_bnb(self):
  166:         from transformers.training_args import OptimizerNames
  167  

train_real_world/transformers_4573/tests/fsdp/test_context_parallel.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      TestCasePlus,

  32  
  33:     from transformers import (
  34          AutoModelForCausalLM,

train_real_world/transformers_4573/tests/fsdp/test_fsdp.py:
  24  from tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa
  25: from transformers import is_torch_available
  26: from transformers.testing_utils import (
  27      TestCasePlus,

  38  )
  39: from transformers.trainer_callback import TrainerState
  40: from transformers.trainer_utils import FSDPOption, set_seed
  41: from transformers.utils import (
  42      is_accelerate_available,

  48  if is_torch_available():
  49:     from transformers.trainer import FSDP_MODEL_NAME
  50  

train_real_world/transformers_4573/tests/generation/test_candidate_generator.py:
   7  
   8: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline
   9: from transformers.generation.candidate_generator import (
  10      AssistantToTargetTranslator,

  13  )
  14: from transformers.testing_utils import require_torch, torch_device
  15  

train_real_world/transformers_4573/tests/generation/test_configuration_utils.py:
  24  
  25: from transformers import AutoConfig, GenerationConfig, WatermarkingConfig, is_torch_available
  26: from transformers import logging as transformers_logging
  27  

  31  
  32: from transformers.generation import (
  33      ClassifierFreeGuidanceLogitsProcessor,

  58  )
  59: from transformers.testing_utils import (
  60      TOKEN,

train_real_world/transformers_4573/tests/generation/test_continuous_batching.py:
  21  
  22: from transformers import (
  23      AutoConfig,

  29  )
  30: from transformers.generation.continuous_batching.cache import (
  31      FullAttentionCacheAllocator,

  34  )
  35: from transformers.generation.continuous_batching.continuous_api import build_attention_mask
  36: from transformers.testing_utils import (
  37      Expectations,

  42  )
  43: from transformers.utils import is_flash_attn_2_available, is_kernels_available
  44  

train_real_world/transformers_4573/tests/generation/test_flash_attention_parity.py:
  22  
  23: from transformers import AutoModelForCausalLM, AutoTokenizer
  24: from transformers.testing_utils import require_flash_attn, require_flash_attn_3, require_torch_gpu, slow
  25  

train_real_world/transformers_4573/tests/generation/test_fsdp.py:
   19  
   20: from transformers import is_torch_available, is_torch_xpu_available
   21: from transformers.testing_utils import (
   22      TestCasePlus,

   30  )
   31: from transformers.utils import is_ccl_available, is_ipex_available
   32  

   49  
   50:     from transformers import AutoModelForCausalLM, AutoTokenizer
   51:     from transformers.models.gpt2.modeling_gpt2 import GPT2Block
   52  

  154              from torch.distributed.fsdp import fully_shard
  155:             from transformers import AutoModelForTokenClassification
  156  

train_real_world/transformers_4573/tests/generation/test_logits_process.py:
  19  
  20: from transformers import is_torch_available
  21: from transformers.testing_utils import require_torch, torch_device
  22  

  29  
  30:     from transformers.generation import (
  31          EncoderNoRepeatNGramLogitsProcessor,

  57      )
  58:     from transformers.generation.logits_process import (
  59          BarkEosPrioritizerLogitsProcessor,

train_real_world/transformers_4573/tests/generation/test_paged_attention.py:
  5  
  6: from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
  7: from transformers.testing_utils import require_flash_attn, require_torch_accelerator, slow
  8  

train_real_world/transformers_4573/tests/generation/test_stopping_criteria.py:
  17  
  18: from transformers import AutoTokenizer, is_torch_available
  19: from transformers.testing_utils import require_torch, torch_device
  20  

  26  
  27:     from transformers.generation import (
  28          ConfidenceCriteria,

train_real_world/transformers_4573/tests/generation/test_streamers.py:
  21  
  22: from transformers import (
  23      AsyncTextIteratorStreamer,

  28  )
  29: from transformers.testing_utils import CaptureStdout, require_torch, torch_device
  30: from transformers.utils.logging import _get_library_root_logger
  31  

  37  
  38:     from transformers import AutoModelForCausalLM
  39  

train_real_world/transformers_4573/tests/generation/test_utils.py:
   31  
   32: from transformers import (
   33      AutoConfig,

   40  )
   41: from transformers.testing_utils import (
   42      CaptureLogger,

   57  )
   58: from transformers.utils import is_ipex_available, is_torchdynamo_exporting
   59  

   64  
   65:     from transformers import (
   66          AutoModelForCausalLM,

   77      )
   78:     from transformers.cache_utils import (
   79          Cache,

   84      )
   85:     from transformers.generation import (
   86          CompileConfig,

  102      )
  103:     from transformers.generation.candidate_generator import (
  104          AssistedCandidateGenerator,

  106      )
  107:     from transformers.generation.utils import _speculative_sampling
  108  

  110  
  111: from transformers.utils import is_sklearn_available
  112  

train_real_world/transformers_4573/tests/integrations/test_hub_kernels.py:
    4  
    5: from transformers.testing_utils import require_kernels
    6  

   17  
   18:             from transformers.integrations import hub_kernels
   19  

   34  
   35:             from transformers.integrations import hub_kernels
   36  

   49  
   50:             from transformers.integrations import hub_kernels
   51  

   66  
   67:             from transformers.integrations import hub_kernels
   68  

   94  
   95:             from transformers.integrations import hub_kernels
   96  

  113  
  114:             from transformers.integrations import hub_kernels
  115  

train_real_world/transformers_4573/tests/kernels/test_kernels.py:
  21  
  22: from transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig
  23: from transformers.integrations.hub_kernels import (
  24      _HUB_KERNEL_MAPPING,

  29  )
  30: from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS
  31: from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS
  32: from transformers.testing_utils import (
  33      TestCasePlus,

  39  )
  40: from transformers.utils.import_utils import is_kernels_available
  41  

train_real_world/transformers_4573/tests/models/afmoe/test_modeling_afmoe.py:
  16  
  17: from transformers import is_torch_available
  18: from transformers.testing_utils import require_torch
  19  

  21  if is_torch_available():
  22:     from transformers import AfmoeForCausalLM, AfmoeModel
  23  

train_real_world/transformers_4573/tests/models/aimv2/test_modeling_aimv2.py:
  24  
  25: from transformers import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig
  26: from transformers.testing_utils import (
  27      is_flaky,

  32  )
  33: from transformers.utils import (
  34      is_torch_available,

  53  
  54:     from transformers import (
  55          Aimv2Model,

  63  
  64:     from transformers import AutoImageProcessor, AutoProcessor
  65  

train_real_world/transformers_4573/tests/models/albert/test_modeling_albert.py:
  20  
  21: from transformers import AlbertConfig, AutoTokenizer, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_torch, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/albert/test_tokenization_albert.py:
  16  
  17: from transformers import AlbertTokenizer
  18: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  19: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  20  

train_real_world/transformers_4573/tests/models/align/test_modeling_align.py:
  21  
  22: from transformers import AlignConfig, AlignProcessor, AlignTextConfig, AlignVisionConfig
  23: from transformers.testing_utils import (
  24      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_vision_available
  30  

  43  
  44:     from transformers import (
  45          AlignModel,

train_real_world/transformers_4573/tests/models/align/test_processing_align.py:
  16  
  17: from transformers.testing_utils import require_vision
  18: from transformers.utils import is_vision_available
  19  

  23  if is_vision_available():
  24:     from transformers import AlignProcessor
  25  

train_real_world/transformers_4573/tests/models/altclip/test_modeling_altclip.py:
  21  
  22: from transformers import AltCLIPConfig, AltCLIPProcessor, AltCLIPTextConfig, AltCLIPVisionConfig
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24: from transformers.utils import is_torch_available, is_vision_available
  25  

  39  
  40:     from transformers import AltCLIPModel, AltCLIPTextModel, AltCLIPVisionModel
  41  

train_real_world/transformers_4573/tests/models/altclip/test_processing_altclip.py:
  17  
  18: from transformers import AltCLIPProcessor
  19: from transformers.testing_utils import require_vision
  20  

train_real_world/transformers_4573/tests/models/apertus/test_modeling_apertus.py:
  22  
  23: from transformers import is_torch_available
  24: from transformers.testing_utils import (
  25      require_read_token,

  34  if is_torch_available():
  35:     from transformers import (
  36          ApertusForCausalLM,

train_real_world/transformers_4573/tests/models/arcee/test_modeling_arcee.py:
  19  
  20: from transformers import AutoTokenizer, is_torch_available
  21: from transformers.testing_utils import (
  22      require_flash_attn,

  33  
  34:     from transformers import (
  35          ArceeConfig,

train_real_world/transformers_4573/tests/models/aria/test_image_processing_aria.py:
  19  
  20: from transformers.image_utils import ChannelDimension, PILImageResampling
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  29  
  30:     from transformers import AriaImageProcessor
  31  

train_real_world/transformers_4573/tests/models/aria/test_modeling_aria.py:
  19  
  20: from transformers import (
  21      AriaConfig,

  30  )
  31: from transformers.models.idefics3 import Idefics3VisionConfig
  32: from transformers.testing_utils import (
  33      Expectations,

train_real_world/transformers_4573/tests/models/aria/test_processing_aria.py:
  18  
  19: from transformers import AriaProcessor
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_torch, require_vision
  22  

train_real_world/transformers_4573/tests/models/audio_spectrogram_transformer/test_feature_extraction_audio_spectrogram_transformer.py:
   23  
   24: from transformers import ASTFeatureExtractor
   25: from transformers.testing_utils import check_json_file_has_correct_format, require_torch, require_torchaudio
   26: from transformers.utils.import_utils import is_torch_available
   27  

  216  
  217:         from transformers.models.audio_spectrogram_transformer.feature_extraction_audio_spectrogram_transformer import (
  218              is_speech_available,

train_real_world/transformers_4573/tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py:
  21  
  22: from transformers import ASTConfig
  23: from transformers.testing_utils import require_torch, require_torchaudio, slow, torch_device
  24: from transformers.utils import is_torch_available, is_torchaudio_available
  25  

  34  
  35:     from transformers import ASTForAudioClassification, ASTModel
  36  

  40  
  41:     from transformers import ASTFeatureExtractor
  42  

train_real_world/transformers_4573/tests/models/audioflamingo3/test_modeling_audioflamingo3.py:
  24  
  25: from transformers import (
  26      AudioFlamingo3Config,

  30  )
  31: from transformers.testing_utils import (
  32      cleanup,

train_real_world/transformers_4573/tests/models/audioflamingo3/test_processing_audioflamingo3.py:
  22  
  23: from transformers import (
  24      AudioFlamingo3Processor,

  28  )
  29: from transformers.testing_utils import require_librosa, require_torch, require_torchaudio
  30  

train_real_world/transformers_4573/tests/models/auto/test_configuration_auto.py:
  24  import transformers.models.auto
  25: from transformers.models.auto.configuration_auto import CONFIG_MAPPING, AutoConfig
  26: from transformers.models.bert.configuration_bert import BertConfig
  27: from transformers.models.roberta.configuration_roberta import RobertaConfig
  28: from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, get_tests_dir
  29  

train_real_world/transformers_4573/tests/models/auto/test_feature_extraction_auto.py:
  22  import transformers
  23: from transformers import (
  24      CONFIG_MAPPING,

  30  )
  31: from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, get_tests_dir
  32  

train_real_world/transformers_4573/tests/models/auto/test_image_processing_auto.py:
  22  import transformers
  23: from transformers import (
  24      CONFIG_MAPPING,

  32  )
  33: from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_torchvision, require_vision
  34  

train_real_world/transformers_4573/tests/models/auto/test_modeling_auto.py:
   24  import transformers
   25: from transformers import BertConfig, GPT2Model, is_torch_available
   26: from transformers.models.auto.configuration_auto import CONFIG_MAPPING
   27: from transformers.testing_utils import (
   28      DUMMY_UNKNOWN_IDENTIFIER,

   46  
   47:     from transformers import (
   48          AutoBackbone,

   78      )
   79:     from transformers.models.auto.modeling_auto import (
   80          MODEL_FOR_CAUSAL_LM_MAPPING,

  523      def test_attr_not_existing(self):
  524:         from transformers.models.auto.auto_factory import _LazyAutoMapping
  525  

train_real_world/transformers_4573/tests/models/auto/test_processor_auto.py:
  25  import transformers
  26: from transformers import (
  27      CONFIG_MAPPING,

  50  )
  51: from transformers.models.auto.feature_extraction_auto import get_feature_extractor_config
  52: from transformers.models.auto.image_processing_auto import get_image_processor_config
  53: from transformers.models.auto.video_processing_auto import get_video_processor_config
  54: from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test
  55: from transformers.tokenization_python import TOKENIZER_CONFIG_FILE
  56: from transformers.utils import (
  57      FEATURE_EXTRACTOR_NAME,

train_real_world/transformers_4573/tests/models/auto/test_tokenization_auto.py:
   27  import transformers
   28: from transformers import (
   29      AutoTokenizer,

   43  )
   44: from transformers.models.auto.configuration_auto import CONFIG_MAPPING, AutoConfig
   45: from transformers.models.auto.tokenization_auto import (
   46      TOKENIZER_MAPPING,

   50  )
   51: from transformers.models.roberta.configuration_roberta import RobertaConfig
   52: from transformers.testing_utils import (
   53      DUMMY_DIFF_TOKENIZER_IDENTIFIER,

  536          nop_config_code = """
  537: from transformers import PreTrainedConfig
  538  

train_real_world/transformers_4573/tests/models/auto/test_video_processing_auto.py:
  22  import transformers
  23: from transformers import (
  24      CONFIG_MAPPING,

  30  )
  31: from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_torch
  32  

train_real_world/transformers_4573/tests/models/autoformer/test_modeling_autoformer.py:
  21  
  22: from transformers import is_torch_available
  23: from transformers.testing_utils import is_flaky, require_torch, slow, torch_device
  24: from transformers.utils import check_torch_load_is_safe
  25  

  35  
  36:     from transformers import AutoformerConfig, AutoformerForPrediction, AutoformerModel
  37:     from transformers.models.autoformer.modeling_autoformer import AutoformerDecoder, AutoformerEncoder
  38  

train_real_world/transformers_4573/tests/models/aya_vision/test_modeling_aya_vision.py:
  19  
  20: from transformers import (
  21      AutoProcessor,

  25  )
  26: from transformers.testing_utils import (
  27      Expectations,

  46  
  47:     from transformers import (
  48          AyaVisionForConditionalGeneration,

train_real_world/transformers_4573/tests/models/aya_vision/test_processing_aya_vision.py:
  16  
  17: from transformers import AyaVisionProcessor
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torch_available
  20  

train_real_world/transformers_4573/tests/models/bamba/test_modeling_bamba.py:
  22  
  23: from transformers import (
  24      AutoTokenizer,

  28  )
  29: from transformers.testing_utils import (
  30      DeviceProperties,

  49  
  50:     from transformers import (
  51          BambaForCausalLM,

  53      )
  54:     from transformers.models.bamba.modeling_bamba import HybridMambaAttentionDynamicCache
  55  

train_real_world/transformers_4573/tests/models/bark/test_modeling_bark.py:
  21  
  22: from transformers import (
  23      BarkCausalModel,

  29  )
  30: from transformers.models.bark.generation_configuration_bark import (
  31      BarkCoarseGenerationConfig,

  34  )
  35: from transformers.testing_utils import (
  36      backend_torch_accelerator_module,

  52  
  53:     from transformers import (
  54          BarkCoarseModel,

train_real_world/transformers_4573/tests/models/bark/test_processing_bark.py:
  21  
  22: from transformers import AutoTokenizer, BarkProcessor
  23: from transformers.testing_utils import require_torch, slow
  24  

train_real_world/transformers_4573/tests/models/bart/test_modeling_bart.py:
  23  
  24: from transformers import BartConfig, is_torch_available
  25: from transformers.testing_utils import (
  26      require_sentencepiece,

  42  
  43:     from transformers import (
  44          AutoModelForSequenceClassification,

  52      )
  53:     from transformers.models.bart.modeling_bart import BartDecoder, BartEncoder, shift_tokens_right
  54  

train_real_world/transformers_4573/tests/models/barthez/test_tokenization_barthez.py:
  16  
  17: from transformers import BarthezTokenizer
  18: from transformers.testing_utils import require_sentencepiece, require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/bartpho/test_tokenization_bartpho.py:
  18  
  19: from transformers.models.bartpho.tokenization_bartpho import VOCAB_FILES_NAMES, BartphoTokenizer
  20: from transformers.testing_utils import get_tests_dir
  21  

train_real_world/transformers_4573/tests/models/beit/test_image_processing_beit.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  29  if is_vision_available():
  30:     from transformers import BeitImageProcessor
  31  
  32      if is_torchvision_available():
  33:         from transformers import BeitImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/beit/test_modeling_beit.py:
  21  
  22: from transformers import BeitConfig
  23: from transformers.testing_utils import (
  24      require_torch,

  29  )
  30: from transformers.utils import (
  31      is_torch_available,

  44  
  45:     from transformers import (
  46          BeitBackbone,

  51      )
  52:     from transformers.models.auto.modeling_auto import MODEL_FOR_BACKBONE_MAPPING_NAMES, MODEL_MAPPING_NAMES
  53  

  57  
  58:     from transformers import BeitImageProcessor
  59  

train_real_world/transformers_4573/tests/models/bert/test_modeling_bert.py:
  20  
  21: from transformers import AutoTokenizer, BertConfig, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import (
  24      require_torch,

  37  
  38:     from transformers import (
  39          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/bert/test_tokenization_bert.py:
  17  
  18: from transformers.models.bert.tokenization_bert import (
  19      BertTokenizer,
  20  )
  21: from transformers.testing_utils import require_read_token, require_tokenizers
  22  

train_real_world/transformers_4573/tests/models/bert_generation/test_modeling_bert_generation.py:
  19  
  20: from transformers import BertGenerationConfig, is_torch_available
  21: from transformers.testing_utils import require_torch, slow, torch_device
  22  

  31  
  32:     from transformers import BertGenerationDecoder, BertGenerationEncoder, DataCollatorWithFlattening
  33  

train_real_world/transformers_4573/tests/models/bert_generation/test_tokenization_bert_generation.py:
   17  
   18: from transformers import BertGenerationTokenizer
   19: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_torch, slow
   20  

  213  
  214:         from transformers import BertGenerationConfig, BertGenerationEncoder
  215  

train_real_world/transformers_4573/tests/models/bert_japanese/test_tokenization_bert_japanese.py:
  20  
  21: from transformers import AutoTokenizer
  22: from transformers.models.bert_japanese.tokenization_bert_japanese import (
  23      VOCAB_FILES_NAMES,

  30  )
  31: from transformers.testing_utils import custom_tokenizers, require_jumanpp, require_sudachi_projection
  32  

train_real_world/transformers_4573/tests/models/bertweet/test_tokenization_bertweet.py:
  17  
  18: from transformers.models.bertweet.tokenization_bertweet import VOCAB_FILES_NAMES, BertweetTokenizer
  19  

train_real_world/transformers_4573/tests/models/big_bird/test_modeling_big_bird.py:
  17  
  18: from transformers import BigBirdConfig, is_torch_available
  19: from transformers.models.auto import get_values
  20: from transformers.models.big_bird.tokenization_big_bird import BigBirdTokenizer
  21: from transformers.testing_utils import require_torch, slow, torch_device
  22  

  30  
  31:     from transformers import (
  32          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/big_bird/test_tokenization_big_bird.py:
  16  
  17: from transformers import BigBirdTokenizer
  18: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py:
  19  
  20: from transformers import BigBirdPegasusConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      require_sentencepiece,

  38  
  39:     from transformers import (
  40          BigBirdPegasusForCausalLM,

  46      )
  47:     from transformers.models.bigbird_pegasus.modeling_bigbird_pegasus import (
  48          BigBirdPegasusDecoder,

train_real_world/transformers_4573/tests/models/biogpt/test_modeling_biogpt.py:
  18  
  19: from transformers import BioGptConfig, is_sacremoses_available, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          BioGptForCausalLM,

train_real_world/transformers_4573/tests/models/biogpt/test_tokenization_biogpt.py:
  21  
  22: from transformers.models.biogpt.tokenization_biogpt import VOCAB_FILES_NAMES, BioGptTokenizer
  23: from transformers.testing_utils import require_sacremoses, slow
  24  

train_real_world/transformers_4573/tests/models/bit/test_image_processing_bit.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import BitImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import BitImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/bit/test_modeling_bit.py:
  18  
  19: from transformers import BitConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import BitBackbone, BitForImageClassification, BitImageProcessor, BitModel
  33  

train_real_world/transformers_4573/tests/models/bitnet/test_modeling_bitnet.py:
  18  
  19: from transformers import AutoTokenizer, BitNetConfig, is_torch_available
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

  35  
  36:     from transformers import (
  37          BitNetForCausalLM,

train_real_world/transformers_4573/tests/models/blenderbot/test_modeling_blenderbot.py:
  19  
  20: from transformers import BlenderbotConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      backend_empty_cache,

  39  
  40:     from transformers import BlenderbotForConditionalGeneration, BlenderbotModel, BlenderbotTokenizer
  41:     from transformers.models.blenderbot.modeling_blenderbot import (
  42          BlenderbotDecoder,

train_real_world/transformers_4573/tests/models/blenderbot/test_tokenization_blenderbot.py:
  2  
  3: from transformers.models.blenderbot.tokenization_blenderbot import BlenderbotTokenizer
  4: from transformers.testing_utils import require_tokenizers
  5  

train_real_world/transformers_4573/tests/models/blenderbot_small/test_modeling_blenderbot_small.py:
  19  
  20: from transformers import BlenderbotSmallConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      require_torch,

  36  
  37:     from transformers import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel, BlenderbotSmallTokenizer
  38:     from transformers.models.blenderbot_small.modeling_blenderbot_small import (
  39          BlenderbotSmallDecoder,

train_real_world/transformers_4573/tests/models/blenderbot_small/test_tokenization_blenderbot_small.py:
  22  
  23: from transformers.models.blenderbot_small.tokenization_blenderbot_small import (
  24      VOCAB_FILES_NAMES,

train_real_world/transformers_4573/tests/models/blip/test_image_processing_blip.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import BlipImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import BlipImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/blip/test_modeling_blip_text.py:
  19  
  20: from transformers import BlipTextConfig
  21: from transformers.testing_utils import require_torch, slow, torch_device
  22: from transformers.utils import is_torch_available
  23  

  30  
  31:     from transformers import BlipTextModel
  32  

train_real_world/transformers_4573/tests/models/blip/test_modeling_blip.py:
  22  
  23: from transformers import BlipConfig, BlipTextConfig, BlipVisionConfig
  24: from transformers.testing_utils import (
  25      require_torch,

  31  )
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  47  
  48:     from transformers import (
  49          BlipForConditionalGeneration,

  60  
  61:     from transformers import BlipProcessor
  62  

train_real_world/transformers_4573/tests/models/blip/test_processing_blip.py:
  15  
  16: from transformers.testing_utils import require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import BlipProcessor
  24  

train_real_world/transformers_4573/tests/models/blip_2/test_modeling_blip_2.py:
  23  
  24: from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig
  25: from transformers.testing_utils import (
  26      Expectations,

  35  )
  36: from transformers.utils import is_torch_available, is_vision_available
  37  

  53  
  54:     from transformers import (
  55          Blip2ForConditionalGeneration,

  66  
  67:     from transformers import Blip2Processor
  68  

train_real_world/transformers_4573/tests/models/blip_2/test_processing_blip_2.py:
  15  
  16: from transformers.testing_utils import require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import Blip2Processor
  24  

train_real_world/transformers_4573/tests/models/bloom/test_modeling_bloom.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device
  21  

  28  
  29:     from transformers import (
  30          AutoTokenizer,

train_real_world/transformers_4573/tests/models/bloom/test_tokenization_bloom.py:
  18  
  19: from transformers import TokenizersBackend
  20: from transformers.testing_utils import require_tokenizers, slow
  21  

train_real_world/transformers_4573/tests/models/blt/test_modeling_blt.py:
  20  
  21: from transformers import AutoTokenizer, is_torch_available
  22: from transformers.testing_utils import (
  23      cleanup,

  41  
  42:     from transformers import BltConfig, BltForCausalLM, BltModel
  43  

train_real_world/transformers_4573/tests/models/bridgetower/test_image_processing_bridgetower.py:
  17  
  18: from transformers.image_utils import load_image
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26  if is_vision_available():
  27:     from transformers import BridgeTowerImageProcessor
  28  
  29      if is_torchvision_available():
  30:         from transformers import BridgeTowerImageProcessorFast
  31  

train_real_world/transformers_4573/tests/models/bridgetower/test_modeling_bridgetower.py:
  18  
  19: from transformers import (
  20      BridgeTowerConfig,

  25  )
  26: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  27  

  40  
  41:     from transformers import (
  42          BridgeTowerForContrastiveLearning,

  50  
  51:     from transformers import BridgeTowerProcessor
  52  

train_real_world/transformers_4573/tests/models/bridgetower/test_processing_bridgetower.py:
  15  
  16: from transformers.testing_utils import require_torch, require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import (
  24          BridgeTowerProcessor,

train_real_world/transformers_4573/tests/models/bros/test_modeling_bros.py:
  18  
  19: from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device
  20: from transformers.utils import is_torch_available
  21  

  29  
  30:     from transformers import (
  31          BrosConfig,

train_real_world/transformers_4573/tests/models/byt5/test_tokenization_byt5.py:
  20  
  21: from transformers import BatchEncoding, ByT5Tokenizer
  22  

train_real_world/transformers_4573/tests/models/camembert/test_modeling_camembert.py:
  16  
  17: from transformers import is_torch_available
  18: from transformers.testing_utils import (
  19      require_sentencepiece,

  29  
  30:     from transformers import CamembertModel
  31  

train_real_world/transformers_4573/tests/models/camembert/test_tokenization_camembert.py:
  2  
  3: from transformers.models.camembert.tokenization_camembert import CamembertTokenizer
  4: from transformers.testing_utils import require_tokenizers
  5  

train_real_world/transformers_4573/tests/models/canine/test_modeling_canine.py:
  17  
  18: from transformers import CanineConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          CanineForMultipleChoice,

train_real_world/transformers_4573/tests/models/canine/test_tokenization_canine.py:
  19  
  20: from transformers import BatchEncoding, CanineTokenizer
  21: from transformers.testing_utils import require_tokenizers, require_torch
  22: from transformers.tokenization_python import AddedToken
  23  

train_real_world/transformers_4573/tests/models/chameleon/test_image_processing_chameleon.py:
  18  
  19: from transformers.image_utils import PILImageResampling
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import ChameleonImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import ChameleonImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/chameleon/test_modeling_chameleon.py:
  20  
  21: from transformers import BitsAndBytesConfig, ChameleonConfig, is_torch_available, is_vision_available
  22: from transformers.testing_utils import (
  23      Expectations,

  42  
  43:     from transformers import (
  44          ChameleonForConditionalGeneration,

train_real_world/transformers_4573/tests/models/chameleon/test_processing_chameleon.py:
  17  
  18: from transformers import ChameleonProcessor
  19: from transformers.testing_utils import get_tests_dir
  20  

train_real_world/transformers_4573/tests/models/chinese_clip/test_image_processing_chinese_clip.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import ChineseCLIPImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import ChineseCLIPImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/chinese_clip/test_modeling_chinese_clip.py:
  20  
  21: from transformers import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24: from transformers.utils import is_torch_available, is_vision_available
  25  

  39  
  40:     from transformers import (
  41          MODEL_FOR_PRETRAINING_MAPPING,

  50  
  51:     from transformers import ChineseCLIPProcessor
  52  

train_real_world/transformers_4573/tests/models/chinese_clip/test_processing_chinese_clip.py:
  17  
  18: from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES
  19: from transformers.testing_utils import require_vision
  20: from transformers.utils import is_vision_available
  21  

  25  if is_vision_available():
  26:     from transformers import ChineseCLIPProcessor
  27  

train_real_world/transformers_4573/tests/models/clap/test_feature_extraction_clap.py:
  22  
  23: from transformers import ClapFeatureExtractor
  24: from transformers.testing_utils import require_torch, require_torchaudio
  25: from transformers.trainer_utils import set_seed
  26: from transformers.utils.import_utils import is_torch_available
  27  

train_real_world/transformers_4573/tests/models/clap/test_modeling_clap.py:
  22  
  23: from transformers import ClapAudioConfig, ClapConfig, ClapProcessor, ClapTextConfig
  24: from transformers.testing_utils import require_torch, slow, torch_device
  25: from transformers.utils import is_torch_available
  26  

  40  
  41:     from transformers import (
  42          ClapAudioModel,

train_real_world/transformers_4573/tests/models/clap/test_processing_clap.py:
  18  
  19: from transformers import ClapFeatureExtractor, ClapProcessor, RobertaTokenizer
  20: from transformers.testing_utils import require_sentencepiece, require_torchaudio
  21: from transformers.tokenization_utils_tokenizers import TokenizersBackend
  22  

train_real_world/transformers_4573/tests/models/clip/test_image_processing_clip.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import CLIPImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import CLIPImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/clip/test_modeling_clip.py:
  24  
  25: from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
  26: from transformers.testing_utils import (
  27      require_torch,

  31  )
  32: from transformers.utils import (
  33      is_torch_available,

  52  
  53:     from transformers import (
  54          CLIPForImageClassification,

  64  
  65:     from transformers import CLIPProcessor
  66  

train_real_world/transformers_4573/tests/models/clip/test_processing_clip.py:
  16  
  17: from transformers.testing_utils import require_vision
  18: from transformers.utils import is_vision_available
  19  

  23  if is_vision_available():
  24:     from transformers import CLIPProcessor
  25  

train_real_world/transformers_4573/tests/models/clip/test_tokenization_clip.py:
  2  
  3: from transformers.models.clip.tokenization_clip import CLIPTokenizer
  4: from transformers.testing_utils import require_tokenizers
  5  

train_real_world/transformers_4573/tests/models/clipseg/test_modeling_clipseg.py:
  22  
  23: from transformers import CLIPSegConfig, CLIPSegProcessor, CLIPSegTextConfig, CLIPSegVisionConfig
  24: from transformers.testing_utils import (
  25      require_torch,

  29  )
  30: from transformers.utils import is_torch_available, is_vision_available
  31  

  45  
  46:     from transformers import CLIPSegForImageSegmentation, CLIPSegModel, CLIPSegTextModel, CLIPSegVisionModel
  47:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  48  

train_real_world/transformers_4573/tests/models/clipseg/test_processing_clipseg.py:
  20  
  21: from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES
  22: from transformers.testing_utils import require_vision
  23: from transformers.utils import is_vision_available
  24  

  28  if is_vision_available():
  29:     from transformers import CLIPSegProcessor
  30  

train_real_world/transformers_4573/tests/models/clvp/test_feature_extraction_clvp.py:
   23  
   24: from transformers import ClvpFeatureExtractor
   25: from transformers.testing_utils import (
   26      check_json_file_has_correct_format,

   31  )
   32: from transformers.utils.import_utils import is_torch_available
   33  

   42  
   43: # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.floats_list
   44  def floats_list(shape, scale=1.0, rng=None, name=None):

   94  
   95:     # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTester.prepare_inputs_for_common
   96      def prepare_inputs_for_common(self, equal_length=False, numpify=False):

  124  
  125:     # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTest.test_feat_extract_from_and_save_pretrained
  126      def test_feat_extract_from_and_save_pretrained(self):

  140  
  141:     # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTest.test_feat_extract_to_json_file
  142      def test_feat_extract_to_json_file(self):

  199  
  200:     # Copied from transformers.tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTest.test_double_precision_pad
  201      def test_double_precision_pad(self):

train_real_world/transformers_4573/tests/models/clvp/test_modeling_clvp.py:
  21  
  22: from transformers import ClvpConfig, ClvpDecoderConfig, ClvpEncoderConfig
  23: from transformers.testing_utils import (
  24      cleanup,

  29  )
  30: from transformers.utils import is_torch_available
  31  

  44  
  45:     from transformers import ClvpEncoder, ClvpForCausalLM, ClvpModel, ClvpModelForConditionalGeneration
  46  
  47: from transformers import ClvpFeatureExtractor, ClvpTokenizer
  48  

train_real_world/transformers_4573/tests/models/clvp/test_processing_clvp.py:
  20  
  21: from transformers import ClvpFeatureExtractor, ClvpProcessor, ClvpTokenizer
  22: from transformers.testing_utils import require_torch
  23  

  37  
  38:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.get_tokenizer with Whisper->Clvp
  39      def get_tokenizer(self, **kwargs):

  41  
  42:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.get_feature_extractor with Whisper->Clvp
  43      def get_feature_extractor(self, **kwargs):

  45  
  46:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_save_load_pretrained_default with Whisper->Clvp
  47      def test_save_load_pretrained_default(self):

  61  
  62:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_feature_extractor with Whisper->Clvp,processor(raw_speech->processor(raw_speech=raw_speech
  63      def test_feature_extractor(self):

  76  
  77:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer with Whisper->Clvp
  78      def test_tokenizer(self):

  92  
  93:     # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->Clvp
  94      def test_tokenizer_decode(self):

train_real_world/transformers_4573/tests/models/clvp/test_tokenization_clvp.py:
   19  
   20: from transformers import ClvpTokenizer
   21: from transformers.testing_utils import slow
   22  

   79  
   80:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_tokenizer with GPT2->Clvp
   81      @classmethod

   86  
   87:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_input_output_texts
   88      def get_input_output_texts(self, tokenizer):

   92  
   93:     # Copied from transformers.tests.models.layoutxlm.test_tokenization_layoutxlm.LayoutXLMTokenizationTest.test_add_special_tokens
   94      def test_add_special_tokens(self):

  109  
  110:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.test_rust_and_python_full_tokenizers
  111      def test_rust_and_python_full_tokenizers(self):

  140  
  141:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.test_padding
  142      def test_padding(self, max_length=15):

  188  
  189:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.test_padding_if_pad_token_set_slow
  190      def test_padding_if_pad_token_set_slow(self):

  240  
  241:     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.test_special_tokens_mask_input_pairs_and_bos_token
  242      def test_special_tokens_mask_input_pairs_and_bos_token(self):

train_real_world/transformers_4573/tests/models/code_llama/test_tokenization_code_llama.py:
  21  
  22: from transformers import CodeLlamaTokenizer
  23: from transformers.testing_utils import (
  24      get_tests_dir,

train_real_world/transformers_4573/tests/models/codegen/test_modeling_codegen.py:
  18  
  19: from transformers import CodeGenConfig, is_torch_available
  20: from transformers.testing_utils import backend_manual_seed, require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import AutoTokenizer, CodeGenForCausalLM, CodeGenModel
  32  

train_real_world/transformers_4573/tests/models/codegen/test_tokenization_codegen.py:
  3  from tests.test_tokenization_common import TokenizerTesterMixin
  4: from transformers.models.codegen.tokenization_codegen import CodeGenTokenizer
  5: from transformers.testing_utils import (
  6      require_tokenizers,

train_real_world/transformers_4573/tests/models/cohere/test_modeling_cohere.py:
  17  
  18: from transformers import CohereConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      require_bitsandbytes,

  35  
  36:     from transformers import AutoTokenizer, CohereForCausalLM, CohereModel
  37  
  38  
  39: # Copied from transformers.tests.models.llama.LlamaModelTester with Llama->Cohere
  40  class CohereModelTester:

train_real_world/transformers_4573/tests/models/cohere/test_tokenization_cohere.py:
  16  
  17: from transformers import CohereTokenizer
  18: from transformers.testing_utils import (
  19      require_jinja,

train_real_world/transformers_4573/tests/models/cohere2/test_modeling_cohere2.py:
   22  
   23: from transformers import AutoModelForCausalLM, AutoTokenizer, Cohere2Config, is_torch_available, pipeline
   24: from transformers.generation.configuration_utils import GenerationConfig
   25: from transformers.testing_utils import (
   26      Expectations,

   42  
   43:     from transformers import (
   44          Cohere2ForCausalLM,

  188  
  189:         from transformers.integrations.executorch import (
  190              TorchExportableModuleWithStaticCache,

train_real_world/transformers_4573/tests/models/cohere2_vision/test_image_processing_cohere2_vision.py:
  18  
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  21  

  31      if is_torchvision_available():
  32:         from transformers import Cohere2VisionImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/cohere2_vision/test_modeling_cohere2_vision.py:
  17  
  18: from transformers import (
  19      AutoProcessor,

  22  )
  23: from transformers.testing_utils import (
  24      Expectations,

  43  
  44:     from transformers import (
  45          Cohere2VisionForConditionalGeneration,

train_real_world/transformers_4573/tests/models/cohere2_vision/test_processing_cohere2_vision.py:
  16  
  17: from transformers import Cohere2VisionProcessor
  18: from transformers.testing_utils import require_read_token, require_vision
  19: from transformers.utils import is_torch_available, is_torchvision_available
  20  

train_real_world/transformers_4573/tests/models/colpali/test_modeling_colpali.py:
  25  from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor
  26: from transformers import (
  27      is_torch_available,
  28  )
  29: from transformers.models.colpali.configuration_colpali import ColPaliConfig
  30: from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliForRetrievalOutput
  31: from transformers.models.colpali.processing_colpali import ColPaliProcessor
  32: from transformers.testing_utils import (
  33      backend_empty_cache,

train_real_world/transformers_4573/tests/models/colpali/test_processing_colpali.py:
  19  
  20: from transformers.models.colpali.processing_colpali import ColPaliProcessor
  21: from transformers.testing_utils import get_tests_dir, require_torch, require_vision
  22: from transformers.utils import is_vision_available
  23  

  27  if is_vision_available():
  28:     from transformers import ColPaliProcessor, GemmaTokenizer
  29  

train_real_world/transformers_4573/tests/models/colqwen2/test_modeling_colqwen2.py:
  25  from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor
  26: from transformers import BitsAndBytesConfig, is_torch_available
  27: from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config
  28: from transformers.models.colqwen2.modeling_colqwen2 import ColQwen2ForRetrieval, ColQwen2ForRetrievalOutput
  29: from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor
  30: from transformers.testing_utils import (
  31      Expectations,

train_real_world/transformers_4573/tests/models/colqwen2/test_processing_colqwen2.py:
  21  
  22: from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor
  23: from transformers.testing_utils import get_tests_dir, require_torch, require_vision
  24: from transformers.utils import is_vision_available
  25  

  29  if is_vision_available():
  30:     from transformers import (
  31          ColQwen2Processor,

train_real_world/transformers_4573/tests/models/conditional_detr/test_image_processing_conditional_detr.py:
  21  
  22: from transformers.testing_utils import require_torch, require_vision, slow
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  33  
  34:     from transformers import ConditionalDetrImageProcessor
  35  
  36      if is_torchvision_available():
  37:         from transformers import ConditionalDetrImageProcessorFast
  38  

train_real_world/transformers_4573/tests/models/conditional_detr/test_modeling_conditional_detr.py:
  20  
  21: from transformers import ConditionalDetrConfig, ResNetConfig, is_torch_available, is_vision_available
  22: from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device
  23  

  31  
  32:     from transformers import (
  33          ConditionalDetrForObjectDetection,

  41  
  42:     from transformers import ConditionalDetrImageProcessor
  43  

train_real_world/transformers_4573/tests/models/convbert/test_modeling_convbert.py:
  17  
  18: from transformers import ConvBertConfig, is_torch_available
  19: from transformers.models.auto import get_values
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          MODEL_FOR_QUESTION_ANSWERING_MAPPING,

train_real_world/transformers_4573/tests/models/convnext/test_image_processing_convnext.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import ConvNextImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import ConvNextImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/convnext/test_modeling_convnext.py:
  18  
  19: from transformers import ConvNextConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import ConvNextBackbone, ConvNextForImageClassification, ConvNextModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/convnextv2/test_modeling_convnextv2.py:
  18  
  19: from transformers import ConvNextV2Config
  20: from transformers.models.auto import get_values
  21: from transformers.models.auto.modeling_auto import MODEL_FOR_BACKBONE_MAPPING_NAMES, MODEL_MAPPING_NAMES
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  32  
  33:     from transformers import ConvNextV2Backbone, ConvNextV2ForImageClassification, ConvNextV2Model
  34  

  38  
  39:     from transformers import AutoImageProcessor
  40  

train_real_world/transformers_4573/tests/models/cpm/test_tokenization_cpm.py:
  15  
  16: from transformers.models.cpm.tokenization_cpm import CpmTokenizer
  17: from transformers.testing_utils import custom_tokenizers
  18  

train_real_world/transformers_4573/tests/models/cpmant/test_modeling_cpmant.py:
  17  
  18: from transformers.testing_utils import is_torch_available, require_torch, tooslow
  19  

  28  
  29:     from transformers import (
  30          CpmAntConfig,

train_real_world/transformers_4573/tests/models/cpmant/test_tokenization_cpmant.py:
  19  
  20: from transformers.models.cpmant.tokenization_cpmant import VOCAB_FILES_NAMES, CpmAntTokenizer
  21: from transformers.testing_utils import require_rjieba, tooslow
  22  

train_real_world/transformers_4573/tests/models/csm/test_modeling_csm.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  28  )
  29: from transformers.testing_utils import (
  30      cleanup,

  35  )
  36: from transformers.utils.import_utils import is_datasets_available
  37  

train_real_world/transformers_4573/tests/models/csm/test_processing_csm.py:
  20  
  21: from transformers import CsmProcessor
  22: from transformers.testing_utils import require_torch
  23: from transformers.utils import is_torch_available
  24  

train_real_world/transformers_4573/tests/models/ctrl/test_modeling_ctrl.py:
  16  
  17: from transformers import CTRLConfig, is_torch_available
  18: from transformers.testing_utils import cleanup, require_torch, slow, torch_device
  19  

  28  
  29:     from transformers import (
  30          CTRLForSequenceClassification,

train_real_world/transformers_4573/tests/models/ctrl/test_tokenization_ctrl.py:
  18  
  19: from transformers.models.ctrl.tokenization_ctrl import VOCAB_FILES_NAMES, CTRLTokenizer
  20  

train_real_world/transformers_4573/tests/models/cvt/test_modeling_cvt.py:
  19  
  20: from transformers import CvtConfig
  21: from transformers.file_utils import is_torch_available, is_vision_available
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23  

  31  
  32:     from transformers import CvtForImageClassification, CvtModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/cwm/test_configuration_cwm.py:
  16  
  17: from transformers.models.cwm import CwmConfig
  18: from transformers.testing_utils import require_torch
  19  

train_real_world/transformers_4573/tests/models/cwm/test_modeling_cwm.py:
   16  
   17: from transformers import is_torch_available
   18: from transformers.testing_utils import (
   19      Expectations,

   34  
   35:     from transformers.models.cwm import (
   36          CwmConfig,

  101      def test_cwm_integration(self):
  102:         from transformers import AutoTokenizer
  103  

  147      def test_cwm_sliding_window_long_sequence(self):
  148:         from transformers import AutoTokenizer
  149  

  198      def test_cwm_generation_20_tokens(self):
  199:         from transformers import AutoTokenizer
  200  

train_real_world/transformers_4573/tests/models/d_fine/test_modeling_d_fine.py:
  24  
  25: from transformers import (
  26      DFineConfig,

  30  )
  31: from transformers.testing_utils import (
  32      require_torch,

  42  
  43:     from transformers import DFineForObjectDetection, DFineModel
  44  

  47  
  48: from transformers import RTDetrImageProcessor
  49  

train_real_world/transformers_4573/tests/models/dab_detr/test_modeling_dab_detr.py:
  21  
  22: from transformers import DabDetrConfig, ResNetConfig, is_torch_available, is_vision_available
  23: from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device
  24  

  34  
  35:     from transformers import (
  36          DabDetrForObjectDetection,

  43  
  44:     from transformers import ConditionalDetrImageProcessor
  45  

train_real_world/transformers_4573/tests/models/dac/test_feature_extraction_dac.py:
   21  
   22: from transformers import DacFeatureExtractor
   23: from transformers.testing_utils import require_torch
   24: from transformers.utils.import_utils import is_torch_available
   25  

   51  @require_torch
   52: # Copied from transformers.tests.encodec.test_feature_extraction_dac.EncodecFeatureExtractionTester with Encodec->Dac
   53  class DacFeatureExtractionTester:

  104  @require_torch
  105: # Copied from transformers.tests.encodec.test_feature_extraction_dac.EnCodecFeatureExtractionTest with Encodec->Dac
  106  class DacFeatureExtractionTest(SequenceFeatureExtractionTestMixin, unittest.TestCase):

train_real_world/transformers_4573/tests/models/dac/test_modeling_dac.py:
   22  
   23: from transformers import AutoProcessor, DacConfig, DacModel
   24: from transformers.testing_utils import (
   25      is_torch_available,

   41  @require_torch
   42: # Copied from transformers.tests.encodec.test_modeling_encodec.EncodecModelTester with Encodec->Dac
   43  class DacModelTester:

  120  @require_torch
  121: # Copied from transformers.tests.encodec.test_modeling_encodec.EncodecModelTest with Encodec->Dac
  122  class DacModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):

  284  
  285: # Copied from transformers.tests.encodec.test_modeling_encodec.normalize
  286  def normalize(arr):

  291  
  292: # Copied from transformers.tests.encodec.test_modeling_encodec.compute_rmse
  293  def compute_rmse(arr1, arr2):

train_real_world/transformers_4573/tests/models/data2vec/test_modeling_data2vec_audio.py:
  22  from tests.test_modeling_common import floats_tensor, ids_tensor, random_attention_mask
  23: from transformers import Data2VecAudioConfig, is_torch_available
  24: from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device
  25  

  33  
  34:     from transformers import (
  35          Data2VecAudioForAudioFrameClassification,

  41      )
  42:     from transformers.models.data2vec.modeling_data2vec_audio import _compute_mask_indices
  43  

train_real_world/transformers_4573/tests/models/data2vec/test_modeling_data2vec_text.py:
  20  from tests.test_modeling_common import floats_tensor, ids_tensor, random_attention_mask
  21: from transformers import Data2VecTextConfig, is_torch_available
  22: from transformers.testing_utils import TestCasePlus, require_torch, slow, torch_device
  23  

  32  
  33:     from transformers import (
  34          Data2VecTextForCausalLM,

  42      )
  43:     from transformers.models.data2vec.modeling_data2vec_text import Data2VecTextEmbeddings
  44  

train_real_world/transformers_4573/tests/models/data2vec/test_modeling_data2vec_vision.py:
  20  
  21: from transformers import Data2VecVisionConfig
  22: from transformers.testing_utils import (
  23      require_torch,

  28  )
  29: from transformers.utils import (
  30      is_torch_available,

  42  
  43:     from transformers import (
  44          Data2VecVisionForImageClassification,

  47      )
  48:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  49  

  53  
  54:     from transformers import BeitImageProcessor
  55  

train_real_world/transformers_4573/tests/models/dbrx/test_modeling_dbrx.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import require_torch, slow
  20  

  26  
  27:     from transformers import DbrxForCausalLM, DbrxModel
  28  

train_real_world/transformers_4573/tests/models/deberta/test_modeling_deberta.py:
  15  
  16: from transformers import DebertaConfig, is_torch_available
  17: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  18  

  26  
  27:     from transformers import (
  28          DebertaForMaskedLM,

train_real_world/transformers_4573/tests/models/deberta/test_tokenization_deberta.py:
  17  
  18: from transformers import DebertaTokenizer
  19  

train_real_world/transformers_4573/tests/models/deberta_v2/test_modeling_deberta_v2.py:
  15  
  16: from transformers import DebertaV2Config, is_torch_available
  17: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  18  

  26  
  27:     from transformers import (
  28          DebertaV2ForMaskedLM,

train_real_world/transformers_4573/tests/models/deberta_v2/test_tokenization_deberta_v2.py:
  16  
  17: from transformers import DebertaV2Tokenizer
  18: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  19: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  20  

train_real_world/transformers_4573/tests/models/decision_transformer/test_modeling_decision_transformer.py:
  18  
  19: from transformers import DecisionTransformerConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import DecisionTransformerModel
  31  

train_real_world/transformers_4573/tests/models/deepseek_v2/test_modeling_deepseek_v2.py:
  20  
  21: from transformers import BitsAndBytesConfig, Cache, is_torch_available
  22: from transformers.testing_utils import require_read_token, require_torch, require_torch_accelerator, slow, torch_device
  23  

  29  
  30:     from transformers import AutoTokenizer, DeepseekV2ForCausalLM, DeepseekV2Model
  31:     from transformers.models.deepseek_v2.modeling_deepseek_v2 import DeepseekV2RotaryEmbedding
  32  

train_real_world/transformers_4573/tests/models/deepseek_v3/test_modeling_deepseek_v3.py:
  21  
  22: from transformers import AutoTokenizer, DeepseekV3Config, is_torch_available
  23: from transformers.testing_utils import (
  24      cleanup,

  41  
  42:     from transformers import (
  43          Cache,

train_real_world/transformers_4573/tests/models/deepseek_vl/test_image_processing_deepseek_vl.py:
  18  
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  21  

  29  if is_vision_available():
  30:     from transformers import DeepseekVLImageProcessor
  31  
  32      if is_torchvision_available():
  33:         from transformers import DeepseekVLImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/deepseek_vl/test_modeling_deepseek_vl.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      require_torch,

train_real_world/transformers_4573/tests/models/deepseek_vl/test_processing_deepseek_vl.py:
  16  
  17: from transformers import DeepseekVLProcessor
  18: from transformers.testing_utils import get_tests_dir
  19  

train_real_world/transformers_4573/tests/models/deepseek_vl_hybrid/test_image_processing_deepseek_vl_hybrid.py:
  19  
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  33  
  34:     from transformers import DeepseekVLHybridImageProcessor
  35  
  36      if is_torchvision_available():
  37:         from transformers import DeepseekVLHybridImageProcessorFast
  38  

train_real_world/transformers_4573/tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      require_torch,

train_real_world/transformers_4573/tests/models/deepseek_vl_hybrid/test_processing_deepseek_vl_hybrid.py:
  16  
  17: from transformers import DeepseekVLHybridProcessor
  18: from transformers.testing_utils import get_tests_dir
  19  

train_real_world/transformers_4573/tests/models/deformable_detr/test_image_processing_deformable_detr.py:
  21  
  22: from transformers.testing_utils import (
  23      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  30  

  39  
  40:     from transformers import DeformableDetrImageProcessor, DeformableDetrImageProcessorFast
  41  

train_real_world/transformers_4573/tests/models/deformable_detr/test_modeling_deformable_detr.py:
  20  
  21: from transformers import DeformableDetrConfig, ResNetConfig, is_torch_available, is_vision_available
  22: from transformers.testing_utils import (
  23      require_timm,

  39  
  40:     from transformers import DeformableDetrForObjectDetection, DeformableDetrModel
  41  

  45  
  46:     from transformers import AutoImageProcessor
  47  

train_real_world/transformers_4573/tests/models/deit/test_image_processing_deit.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import DeiTImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import DeiTImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/deit/test_modeling_deit.py:
  19  
  20: from transformers import DeiTConfig
  21: from transformers.testing_utils import (
  22      require_accelerate,

  29  )
  30: from transformers.utils import is_torch_available, is_vision_available
  31  

  40  
  41:     from transformers import (
  42          DeiTForImageClassification,

  46      )
  47:     from transformers.models.auto.modeling_auto import (
  48          MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,

  56  
  57:     from transformers import DeiTImageProcessor
  58  

train_real_world/transformers_4573/tests/models/depth_anything/test_modeling_depth_anything.py:
  19  
  20: from transformers import DepthAnythingConfig, Dinov2Config
  21: from transformers.file_utils import is_torch_available, is_vision_available
  22: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24: from transformers.utils.import_utils import get_torch_major_and_minor_version
  25  

  33  
  34:     from transformers import DepthAnythingForDepthEstimation
  35  

  39  
  40:     from transformers import DPTImageProcessor
  41  

train_real_world/transformers_4573/tests/models/depth_pro/test_image_processing_depth_pro.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import DepthProImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import DepthProImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/depth_pro/test_modeling_depth_pro.py:
  19  
  20: from transformers import DepthProConfig
  21: from transformers.file_utils import is_torch_available, is_vision_available
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23  

  32  
  33:     from transformers import DepthProForDepthEstimation, DepthProModel
  34:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  35  

  39  
  40:     from transformers import DepthProImageProcessor
  41  

train_real_world/transformers_4573/tests/models/detr/test_image_processing_detr.py:
  20  
  21: from transformers.testing_utils import (
  22      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  30  

  39  
  40:     from transformers import DetrImageProcessor
  41  
  42      if is_torchvision_available():
  43:         from transformers import DetrImageProcessorFast
  44  

train_real_world/transformers_4573/tests/models/detr/test_modeling_detr.py:
  20  
  21: from transformers import DetrConfig, ResNetConfig, is_torch_available, is_vision_available
  22: from transformers.testing_utils import Expectations, require_timm, require_torch, require_vision, slow, torch_device
  23  

  31  
  32:     from transformers import DetrForObjectDetection, DetrForSegmentation, DetrModel
  33  

  37  
  38:     from transformers import DetrImageProcessor
  39  

train_real_world/transformers_4573/tests/models/dia/test_feature_extraction_dia.py:
  21  
  22: from transformers import DiaFeatureExtractor
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils.import_utils import is_torch_available
  25  

train_real_world/transformers_4573/tests/models/dia/test_modeling_dia.py:
  22  
  23: from transformers.models.dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig
  24: from transformers.testing_utils import (
  25      cleanup,

  31  )
  32: from transformers.utils import is_soundfile_available, is_torch_available, is_torchaudio_available
  33: from transformers.utils.import_utils import is_datasets_available
  34  

  46  
  47:     from transformers import (
  48          DiaForConditionalGeneration,

  53      )
  54:     from transformers.cache_utils import (
  55          StaticCache,
  56      )
  57:     from transformers.models.dia.modeling_dia import DiaDecoder, DiaEncoder
  58  

train_real_world/transformers_4573/tests/models/dia/test_processing_dia.py:
  21  
  22: from transformers import DacModel, DiaFeatureExtractor, DiaProcessor, DiaTokenizer
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils import is_torch_available
  25  

train_real_world/transformers_4573/tests/models/dia/test_tokenization_dia.py:
  16  
  17: from transformers.models.dia import DiaTokenizer
  18: from transformers.testing_utils import slow
  19  

train_real_world/transformers_4573/tests/models/diffllama/test_modeling_diffllama.py:
  22  
  23: from transformers import AutoTokenizer, BitsAndBytesConfig, DiffLlamaConfig, StaticCache, is_torch_available
  24: from transformers.testing_utils import (
  25      backend_empty_cache,

  44  
  45:     from transformers import (
  46          DiffLlamaForCausalLM,

train_real_world/transformers_4573/tests/models/dinat/test_modeling_dinat.py:
  19  
  20: from transformers import DinatConfig
  21: from transformers.testing_utils import require_natten, require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  33  
  34:     from transformers import DinatBackbone, DinatForImageClassification, DinatModel
  35  

  38  
  39:     from transformers import AutoImageProcessor
  40  

train_real_world/transformers_4573/tests/models/dinov2/test_modeling_dinov2.py:
  18  
  19: from transformers import Dinov2Config
  20: from transformers.testing_utils import (
  21      require_torch,

  25  )
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  37  
  38:     from transformers import Dinov2Backbone, Dinov2ForImageClassification, Dinov2Model
  39  

  43  
  44:     from transformers import AutoImageProcessor
  45  

train_real_world/transformers_4573/tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py:
  18  
  19: from transformers import Dinov2WithRegistersConfig
  20: from transformers.testing_utils import (
  21      require_torch,

  25  )
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  37  
  38:     from transformers import (
  39          Dinov2WithRegistersBackbone,

  47  
  48:     from transformers import AutoImageProcessor
  49  

train_real_world/transformers_4573/tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py:
  18  
  19: from transformers import DINOv3ConvNextConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import DINOv3ConvNextBackbone, DINOv3ConvNextModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/dinov3_vit/test_image_processing_dinov3_vit_fast.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available
  20  

  24  if is_torchvision_available():
  25:     from transformers import DINOv3ViTImageProcessorFast
  26  

train_real_world/transformers_4573/tests/models/dinov3_vit/test_modeling_dinov3_vit.py:
  18  
  19: from transformers import DINOv3ViTConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import DINOv3ViTBackbone, DINOv3ViTModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/distilbert/test_modeling_distilbert.py:
  18  
  19: from transformers import DistilBertConfig, is_torch_available
  20: from transformers.testing_utils import require_flash_attn, require_torch, require_torch_accelerator, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          AutoTokenizer,

  38      )
  39:     from transformers.models.distilbert.modeling_distilbert import _create_sinusoidal_embeddings
  40:     from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  41  

train_real_world/transformers_4573/tests/models/distilbert/test_tokenization_distilbert.py:
  15  
  16: from transformers import AutoTokenizer
  17: from transformers.models.distilbert.tokenization_distilbert import DistilBertTokenizer
  18: from transformers.testing_utils import require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/dit/test_modeling_dit.py:
  16  
  17: from transformers import is_torch_available, is_vision_available
  18: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  19  

  23  
  24:     from transformers import AutoModelForImageClassification
  25  
  26  if is_vision_available():
  27:     from transformers import AutoImageProcessor
  28  

train_real_world/transformers_4573/tests/models/doge/test_modeling_doge.py:
  18  
  19: from transformers import AutoTokenizer, DogeConfig, is_torch_available, set_seed
  20: from transformers.testing_utils import (
  21      require_read_token,

  36  
  37:     from transformers import (
  38          DogeForCausalLM,

train_real_world/transformers_4573/tests/models/donut/test_image_processing_donut.py:
  19  
  20: from transformers.testing_utils import is_flaky, require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import DonutImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import DonutImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/donut/test_modeling_donut_swin.py:
  18  
  19: from transformers import DonutSwinConfig
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21: from transformers.utils import is_torch_available
  22  

  31  
  32:     from transformers import DonutSwinForImageClassification, DonutSwinModel
  33  

train_real_world/transformers_4573/tests/models/donut/test_processing_donut.py:
  17  
  18: from transformers import DonutProcessor
  19  

train_real_world/transformers_4573/tests/models/dots1/test_modeling_dots1.py:
  18  
  19: from transformers import AutoTokenizer, is_torch_available
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

  34  
  35:     from transformers import (
  36          Dots1ForCausalLM,

train_real_world/transformers_4573/tests/models/dpr/test_modeling_dpr.py:
  18  
  19: from transformers import DPRConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import DPRContextEncoder, DPRQuestionEncoder, DPRReader, DPRReaderTokenizer
  31  

train_real_world/transformers_4573/tests/models/dpr/test_tokenization_dpr.py:
  14  
  15: from transformers import (
  16      DPRContextEncoderTokenizer,

  23  )
  24: from transformers.testing_utils import require_tokenizers, slow
  25: from transformers.tokenization_utils_base import BatchEncoding
  26  

train_real_world/transformers_4573/tests/models/dpt/test_image_processing_dpt.py:
   20  
   21: from transformers.file_utils import is_torch_available, is_vision_available
   22: from transformers.testing_utils import require_torch, require_vision
   23: from transformers.utils import is_torchvision_available
   24  

   31  if is_vision_available():
   32:     from transformers import DPTImageProcessor
   33  
   34      if is_torchvision_available():
   35:         from transformers import DPTImageProcessorFast
   36  

   92  
   93: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
   94  def prepare_semantic_single_inputs():

   99  
  100: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  101  def prepare_semantic_batch_inputs():

  178  
  179:     # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_call_segmentation_maps
  180      def test_call_segmentation_maps(self):

train_real_world/transformers_4573/tests/models/dpt/test_modeling_dpt_auto_backbone.py:
  17  
  18: from transformers import Dinov2Config, DPTConfig
  19: from transformers.file_utils import is_torch_available, is_vision_available
  20: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  21: from transformers.utils.import_utils import get_torch_major_and_minor_version
  22  

  30  
  31:     from transformers import DPTForDepthEstimation
  32:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  33  

  37  
  38:     from transformers import DPTImageProcessor
  39  

train_real_world/transformers_4573/tests/models/dpt/test_modeling_dpt_hybrid.py:
  17  
  18: from transformers import DPTConfig
  19: from transformers.file_utils import is_torch_available, is_vision_available
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21  

  30  
  31:     from transformers import DPTForDepthEstimation, DPTForSemanticSegmentation, DPTModel
  32:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  33  

  37  
  38:     from transformers import DPTImageProcessor
  39  

train_real_world/transformers_4573/tests/models/dpt/test_modeling_dpt.py:
  19  
  20: from transformers import DPTConfig
  21: from transformers.file_utils import is_torch_available, is_vision_available
  22: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  23: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  24  

  33  
  34:     from transformers import DPTForDepthEstimation, DPTForSemanticSegmentation, DPTModel
  35:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  36  

  40  
  41:     from transformers import DPTImageProcessor
  42  

train_real_world/transformers_4573/tests/models/edgetam/test_modeling_edgetam.py:
  22  
  23: from transformers import (
  24      EdgeTamConfig,

  30  )
  31: from transformers.testing_utils import (
  32      backend_empty_cache,

  36  )
  37: from transformers.utils import is_torch_available, is_vision_available
  38: from transformers.video_utils import load_video
  39  

  47  
  48:     from transformers import AutoConfig, EdgeTamModel, Sam2Processor
  49  

train_real_world/transformers_4573/tests/models/edgetam_video/test_modeling_edgetam_video.py:
  21  
  22: from transformers.testing_utils import (
  23      backend_empty_cache,

  26  )
  27: from transformers.utils import is_torch_available, is_vision_available
  28: from transformers.video_utils import load_video
  29  

  33  
  34:     from transformers import EdgeTamVideoModel, Sam2VideoProcessor
  35  

train_real_world/transformers_4573/tests/models/efficientloftr/test_image_processing_efficientloftr.py:
  21  
  22: from transformers.testing_utils import (
  23      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  30  

  36  
  37:     from transformers.models.efficientloftr.modeling_efficientloftr import EfficientLoFTRKeypointMatchingOutput
  38  
  39  if is_vision_available():
  40:     from transformers import EfficientLoFTRImageProcessor
  41  
  42      if is_torchvision_available():
  43:         from transformers import EfficientLoFTRImageProcessorFast
  44  

train_real_world/transformers_4573/tests/models/efficientloftr/test_modeling_efficientloftr.py:
  19  
  20: from transformers.models.efficientloftr import EfficientLoFTRConfig, EfficientLoFTRModel
  21: from transformers.testing_utils import (
  22      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_vision_available
  30  

  37  
  38:     from transformers import EfficientLoFTRForKeypointMatching
  39  
  40  if is_vision_available():
  41:     from transformers import AutoImageProcessor
  42  

train_real_world/transformers_4573/tests/models/efficientnet/test_image_processing_efficientnet.py:
  19  
  20: from transformers.image_utils import PILImageResampling
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import (
  23      is_torch_available,

  34  if is_vision_available():
  35:     from transformers import EfficientNetImageProcessor
  36  
  37      if is_torchvision_available():
  38:         from transformers import EfficientNetImageProcessorFast
  39  

train_real_world/transformers_4573/tests/models/efficientnet/test_modeling_efficientnet.py:
  18  
  19: from transformers import EfficientNetConfig
  20: from transformers.testing_utils import is_pipeline_test, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  30  
  31:     from transformers import EfficientNetForImageClassification, EfficientNetModel
  32  

  36  
  37:     from transformers import AutoImageProcessor
  38  

train_real_world/transformers_4573/tests/models/electra/test_modeling_electra.py:
  19  
  20: from transformers import ElectraConfig, is_torch_available
  21: from transformers.models.auto import get_values
  22: from transformers.testing_utils import require_torch, slow, torch_device
  23  

  32  
  33:     from transformers import (
  34          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/emu3/test_modeling_emu3.py:
  22  
  23: from transformers import BitsAndBytesConfig, Emu3Config, Emu3TextConfig, is_torch_available, is_vision_available
  24: from transformers.testing_utils import (
  25      Expectations,

  44  
  45:     from transformers import (
  46          Emu3ForCausalLM,

train_real_world/transformers_4573/tests/models/emu3/test_processing_emu3.py:
  19  
  20: from transformers import Emu3Processor
  21  

train_real_world/transformers_4573/tests/models/encodec/test_feature_extraction_encodec.py:
  21  
  22: from transformers import EncodecFeatureExtractor
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils.import_utils import is_torch_available
  25  

train_real_world/transformers_4573/tests/models/encodec/test_modeling_encodec.py:
  24  
  25: from transformers import AutoProcessor, EncodecConfig
  26: from transformers.testing_utils import (
  27      is_torch_available,

  40  
  41:     from transformers import EncodecFeatureExtractor, EncodecModel
  42  

train_real_world/transformers_4573/tests/models/encoder_decoder/test_modeling_encoder_decoder.py:
  18  
  19: from transformers import is_torch_available, logging
  20: from transformers.testing_utils import (
  21      CaptureLogger,

  41  
  42:     from transformers import (
  43          AutoConfig,

  57      )
  58:     from transformers.modeling_outputs import BaseModelOutput
  59  

train_real_world/transformers_4573/tests/models/eomt/test_image_processing_eomt.py:
  20  
  21: from transformers.image_utils import load_image
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  34  
  35:     from transformers import EomtImageProcessor
  36  
  37      if is_torchvision_available():
  38:         from transformers import EomtImageProcessorFast
  39:     from transformers.models.eomt.modeling_eomt import EomtForUniversalSegmentationOutput
  40  

train_real_world/transformers_4573/tests/models/eomt/test_modeling_eomt.py:
  19  
  20: from transformers import AutoImageProcessor, EomtConfig, EomtForUniversalSegmentation, pipeline
  21: from transformers.testing_utils import require_torch, require_torch_accelerator, require_torch_fp16, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

train_real_world/transformers_4573/tests/models/ernie/test_modeling_ernie.py:
  17  
  18: from transformers import ErnieConfig, is_torch_available
  19: from transformers.models.auto import get_values
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/ernie4_5/test_modeling_ernie4_5.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  33  
  34:     from transformers import (
  35          AutoTokenizer,

train_real_world/transformers_4573/tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py:
  20  
  21: from transformers import BitsAndBytesConfig, is_torch_available
  22: from transformers.testing_utils import (
  23      cleanup,

  37  
  38:     from transformers import (
  39          AutoTokenizer,

train_real_world/transformers_4573/tests/models/ernie4_5_vl_moe/test_image_processing_ernie4_5_vl_moe.py:
  20  
  21: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, load_image
  22: from transformers.models.ernie4_5_vl_moe.image_processing_ernie4_5_vl_moe import smart_resize
  23: from transformers.testing_utils import require_torch, require_vision
  24: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  25  

  35  
  36:     from transformers import Ernie4_5_VL_MoeImageProcessor
  37  
  38      if is_torchvision_available():
  39:         from transformers import Ernie4_5_VL_MoeImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/ernie4_5_vl_moe/test_modeling_ernie4_5_vl_moe.py:
  17  
  18: from transformers import (
  19      AutoModelForImageTextToText,

  26  )
  27: from transformers.testing_utils import (
  28      cleanup,

  33  )
  34: from transformers.utils import is_cv2_available
  35  

train_real_world/transformers_4573/tests/models/ernie4_5_vl_moe/test_processing_ernie4_5_vl_moe.py:
  22  
  23: from transformers import AutoProcessor, LlamaTokenizerFast
  24: from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision
  25: from transformers.utils import is_torch_available, is_vision_available
  26  

  30  if is_vision_available():
  31:     from transformers import Ernie4_5_VL_MoeImageProcessorFast, Ernie4_5_VL_MoeProcessor
  32  

train_real_world/transformers_4573/tests/models/ernie4_5_vl_moe/test_video_processing_ernie4_5_vl_moe.py:
  19  
  20: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  31      if is_torchvision_available():
  32:         from transformers import Ernie4_5_VL_MoeVideoProcessor
  33:         from transformers.models.ernie4_5_vl_moe.video_processing_ernie4_5_vl_moe import smart_resize
  34  

train_real_world/transformers_4573/tests/models/esm/test_modeling_esm.py:
  20  
  21: from transformers import BitsAndBytesConfig, EsmConfig, is_torch_available
  22: from transformers.testing_utils import (
  23      TestCasePlus,

  40  
  41:     from transformers import EsmForMaskedLM, EsmForSequenceClassification, EsmForTokenClassification, EsmModel
  42:     from transformers.models.esm.modeling_esm import (
  43          EsmEmbeddings,

train_real_world/transformers_4573/tests/models/esm/test_modeling_esmfold.py:
  17  
  18: from transformers import EsmConfig, is_torch_available
  19: from transformers.testing_utils import TestCasePlus, is_flaky, require_torch, slow, torch_device
  20  

  28  
  29:     from transformers.models.esm.modeling_esmfold import EsmForProteinFolding
  30  

train_real_world/transformers_4573/tests/models/esm/test_tokenization_esm.py:
  19  
  20: from transformers.models.esm.tokenization_esm import VOCAB_FILES_NAMES, EsmTokenizer
  21: from transformers.testing_utils import require_tokenizers
  22: from transformers.tokenization_python import PreTrainedTokenizer
  23: from transformers.tokenization_utils_base import PreTrainedTokenizerBase
  24  

train_real_world/transformers_4573/tests/models/evolla/test_modeling_evolla.py:
  21  
  22: from transformers import BitsAndBytesConfig, EvollaConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      TestCasePlus,

  43  
  44:     from transformers import EvollaForProteinText2Text, EvollaModel, EvollaProcessor
  45  

train_real_world/transformers_4573/tests/models/evolla/test_processing_evolla.py:
  17  
  18: from transformers import (
  19      AutoProcessor,

  21  )
  22: from transformers.testing_utils import require_torch
  23: from transformers.utils import is_torch_available
  24  

train_real_world/transformers_4573/tests/models/exaone4/test_modeling_exaone4.py:
   21  
   22: from transformers import (
   23      AutoTokenizer,

   26  )
   27: from transformers.testing_utils import (
   28      cleanup,

   41  
   42:     from transformers import (
   43          Exaone4ForCausalLM,

  158  
  159:         from transformers.integrations.executorch import (
  160              TorchExportableModuleWithStaticCache,

train_real_world/transformers_4573/tests/models/falcon/test_modeling_falcon.py:
  17  
  18: from transformers import (
  19      AutoModelForCausalLM,

  24  )
  25: from transformers.testing_utils import (
  26      require_bitsandbytes,

  37  
  38:     from transformers import (
  39          FalconForCausalLM,

train_real_world/transformers_4573/tests/models/falcon_h1/test_modeling_falcon_h1.py:
  20  
  21: from transformers import FalconH1Config, is_torch_available
  22: from transformers.testing_utils import (
  23      Expectations,

  39  
  40:     from transformers import AutoTokenizer, FalconH1ForCausalLM, FalconH1Model
  41:     from transformers.models.falcon_h1.modeling_falcon_h1 import (
  42          FalconHybridMambaAttentionDynamicCache,

train_real_world/transformers_4573/tests/models/falcon_mamba/test_modeling_falcon_mamba.py:
   20  
   21: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, FalconMambaConfig, is_torch_available
   22: from transformers.testing_utils import (
   23      Expectations,

   43  
   44:     from transformers import FalconMambaForCausalLM, FalconMambaModel
   45:     from transformers.models.falcon_mamba.modeling_falcon_mamba import FalconMambaCache
   46  
   47  
   48: # Copied from transformers.tests.models.mamba.MambaModelTester with Mamba->FalconMamba,mamba->falcon_mamba
   49  class FalconMambaModelTester:

  261  @require_torch
  262: # Copied from transformers.tests.models.mamba.MambaModelTest with Mamba->Falcon,mamba->falcon_mamba,FalconMambaCache->MambaCache
  263  class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):

train_real_world/transformers_4573/tests/models/fast_vlm/test_modeling_fast_vlm.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  28  )
  29: from transformers.testing_utils import (
  30      cleanup,

train_real_world/transformers_4573/tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py:
  19  
  20: from transformers import (
  21      FastSpeech2ConformerConfig,

  26  )
  27: from transformers.testing_utils import (
  28      Expectations,

  41  
  42:     from transformers import FastSpeech2ConformerModel, FastSpeech2ConformerWithHifiGan, set_seed
  43  

train_real_world/transformers_4573/tests/models/fastspeech2_conformer/test_tokenization_fastspeech2_conformer.py:
  17  
  18: from transformers.models.fastspeech2_conformer import FastSpeech2ConformerTokenizer
  19: from transformers.testing_utils import require_g2p_en, slow
  20  

train_real_world/transformers_4573/tests/models/flaubert/test_modeling_flaubert.py:
  15  
  16: from transformers import FlaubertConfig, is_sacremoses_available, is_torch_available
  17: from transformers.testing_utils import require_torch, slow, torch_device
  18  

  26  
  27:     from transformers import (
  28          FlaubertForMultipleChoice,

  35      )
  36:     from transformers.models.flaubert.modeling_flaubert import create_sinusoidal_embeddings
  37  

train_real_world/transformers_4573/tests/models/flaubert/test_tokenization_flaubert.py:
  20  
  21: from transformers import FlaubertTokenizer
  22: from transformers.models.flaubert.tokenization_flaubert import VOCAB_FILES_NAMES
  23: from transformers.testing_utils import slow
  24  

  32  
  33:     # Copied from transformers.tests.models.xlm.test_tokenization_xlm.XLMTokenizationTest.test_full_tokenizer
  34      def test_full_tokenizer(self):

  60      @slow
  61:     # Copied from transformers.tests.models.xlm.test_tokenization_xlm.XLMTokenizationTest.test_sequence_builders
  62      def test_sequence_builders(self):

train_real_world/transformers_4573/tests/models/flava/test_image_processing_flava.py:
  21  
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  33  
  34:     from transformers import FlavaImageProcessor
  35  
  36      if is_torchvision_available():
  37:         from transformers import FlavaImageProcessorFast
  38:     from transformers.image_utils import PILImageResampling
  39:     from transformers.models.flava.image_processing_flava import (
  40          FLAVA_CODEBOOK_MEAN,

train_real_world/transformers_4573/tests/models/flava/test_modeling_flava.py:
  23  
  24: from transformers import (
  25      FlavaConfig,

  30  )
  31: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  47  
  48:     from transformers import (
  49          FlavaForPreTraining,

  64  
  65:     from transformers import FlavaProcessor
  66  

train_real_world/transformers_4573/tests/models/flava/test_processing_flava.py:
  17  
  18: from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES
  19: from transformers.testing_utils import require_vision
  20: from transformers.utils import is_vision_available
  21  

  25  if is_vision_available():
  26:     from transformers import FlavaProcessor
  27:     from transformers.models.flava.image_processing_flava import (
  28          FLAVA_CODEBOOK_MEAN,

train_real_world/transformers_4573/tests/models/flex_olmo/test_modeling_flex_olmo.py:
  20  
  21: from transformers import is_torch_available
  22: from transformers.models.auto.tokenization_auto import AutoTokenizer
  23: from transformers.testing_utils import (
  24      Expectations,

  36  
  37:     from transformers import (
  38          FlexOlmoForCausalLM,

train_real_world/transformers_4573/tests/models/florence2/test_modeling_florence2.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  28  )
  29: from transformers.testing_utils import (
  30      cleanup,

train_real_world/transformers_4573/tests/models/florence2/test_processing_florence2.py:
  15  
  16: from transformers import Florence2Processor
  17: from transformers.testing_utils import require_torch, require_vision
  18: from transformers.utils import is_torch_available
  19  

train_real_world/transformers_4573/tests/models/fnet/test_modeling_fnet.py:
  17  
  18: from transformers import FNetConfig, is_torch_available
  19: from transformers.models.auto import get_values
  20: from transformers.testing_utils import require_tokenizers, require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/focalnet/test_modeling_focalnet.py:
  19  
  20: from transformers import FocalNetConfig
  21: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  33  
  34:     from transformers import (
  35          FocalNetBackbone,

  43  
  44:     from transformers import AutoImageProcessor
  45  

train_real_world/transformers_4573/tests/models/fsmt/test_modeling_fsmt.py:
  21  
  22: from transformers import FSMTConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      require_sentencepiece,

  41  
  42:     from transformers import FSMTForConditionalGeneration, FSMTModel, FSMTTokenizer
  43:     from transformers.models.fsmt.modeling_fsmt import (
  44          SinusoidalPositionalEmbedding,

  48      )
  49:     from transformers.pipelines import TranslationPipeline
  50  

train_real_world/transformers_4573/tests/models/fsmt/test_tokenization_fsmt.py:
  20  
  21: from transformers.models.fsmt.tokenization_fsmt import VOCAB_FILES_NAMES, FSMTTokenizer
  22: from transformers.testing_utils import slow
  23  

train_real_world/transformers_4573/tests/models/funnel/test_modeling_funnel.py:
  17  
  18: from transformers import FunnelConfig, FunnelTokenizer, is_torch_available
  19: from transformers.models.auto import get_values
  20: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/funnel/test_tokenization_funnel.py:
  17  
  18: from transformers import FunnelTokenizer
  19: from transformers.testing_utils import require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/fuyu/test_image_processing_fuyu.py:
    8  
    9: from transformers.image_utils import SizeDict
   10: from transformers.testing_utils import (
   11      require_torch,

   17  )
   18: from transformers.utils import is_torch_available, is_vision_available
   19  

   25  
   26:     from transformers import FuyuImageProcessor, FuyuImageProcessorFast
   27  

  388  
  389:         from transformers.image_utils import SizeDict
  390  

train_real_world/transformers_4573/tests/models/fuyu/test_modeling_fuyu.py:
  25  
  26: from transformers import FuyuConfig, is_torch_available, is_vision_available
  27: from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device
  28  

  38  if is_torch_available() and is_vision_available():
  39:     from transformers import FuyuProcessor
  40  

  42  if is_torch_available():
  43:     from transformers import FuyuForCausalLM, FuyuModel
  44  

train_real_world/transformers_4573/tests/models/fuyu/test_processing_fuyu.py:
   2  
   3: from transformers import (
   4      FuyuImageProcessor,

   7  )
   8: from transformers.image_utils import load_image
   9: from transformers.testing_utils import require_torch, require_vision
  10  

  16  
  17:     from transformers.models.fuyu.processing_fuyu import construct_full_unpacked_stream, full_unpacked_stream_to_tensor
  18  

train_real_world/transformers_4573/tests/models/gemma/test_modeling_gemma.py:
   20  
   21: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, is_torch_available
   22: from transformers.generation.configuration_utils import GenerationConfig
   23: from transformers.testing_utils import (
   24      DeviceProperties,

   42  
   43:     from transformers import (
   44          GemmaForCausalLM,

  378  
  379:         from transformers.integrations.executorch import (
  380              TorchExportableModuleWithStaticCache,

  438          # Static Cache + export
  439:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  440  

train_real_world/transformers_4573/tests/models/gemma/test_tokenization_gemma.py:
  18  from tests.test_tokenization_common import TokenizerTesterMixin
  19: from transformers.models.gemma.tokenization_gemma import GemmaTokenizer
  20: from transformers.testing_utils import (
  21      require_read_token,

train_real_world/transformers_4573/tests/models/gemma2/test_modeling_gemma2.py:
   22  
   23: from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, is_torch_available, pipeline
   24: from transformers.cache_utils import DynamicLayer, DynamicSlidingWindowLayer
   25: from transformers.generation.configuration_utils import GenerationConfig
   26: from transformers.testing_utils import (
   27      Expectations,

   46  
   47:     from transformers import (
   48          Gemma2Model,

  211  
  212:         from transformers.integrations.executorch import (
  213              TorchExportableModuleWithStaticCache,

  265          # Static Cache + export
  266:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  267  

  283      def test_export_hybrid_cache(self):
  284:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  285:         from transformers.pytorch_utils import is_torch_greater_or_equal
  286  

train_real_world/transformers_4573/tests/models/gemma3/test_image_processing_gemma3.py:
  18  
  19: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import Gemma3ImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import Gemma3ImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/gemma3/test_modeling_gemma3.py:
   23  
   24: from transformers import (
   25      AutoModelForCausalLM,

   30  )
   31: from transformers.testing_utils import (
   32      Expectations,

   55  
   56:     from transformers import (
   57          Gemma3ForCausalLM,

   64      )
   65:     from transformers.pytorch_utils import is_torch_greater_or_equal
   66  

  838  
  839:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  840  

train_real_world/transformers_4573/tests/models/gemma3/test_processing_gemma3.py:
  16  
  17: from transformers import Gemma3Processor
  18: from transformers.testing_utils import get_tests_dir, require_vision
  19  

train_real_world/transformers_4573/tests/models/gemma3n/test_feature_extraction_gemma3n.py:
  25  
  26: from transformers.models.gemma3n import Gemma3nAudioFeatureExtractor
  27: from transformers.testing_utils import (
  28      check_json_file_has_correct_format,

  30  )
  31: from transformers.utils.import_utils import is_torch_available
  32  

train_real_world/transformers_4573/tests/models/gemma3n/test_modeling_gemma3n.py:
  26  
  27: from transformers import (
  28      AutoModelForCausalLM,

  36  )
  37: from transformers.testing_utils import (
  38      Expectations,

  48  )
  49: from transformers.utils import is_flash_attn_2_available
  50  

  65  
  66:     from transformers import (
  67          Gemma3nAudioEncoder,

train_real_world/transformers_4573/tests/models/gemma3n/test_processing_gemma3n.py:
  16  
  17: from transformers import is_speech_available
  18: from transformers.testing_utils import require_sentencepiece, require_torch, require_torchaudio, require_vision
  19  

  24  if is_speech_available():
  25:     from transformers.models.gemma3n import Gemma3nProcessor
  26  

train_real_world/transformers_4573/tests/models/git/test_modeling_git.py:
  20  
  21: from transformers import GitConfig, GitProcessor, GitVisionConfig, is_torch_available, is_vision_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24  

  34  
  35:     from transformers import MODEL_FOR_CAUSAL_LM_MAPPING, GitForCausalLM, GitModel, GitVisionModel
  36  

train_real_world/transformers_4573/tests/models/git/test_processing_git.py:
  15  
  16: from transformers.testing_utils import require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import GitProcessor
  24  

train_real_world/transformers_4573/tests/models/glm/test_modeling_glm.py:
  19  
  20: from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available
  21: from transformers.testing_utils import (
  22      Expectations,

  35  
  36:     from transformers import (
  37          GlmModel,

train_real_world/transformers_4573/tests/models/glm4/test_modeling_glm4.py:
  20  
  21: from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available
  22: from transformers.testing_utils import (
  23      Expectations,

  37  
  38:     from transformers import (
  39          Glm4Model,

train_real_world/transformers_4573/tests/models/glm46v/test_modeling_glm46v.py:
  18  
  19: from transformers import (
  20      AutoProcessor,

  25  )
  26: from transformers.testing_utils import (
  27      Expectations,

train_real_world/transformers_4573/tests/models/glm46v/test_processor_glm46v.py:
  19  
  20: from transformers.testing_utils import require_av, require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import Glm46VProcessor
  28  

train_real_world/transformers_4573/tests/models/glm46v/test_video_processing_glm46v.py:
  19  
  20: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  31      if is_torchvision_available():
  32:         from transformers import Glm46VVideoProcessor
  33:         from transformers.models.glm46v.video_processing_glm46v import smart_resize
  34  

train_real_world/transformers_4573/tests/models/glm4_moe/test_modeling_glm4_moe.py:
  21  
  22: from transformers import is_torch_available
  23: from transformers.testing_utils import (
  24      cleanup,

  35  if is_torch_available():
  36:     from transformers import AutoTokenizer, Glm4MoeForCausalLM, Glm4MoeModel
  37  

train_real_world/transformers_4573/tests/models/glm4v/test_image_processing_glm4v.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  32  
  33:     from transformers import Glm4vImageProcessor
  34:     from transformers.models.glm4v.image_processing_glm4v import smart_resize
  35  
  36      if is_torchvision_available():
  37:         from transformers import Glm4vImageProcessorFast
  38  

train_real_world/transformers_4573/tests/models/glm4v/test_modeling_glm4v.py:
  18  
  19: from transformers import (
  20      AutoProcessor,

  25  )
  26: from transformers.testing_utils import (
  27      Expectations,

train_real_world/transformers_4573/tests/models/glm4v/test_processor_glm4v.py:
  19  
  20: from transformers.testing_utils import require_av, require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import Glm4vProcessor
  28  

train_real_world/transformers_4573/tests/models/glm4v/test_video_processing_glm4v.py:
  19  
  20: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  31      if is_torchvision_available():
  32:         from transformers import Glm4vVideoProcessor
  33:         from transformers.models.glm4v.video_processing_glm4v import smart_resize
  34  

train_real_world/transformers_4573/tests/models/glm4v_moe/test_modeling_glm4v_moe.py:
  18  
  19: from transformers import (
  20      AutoProcessor,

  25  )
  26: from transformers.testing_utils import (
  27      cleanup,

train_real_world/transformers_4573/tests/models/glpn/test_image_processing_glpn.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import GLPNImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import GLPNImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/glpn/test_modeling_glpn.py:
  17  
  18: from transformers import is_torch_available, is_vision_available
  19: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  20  

  28  
  29:     from transformers import GLPNConfig, GLPNForDepthEstimation, GLPNModel
  30:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  31  

  35  
  36:     from transformers import GLPNImageProcessor
  37  

train_real_world/transformers_4573/tests/models/got_ocr2/test_image_processing_got_ocr2.py:
  17  
  18: from transformers.image_utils import SizeDict
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  21  

  28  if is_vision_available():
  29:     from transformers import GotOcr2ImageProcessor
  30  
  31      if is_torchvision_available():
  32:         from transformers import GotOcr2ImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/got_ocr2/test_modeling_got_ocr2.py:
  17  
  18: from transformers import (
  19      AutoProcessor,

  23  )
  24: from transformers.testing_utils import cleanup, require_torch, slow, torch_device
  25  

  34  
  35:     from transformers import (
  36          GotOcr2ForConditionalGeneration,

  41  if is_vision_available():
  42:     from transformers.image_utils import load_image
  43  

train_real_world/transformers_4573/tests/models/got_ocr2/test_processing_got_ocr2.py:
  16  
  17: from transformers import GotOcr2Processor
  18: from transformers.testing_utils import require_vision
  19  

train_real_world/transformers_4573/tests/models/gpt2/test_modeling_gpt2.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      Expectations,

  36  
  37:     from transformers import (
  38          GPT2DoubleHeadsModel,

train_real_world/transformers_4573/tests/models/gpt2/test_tokenization_gpt2.py:
  17  
  18: from transformers import AutoTokenizer, GPT2Tokenizer
  19: from transformers.testing_utils import require_tiktoken, require_tokenizers
  20  

  73  
  74:         from transformers.integrations.tiktoken import convert_tiktoken_to_fast
  75  

train_real_world/transformers_4573/tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py:
  18  
  19: from transformers import GPTBigCodeConfig, is_torch_available
  20: from transformers.testing_utils import cleanup, require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          AutoTokenizer,

  38      )
  39:     from transformers.models.gpt_bigcode.modeling_gpt_bigcode import GPTBigCodeAttention
  40  

train_real_world/transformers_4573/tests/models/gpt_neo/test_modeling_gpt_neo.py:
  18  
  19: from transformers import GPTNeoConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          GPT2Tokenizer,

train_real_world/transformers_4573/tests/models/gpt_neox/test_modeling_gpt_neox.py:
  17  
  18: from transformers import AutoTokenizer, DynamicCache, GPTNeoXConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  29  
  30:     from transformers import (
  31          GPTNeoXForCausalLM,

train_real_world/transformers_4573/tests/models/gpt_neox/test_tokenization_gpt_neox.py:
  16  
  17: from transformers import GPTNeoXTokenizer
  18: from transformers.testing_utils import require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py:
  17  
  18: from transformers import GPTNeoXJapaneseConfig, is_torch_available
  19: from transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese import GPTNeoXJapaneseTokenizer
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseModel
  32  

train_real_world/transformers_4573/tests/models/gpt_neox_japanese/test_tokenization_gpt_neox_japanese.py:
  19  
  20: from transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese import (
  21      VOCAB_FILES_NAMES,

  23  )
  24: from transformers.testing_utils import require_tokenizers, slow
  25  

train_real_world/transformers_4573/tests/models/gpt_oss/test_modeling_gpt_oss.py:
   26  
   27: from transformers import (
   28      AutoModelForCausalLM,

   31  )
   32: from transformers.testing_utils import (
   33      cleanup,

   46  
   47:     from transformers import (
   48          GptOssModel,

   99  
  100:     from transformers import AutoModelForCausalLM, AutoTokenizer
  101:     from transformers.testing_utils import torch_device
  102  

train_real_world/transformers_4573/tests/models/gpt_sw3/test_tokenization_gpt_sw3.py:
  16  
  17: from transformers import GPTSw3Tokenizer
  18: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow
  19  

train_real_world/transformers_4573/tests/models/gptj/test_modeling_gptj.py:
  17  
  18: from transformers import GPTJConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      require_torch,

  34  
  35:     from transformers import (
  36          AutoTokenizer,

train_real_world/transformers_4573/tests/models/granite/test_modeling_granite.py:
  17  
  18: from transformers import GraniteConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  36  
  37:     from transformers import (
  38          GraniteForCausalLM,

train_real_world/transformers_4573/tests/models/granite_speech/test_modeling_granite_speech.py:
  21  
  22: from transformers import (
  23      AutoProcessor,

  26  )
  27: from transformers.testing_utils import (
  28      cleanup,

  32  )
  33: from transformers.utils import (
  34      is_datasets_available,

train_real_world/transformers_4573/tests/models/granite_speech/test_processing_granite_speech.py:
  22  
  23: from transformers import AutoTokenizer, GPT2TokenizerFast
  24: from transformers.testing_utils import (
  25      require_torch,

  29  )
  30: from transformers.utils import is_torchaudio_available
  31  

  33  if is_torchaudio_available():
  34:     from transformers import GraniteSpeechFeatureExtractor, GraniteSpeechProcessor
  35  

train_real_world/transformers_4573/tests/models/granitemoe/test_modeling_granitemoe.py:
  17  
  18: from transformers import AutoTokenizer, GraniteMoeConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  35  
  36:     from transformers import (
  37          GraniteMoeForCausalLM,

train_real_world/transformers_4573/tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py:
  23  
  24: from transformers import (
  25      AutoTokenizer,

  29  )
  30: from transformers.testing_utils import (
  31      require_flash_attn,

  47  
  48:     from transformers import (
  49          GraniteMoeHybridForCausalLM,

  51      )
  52:     from transformers.models.granitemoehybrid.modeling_granitemoehybrid import HybridMambaAttentionDynamicCache
  53  

train_real_world/transformers_4573/tests/models/granitemoeshared/test_modeling_granitemoeshared.py:
  17  
  18: from transformers import AutoTokenizer, GraniteMoeSharedConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  35  
  36:     from transformers import (
  37          GraniteMoeSharedForCausalLM,

train_real_world/transformers_4573/tests/models/grounding_dino/test_image_processing_grounding_dino.py:
  21  
  22: from transformers.testing_utils import require_torch, require_vision, slow
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  30  
  31:     from transformers.models.grounding_dino.modeling_grounding_dino import GroundingDinoObjectDetectionOutput
  32  

  35  
  36:     from transformers import GroundingDinoImageProcessor
  37  
  38      if is_torchvision_available():
  39:         from transformers import GroundingDinoImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/grounding_dino/test_modeling_grounding_dino.py:
  24  
  25: from transformers import (
  26      GroundingDinoConfig,

  30  )
  31: from transformers.testing_utils import (
  32      Expectations,

  49  
  50:     from transformers import GroundingDinoConfig, GroundingDinoForObjectDetection, GroundingDinoModel
  51:     from transformers.pytorch_utils import id_tensor_storage
  52  

  56  
  57:     from transformers import AutoProcessor
  58  

train_real_world/transformers_4573/tests/models/grounding_dino/test_processing_grounding_dino.py:
  17  
  18: from transformers import GroundingDinoProcessor
  19: from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available
  22  

  28  
  29:     from transformers.models.grounding_dino.modeling_grounding_dino import GroundingDinoObjectDetectionOutput
  30  

train_real_world/transformers_4573/tests/models/groupvit/test_modeling_groupvit.py:
  23  
  24: from transformers import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig
  25: from transformers.testing_utils import is_flaky, require_torch, require_vision, slow, torch_device
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  41  
  42:     from transformers import GroupViTModel, GroupViTTextModel, GroupViTVisionModel
  43  

  47  
  48:     from transformers import CLIPProcessor
  49  

train_real_world/transformers_4573/tests/models/helium/test_modeling_helium.py:
  17  
  18: from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  32  
  33:     from transformers import (
  34          HeliumModel,

train_real_world/transformers_4573/tests/models/herbert/test_tokenization_herbert.py:
  17  
  18: from transformers import HerbertTokenizer
  19: from transformers.testing_utils import require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/hgnet_v2/test_modeling_hgnet_v2.py:
  19  
  20: from transformers import HGNetV2Config
  21: from transformers.testing_utils import require_torch, torch_device
  22: from transformers.utils.import_utils import is_torch_available
  23  

  29  if is_torch_available():
  30:     from transformers import HGNetV2Backbone, HGNetV2ForImageClassification
  31  

train_real_world/transformers_4573/tests/models/hiera/test_modeling_hiera.py:
  19  
  20: from transformers import HieraConfig
  21: from transformers.testing_utils import (
  22      require_torch,

  26  )
  27: from transformers.utils import (
  28      is_torch_available,

  41  
  42:     from transformers import HieraBackbone, HieraForImageClassification, HieraForPreTraining, HieraModel
  43  

  46  
  47:     from transformers import AutoImageProcessor
  48  

train_real_world/transformers_4573/tests/models/hubert/test_modeling_hubert.py:
  20  
  21: from transformers import HubertConfig, is_torch_available
  22: from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device
  23  

  36  
  37:     from transformers import (
  38          HubertForCTC,

  43      )
  44:     from transformers.models.hubert.modeling_hubert import _compute_mask_indices
  45  

train_real_world/transformers_4573/tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      cleanup,

  27  if is_torch_available():
  28:     from transformers import (
  29          HunYuanDenseV1Model,

train_real_world/transformers_4573/tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py:
  20  
  21: from transformers import is_torch_available
  22: from transformers.testing_utils import (
  23      cleanup,

  30  if is_torch_available():
  31:     from transformers import (
  32          AutoModelForCausalLM,

train_real_world/transformers_4573/tests/models/ibert/test_modeling_ibert.py:
  18  
  19: from transformers import IBertConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          IBertForMaskedLM,

  38      )
  39:     from transformers.models.ibert.modeling_ibert import (
  40          IBertEmbeddings,

train_real_world/transformers_4573/tests/models/idefics/test_image_processing_idefics.py:
  19  
  20: from transformers.testing_utils import require_torch, require_torchvision, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  34  
  35:     from transformers import IdeficsImageProcessor
  36  

train_real_world/transformers_4573/tests/models/idefics/test_modeling_idefics.py:
  21  
  22: from transformers import BitsAndBytesConfig, IdeficsConfig, is_torch_available, is_vision_available
  23: from transformers.testing_utils import (
  24      TestCasePlus,

  46  
  47:     from transformers import IdeficsForVisionText2Text, IdeficsModel, IdeficsProcessor
  48:     from transformers.models.idefics.configuration_idefics import IdeficsPerceiverConfig, IdeficsVisionConfig
  49  

train_real_world/transformers_4573/tests/models/idefics/test_processing_idefics.py:
  18  
  19: from transformers import (
  20      IdeficsProcessor,
  21  )
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

train_real_world/transformers_4573/tests/models/idefics2/test_image_processing_idefics2.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  28  
  29:     from transformers import Idefics2ImageProcessor
  30  
  31      if is_torchvision_available():
  32:         from transformers import Idefics2ImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/idefics2/test_modeling_idefics2.py:
  23  
  24: from transformers import (
  25      AutoProcessor,

  32  )
  33: from transformers.testing_utils import (
  34      Expectations,

train_real_world/transformers_4573/tests/models/idefics2/test_processing_idefics2.py:
  16  
  17: from transformers import Idefics2Processor
  18: from transformers.image_utils import load_image
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_vision_available
  21  

  25  if is_vision_available():
  26:     from transformers import (
  27          Idefics2Processor,

train_real_world/transformers_4573/tests/models/idefics3/test_image_processing_idefics3.py:
  20  
  21: from transformers.image_utils import PILImageResampling, load_image
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  31  
  32:     from transformers import Idefics3ImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import Idefics3ImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/idefics3/test_modeling_idefics3.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      cleanup,

  43  
  44:     from transformers import (
  45          BitsAndBytesConfig,

train_real_world/transformers_4573/tests/models/idefics3/test_processing_idefics3.py:
  18  
  19: from transformers import Idefics3Processor
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_torch, require_vision
  22  

train_real_world/transformers_4573/tests/models/ijepa/test_modeling_ijepa.py:
  18  
  19: from transformers import IJepaConfig
  20: from transformers.testing_utils import (
  21      require_accelerate,

  28  )
  29: from transformers.utils import (
  30      is_torch_available,

  42  
  43:     from transformers import IJepaForImageClassification, IJepaModel
  44  

  48  
  49:     from transformers import ViTImageProcessor
  50  

train_real_world/transformers_4573/tests/models/imagegpt/test_image_processing_imagegpt.py:
  26  
  27: from transformers import AutoImageProcessor
  28: from transformers.testing_utils import (
  29      check_json_file_has_correct_format,

  35  )
  36: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  37  

  46  
  47:     from transformers import ImageGPTImageProcessor
  48  
  49      if is_torchvision_available():
  50:         from transformers import ImageGPTImageProcessorFast
  51  

train_real_world/transformers_4573/tests/models/imagegpt/test_modeling_imagegpt.py:
  19  
  20: from transformers import ImageGPTConfig
  21: from transformers.testing_utils import require_torch, require_vision, run_test_using_subprocess, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  32  
  33:     from transformers import (
  34          ImageGPTForCausalImageModeling,

  41  
  42:     from transformers import ImageGPTImageProcessor
  43  

train_real_world/transformers_4573/tests/models/informer/test_modeling_informer.py:
  22  
  23: from transformers import is_torch_available
  24: from transformers.testing_utils import is_flaky, require_torch, slow, torch_device
  25: from transformers.utils import check_torch_load_is_safe
  26  

  36  
  37:     from transformers import InformerConfig, InformerForPrediction, InformerModel
  38:     from transformers.models.informer.modeling_informer import (
  39          InformerDecoder,

train_real_world/transformers_4573/tests/models/instructblip/test_modeling_instructblip.py:
  22  
  23: from transformers import (
  24      CONFIG_MAPPING,

  30  )
  31: from transformers.testing_utils import (
  32      Expectations,

  40  )
  41: from transformers.utils import is_torch_available, is_vision_available
  42  

  56  
  57:     from transformers import InstructBlipForConditionalGeneration, InstructBlipModel, InstructBlipVisionModel
  58  

train_real_world/transformers_4573/tests/models/instructblip/test_processing_instructblip.py:
  15  
  16: from transformers.testing_utils import require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import (
  24          InstructBlipProcessor,

train_real_world/transformers_4573/tests/models/instructblipvideo/test_modeling_instructblipvideo.py:
  22  
  23: from transformers import (
  24      CONFIG_MAPPING,

  30  )
  31: from transformers.testing_utils import (
  32      require_accelerate,

  38  )
  39: from transformers.utils import is_torch_available
  40  

  54  
  55:     from transformers import (
  56          InstructBlipVideoForConditionalGeneration,

train_real_world/transformers_4573/tests/models/instructblipvideo/test_processing_instructblipvideo.py:
  15  
  16: from transformers.testing_utils import require_torch, require_vision
  17: from transformers.utils import is_torchvision_available, is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import (
  24          InstructBlipVideoProcessor,

train_real_world/transformers_4573/tests/models/instructblipvideo/test_video_processing_instructblipvideo.py:
  17  
  18: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import InstructBlipVideoVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/internvl/test_modeling_internvl.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  29  )
  30: from transformers.testing_utils import (
  31      Expectations,

  50  
  51:     from transformers import InternVLForConditionalGeneration, InternVLModel
  52  

train_real_world/transformers_4573/tests/models/internvl/test_processing_internvl.py:
  19  
  20: from transformers import InternVLProcessor
  21: from transformers.testing_utils import require_av, require_torch, require_vision
  22: from transformers.utils import is_torch_available
  23  

train_real_world/transformers_4573/tests/models/internvl/test_video_processing_internvl.py:
  17  
  18: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import InternVLVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/jais2/test_modeling_jais2.py:
  17  
  18: from transformers import AutoTokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      cleanup,

  33  
  34:     from transformers import (
  35          Jais2Config,

train_real_world/transformers_4573/tests/models/jamba/test_modeling_jamba.py:
  21  
  22: from transformers import AutoTokenizer, BitsAndBytesConfig, JambaConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      DeviceProperties,

  44  
  45:     from transformers import (
  46          JambaForCausalLM,

  49      )
  50:     from transformers.models.jamba.modeling_jamba import (
  51          HybridMambaAttentionDynamicCache,

train_real_world/transformers_4573/tests/models/janus/test_image_processing_janus.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import JanusImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import JanusImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/janus/test_modeling_janus.py:
  25  
  26: from transformers import (
  27      AutoProcessor,

  35  )
  36: from transformers.models.auto import get_values
  37: from transformers.models.auto.modeling_auto import MODEL_FOR_BACKBONE_MAPPING_NAMES, MODEL_MAPPING_NAMES
  38: from transformers.testing_utils import (
  39      Expectations,

train_real_world/transformers_4573/tests/models/janus/test_processing_janus.py:
  20  
  21: from transformers import JanusProcessor
  22  

train_real_world/transformers_4573/tests/models/jetmoe/test_modeling_jetmoe.py:
  19  
  20: from transformers import AutoTokenizer, is_torch_available
  21: from transformers.testing_utils import (
  22      cleanup,

  35  
  36:     from transformers import (
  37          JetMoeForCausalLM,

train_real_world/transformers_4573/tests/models/kosmos2/test_modeling_kosmos2.py:
  25  
  26: from transformers import AutoModelForImageTextToText, AutoProcessor, Kosmos2Config
  27: from transformers.models.kosmos2.configuration_kosmos2 import Kosmos2TextConfig, Kosmos2VisionConfig
  28: from transformers.testing_utils import (
  29      IS_ROCM_SYSTEM,

  35  )
  36: from transformers.utils import (
  37      is_torch_available,

  55  
  56:     from transformers import Kosmos2ForConditionalGeneration, Kosmos2Model
  57  

train_real_world/transformers_4573/tests/models/kosmos2/test_processing_kosmos2.py:
  21  
  22: from transformers.image_utils import load_image
  23: from transformers.testing_utils import (
  24      get_tests_dir,

  29  )
  30: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  31: from transformers.utils import is_vision_available
  32  

  38  
  39:     from transformers import (
  40          AutoProcessor,

train_real_world/transformers_4573/tests/models/kosmos2_5/test_image_processing_kosmos2_5.py:
  22  
  23: from transformers.image_utils import load_image
  24: from transformers.testing_utils import require_torch, require_torch_accelerator, require_vision, slow, torch_device
  25: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  26  

  36  
  37:     from transformers import Kosmos2_5ImageProcessor
  38  
  39      if is_torchvision_available():
  40:         from transformers import Kosmos2_5ImageProcessorFast
  41  

train_real_world/transformers_4573/tests/models/kosmos2_5/test_modeling_kosmos2_5.py:
  26  
  27: from transformers import AutoProcessor, Kosmos2_5Config
  28: from transformers.models.kosmos2_5.configuration_kosmos2_5 import (
  29      Kosmos2_5TextConfig,

  31  )
  32: from transformers.testing_utils import (
  33      require_flash_attn,

  39  )
  40: from transformers.utils import is_torch_available, is_vision_available
  41  

  55  
  56:     from transformers import Kosmos2_5ForConditionalGeneration, Kosmos2_5Model
  57  

train_real_world/transformers_4573/tests/models/kosmos2_5/test_processor_kosmos2_5.py:
  22  
  23: from transformers.image_utils import load_image
  24: from transformers.testing_utils import (
  25      require_torch,

  27  )
  28: from transformers.utils import is_vision_available
  29  

  35  
  36:     from transformers import (
  37          AutoProcessor,

train_real_world/transformers_4573/tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py:
  23  
  24: from transformers import (
  25      KyutaiSpeechToTextConfig,

  29  )
  30: from transformers.testing_utils import (
  31      cleanup,

  52  
  53:     from transformers import (
  54          KyutaiSpeechToTextForConditionalGeneration,

train_real_world/transformers_4573/tests/models/lasr/test_modeling_lasr.py:
  18  
  19: from transformers import is_datasets_available, is_torch_available
  20: from transformers.testing_utils import cleanup, require_torch, slow, torch_device
  21  

  31  
  32:     from transformers import (
  33          AutoProcessor,

train_real_world/transformers_4573/tests/models/layoutlm/test_modeling_layoutlm.py:
  15  
  16: from transformers import LayoutLMConfig, is_torch_available
  17: from transformers.testing_utils import require_torch, slow, torch_device
  18  

  26  
  27:     from transformers import (
  28          LayoutLMForMaskedLM,

train_real_world/transformers_4573/tests/models/layoutlmv2/test_image_processing_layoutlmv2.py:
  20  
  21: from transformers.testing_utils import (
  22      require_pytesseract,

  28  )
  29: from transformers.utils import (
  30      is_pytesseract_available,

  43  
  44:     from transformers import LayoutLMv2ImageProcessor
  45  
  46      if is_torchvision_available():
  47:         from transformers import LayoutLMv2ImageProcessorFast
  48  

train_real_world/transformers_4573/tests/models/layoutlmv2/test_modeling_layoutlmv2.py:
  17  
  18: from transformers.testing_utils import (
  19      require_detectron2,

  25  )
  26: from transformers.utils import is_detectron2_available, is_torch_available
  27  

  36  
  37:     from transformers import (
  38          LayoutLMv2Config,

train_real_world/transformers_4573/tests/models/layoutlmv2/test_processing_layoutlmv2.py:
  18  
  19: from transformers.models.layoutlmv2 import LayoutLMv2Processor, LayoutLMv2Tokenizer, LayoutLMv2TokenizerFast
  20: from transformers.models.layoutlmv2.tokenization_layoutlmv2 import VOCAB_FILES_NAMES
  21: from transformers.testing_utils import require_pytesseract, require_tokenizers, require_torch, slow
  22: from transformers.utils import is_pytesseract_available, is_torchvision_available
  23  

  27  if is_torchvision_available():
  28:     from transformers import LayoutLMv2ImageProcessorFast
  29  
  30  if is_pytesseract_available():
  31:     from transformers import LayoutLMv2ImageProcessor
  32  

train_real_world/transformers_4573/tests/models/layoutlmv2/test_tokenization_layoutlmv2.py:
    23  
    24: from transformers import (
    25      AddedToken,

    31  )
    32: from transformers.models.layoutlmv2.tokenization_layoutlmv2 import (
    33      VOCAB_FILES_NAMES,
    34  )
    35: from transformers.testing_utils import (
    36      require_detectron2,

  1097  
  1098:         from transformers import MODEL_MAPPING, TOKENIZER_MAPPING
  1099  

train_real_world/transformers_4573/tests/models/layoutlmv3/test_image_processing_layoutlmv3.py:
  17  
  18: from transformers.testing_utils import require_pytesseract, require_torch
  19: from transformers.utils import is_pytesseract_available, is_torchvision_available
  20  

  24  if is_pytesseract_available():
  25:     from transformers import LayoutLMv3ImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import LayoutLMv3ImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/layoutlmv3/test_modeling_layoutlmv3.py:
  19  
  20: from transformers.models.auto import get_values
  21: from transformers.testing_utils import require_torch, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  31  
  32:     from transformers import (
  33          MODEL_FOR_MULTIPLE_CHOICE_MAPPING,

  46  
  47:     from transformers import LayoutLMv3ImageProcessor
  48  

train_real_world/transformers_4573/tests/models/layoutlmv3/test_processing_layoutlmv3.py:
  19  
  20: from transformers.models.layoutlmv3 import LayoutLMv3Processor, LayoutLMv3Tokenizer, LayoutLMv3TokenizerFast
  21: from transformers.models.layoutlmv3.tokenization_layoutlmv3 import VOCAB_FILES_NAMES
  22: from transformers.testing_utils import require_pytesseract, require_tokenizers, require_torch, slow
  23: from transformers.utils import is_pytesseract_available
  24  

  28  if is_pytesseract_available():
  29:     from transformers import LayoutLMv3ImageProcessor
  30  

train_real_world/transformers_4573/tests/models/layoutlmv3/test_tokenization_layoutlmv3.py:
  23  
  24: from transformers import (
  25      AddedToken,

  30  )
  31: from transformers.models.layoutlmv3.tokenization_layoutlmv3 import VOCAB_FILES_NAMES, LayoutLMv3Tokenizer
  32: from transformers.testing_utils import (
  33      require_pandas,

  36  )
  37: from transformers.tokenization_utils_base import PreTrainedTokenizerBase
  38  

train_real_world/transformers_4573/tests/models/layoutxlm/test_processing_layoutxlm.py:
  17  
  18: from transformers.models.layoutxlm import LayoutXLMProcessor, LayoutXLMTokenizer, LayoutXLMTokenizerFast
  19: from transformers.testing_utils import (
  20      require_pytesseract,

  25  )
  26: from transformers.utils import is_pytesseract_available
  27  

  31  if is_pytesseract_available():
  32:     from transformers import LayoutLMv2ImageProcessor
  33  

train_real_world/transformers_4573/tests/models/layoutxlm/test_tokenization_layoutxlm.py:
    21  
    22: from transformers import (
    23      AddedToken,

    29  )
    30: from transformers.testing_utils import (
    31      get_tests_dir,

    37  )
    38: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
    39  

  1232  
  1233:         from transformers import MODEL_MAPPING, TOKENIZER_MAPPING
  1234  

train_real_world/transformers_4573/tests/models/led/test_modeling_led.py:
  20  
  21: from transformers import LEDConfig, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import (
  24      require_sentencepiece,

  40  
  41:     from transformers import (
  42          MODEL_FOR_QUESTION_ANSWERING_MAPPING,

  48      )
  49:     from transformers.models.led.modeling_led import LEDDecoder, LEDEncoder
  50  

train_real_world/transformers_4573/tests/models/levit/test_image_processing_levit.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import LevitImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import LevitImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/levit/test_modeling_levit.py:
  20  
  21: from transformers import LevitConfig
  22: from transformers.file_utils import is_torch_available, is_vision_available
  23: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          LevitForImageClassification,

  37      )
  38:     from transformers.models.auto.modeling_auto import (
  39          MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,

  46  
  47:     from transformers import LevitImageProcessor
  48  

train_real_world/transformers_4573/tests/models/lfm2/test_modeling_lfm2.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      require_read_token,

  32  
  33:     from transformers import Lfm2ForCausalLM, Lfm2Model
  34:     from transformers.models.lfm2.modeling_lfm2 import Lfm2HybridConvCache
  35  

train_real_world/transformers_4573/tests/models/lfm2_moe/test_modeling_lfm2_moe.py:
  18  
  19: from transformers import AutoTokenizer, is_torch_available, set_seed
  20: from transformers.testing_utils import (
  21      Expectations,

  36  
  37:     from transformers import Lfm2MoeConfig, Lfm2MoeForCausalLM, Lfm2MoeModel
  38:     from transformers.models.lfm2_moe.modeling_lfm2_moe import Lfm2MoeHybridConvCache
  39  

train_real_world/transformers_4573/tests/models/lfm2_vl/test_image_processing_lfm2_vl.py:
  20  
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  34      if is_torchvision_available():
  35:         from transformers import Lfm2VlImageProcessorFast
  36:         from transformers.models.lfm2_vl.image_processing_lfm2_vl_fast import (
  37              find_closest_aspect_ratio,

train_real_world/transformers_4573/tests/models/lfm2_vl/test_modeling_lfm2_vl.py:
  22  
  23: from transformers import AutoProcessor, is_torch_available
  24: from transformers.models.lfm2_vl.modeling_lfm2_vl import Lfm2VlForConditionalGeneration
  25: from transformers.testing_utils import (
  26      cleanup,

  32  )
  33: from transformers.utils.import_utils import is_vision_available
  34  

  46  
  47:     from transformers import Lfm2VlConfig, Lfm2VlForConditionalGeneration, Lfm2VlModel
  48:     from transformers.models.lfm2.modeling_lfm2 import Lfm2HybridConvCache
  49  

train_real_world/transformers_4573/tests/models/lfm2_vl/test_processing_lfm2_vl.py:
  19  
  20: from transformers import Lfm2VlProcessor
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torchvision_available, is_vision_available
  23  

train_real_world/transformers_4573/tests/models/lightglue/test_image_processing_lightglue.py:
  19  )
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  27  
  28:     from transformers.models.lightglue.modeling_lightglue import LightGlueKeypointMatchingOutput
  29  
  30  if is_vision_available():
  31:     from transformers import LightGlueImageProcessor
  32  
  33      if is_torchvision_available():
  34:         from transformers import LightGlueImageProcessorFast
  35  

train_real_world/transformers_4573/tests/models/lightglue/test_modeling_lightglue.py:
  19  
  20: from transformers.models.lightglue.configuration_lightglue import LightGlueConfig
  21: from transformers.testing_utils import get_device_properties, require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  30  
  31:     from transformers import LightGlueForKeypointMatching
  32  
  33  if is_vision_available():
  34:     from transformers import AutoImageProcessor
  35  

train_real_world/transformers_4573/tests/models/lilt/test_modeling_lilt.py:
  17  
  18: from transformers import LiltConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          LiltForQuestionAnswering,

train_real_world/transformers_4573/tests/models/llama/test_modeling_llama.py:
   20  
   21: from transformers import AutoTokenizer, StaticCache, is_torch_available
   22: from transformers.generation.configuration_utils import GenerationConfig
   23: from transformers.testing_utils import (
   24      Expectations,

   39  
   40:     from transformers import (
   41          LlamaForCausalLM,

  275  
  276:         from transformers.integrations.executorch import (
  277              TorchExportableModuleWithStaticCache,

  322              # Static Cache + export
  323:             from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  324  

train_real_world/transformers_4573/tests/models/llama/test_tokenization_llama.py:
  3  from tests.test_tokenization_common import TokenizerTesterMixin
  4: from transformers.models.llama.tokenization_llama import LlamaTokenizer
  5: from transformers.testing_utils import (
  6      require_tokenizers,

train_real_world/transformers_4573/tests/models/llama4/test_image_processing_llama4.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available() and is_torchvision_available():
  25:     from transformers import Llama4ImageProcessorFast
  26  

train_real_world/transformers_4573/tests/models/llama4/test_modeling_llama4.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  31  
  32:     from transformers import (
  33          Llama4ForConditionalGeneration,

train_real_world/transformers_4573/tests/models/llama4/test_processing_llama4.py:
  16  
  17: from transformers import Llama4Processor
  18: from transformers.testing_utils import require_vision
  19  

train_real_world/transformers_4573/tests/models/llava/test_configuration_llava.py:
  3  
  4: from transformers import LlavaConfig
  5  

train_real_world/transformers_4573/tests/models/llava/test_image_processing_llava.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torchvision_available, is_vision_available
  22  

  28  
  29:     from transformers import LlavaImageProcessor
  30  

  33  
  34:         from transformers import LlavaImageProcessorFast
  35  

train_real_world/transformers_4573/tests/models/llava/test_modeling_llava.py:
  21  
  22: from transformers import (
  23      AutoProcessor,

  31  )
  32: from transformers.testing_utils import (
  33      Expectations,

train_real_world/transformers_4573/tests/models/llava/test_processing_llava.py:
  16  
  17: from transformers import AutoTokenizer, LlavaProcessor
  18: from transformers.testing_utils import require_vision
  19  

train_real_world/transformers_4573/tests/models/llava_next/test_image_processing_llava_next.py:
  18  
  19: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension
  20: from transformers.models.llava_next.image_processing_llava_next import select_best_resolution
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  32  
  33:     from transformers import LlavaNextImageProcessor
  34  
  35      if is_torchvision_available():
  36:         from transformers import LlavaNextImageProcessorFast
  37  

train_real_world/transformers_4573/tests/models/llava_next/test_modeling_llava_next.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  31  )
  32: from transformers.testing_utils import (
  33      cleanup,

  38  )
  39: from transformers.utils import check_torch_load_is_safe
  40  

  52  
  53:     from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches
  54  

train_real_world/transformers_4573/tests/models/llava_next/test_processing_llava_next.py:
  19  
  20: from transformers import LlavaNextProcessor
  21: from transformers.testing_utils import (
  22      require_vision,

train_real_world/transformers_4573/tests/models/llava_next_video/test_modeling_llava_next_video.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  31  )
  32: from transformers.testing_utils import (
  33      Expectations,

train_real_world/transformers_4573/tests/models/llava_next_video/test_processing_llava_next_video.py:
  19  
  20: from transformers import LlavaNextVideoProcessor
  21: from transformers.testing_utils import require_vision
  22: from transformers.utils import is_torchvision_available, is_vision_available
  23  

train_real_world/transformers_4573/tests/models/llava_next_video/test_video_processing_llava_next_video.py:
  17  
  18: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import LlavaNextVideoVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/llava_onevision/test_image_processing_llava_onevision.py:
  19  
  20: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  32  
  33:     from transformers import LlavaOnevisionImageProcessor
  34  
  35      if is_torchvision_available():
  36:         from transformers import LlavaOnevisionImageProcessorFast
  37  

train_real_world/transformers_4573/tests/models/llava_onevision/test_modeling_llava_onevision.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  30  )
  31: from transformers.testing_utils import (
  32      Expectations,

train_real_world/transformers_4573/tests/models/llava_onevision/test_processing_llava_onevision.py:
  20  
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_vision_available
  23  

  27  if is_vision_available():
  28:     from transformers import (
  29          LlavaOnevisionProcessor,

  41          from tests.test_processing_common import MODALITY_INPUT_DATA
  42:         from transformers import video_processing_utils, video_utils
  43  

train_real_world/transformers_4573/tests/models/llava_onevision/test_video_processing_llava_onevision.py:
  17  
  18: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import LlavaOnevisionVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/longcat_flash/test_modeling_longcat_flash.py:
  21  
  22: from transformers import LongcatFlashConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      require_bitsandbytes,

  39  
  40:     from transformers import AutoTokenizer, Cache, LongcatFlashForCausalLM, LongcatFlashModel
  41  

train_real_world/transformers_4573/tests/models/longformer/test_modeling_longformer.py:
  17  
  18: from transformers import LongformerConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      is_flaky,

  35  
  36:     from transformers import (
  37          LongformerForMaskedLM,

  43      )
  44:     from transformers.models.longformer.modeling_longformer import LongformerSelfAttention
  45  

train_real_world/transformers_4573/tests/models/longt5/test_modeling_longt5.py:
  18  
  19: from transformers import LongT5Config, is_torch_available
  20: from transformers.models.auto import get_values
  21: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  22  

  32  
  33:     from transformers import (
  34          MODEL_FOR_QUESTION_ANSWERING_MAPPING,

train_real_world/transformers_4573/tests/models/luke/test_modeling_luke.py:
  17  
  18: from transformers import LukeConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          LukeForEntityClassification,

train_real_world/transformers_4573/tests/models/luke/test_tokenization_luke.py:
  16  
  17: from transformers import LukeTokenizer
  18: from transformers.testing_utils import get_tests_dir, require_torch, slow
  19  

train_real_world/transformers_4573/tests/models/lxmert/test_modeling_lxmert.py:
  20  
  21: from transformers import LxmertConfig, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_torch, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/m2m_100/test_modeling_m2m_100.py:
  22  
  23: from transformers import M2M100Config, is_torch_available
  24: from transformers.testing_utils import (
  25      require_flash_attn,

  43  
  44:     from transformers import M2M100ForConditionalGeneration, M2M100Model, M2M100Tokenizer
  45:     from transformers.models.m2m_100.modeling_m2m_100 import M2M100Decoder, M2M100Encoder
  46  

train_real_world/transformers_4573/tests/models/m2m_100/test_tokenization_m2m_100.py:
  20  
  21: from transformers import M2M100Tokenizer, is_torch_available
  22: from transformers.testing_utils import (
  23      get_tests_dir,

  29  )
  30: from transformers.utils import is_sentencepiece_available
  31  

  33  if is_sentencepiece_available():
  34:     from transformers.models.m2m_100.tokenization_m2m_100 import VOCAB_FILES_NAMES, save_json
  35  

  43  if is_torch_available():
  44:     from transformers.models.m2m_100.modeling_m2m_100 import shift_tokens_right
  45  

train_real_world/transformers_4573/tests/models/mamba/test_modeling_mamba.py:
  21  
  22: from transformers import AutoTokenizer, MambaConfig, is_torch_available
  23: from transformers.testing_utils import require_torch, slow, torch_device
  24  

  33  
  34:     from transformers import (
  35          MambaForCausalLM,

  37      )
  38:     from transformers.models.mamba.modeling_mamba import MambaCache
  39  

train_real_world/transformers_4573/tests/models/mamba2/test_modeling_mamba2.py:
  17  
  18: from transformers import AutoTokenizer, Mamba2Config, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  26  )
  27: from transformers.utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available
  28  

  37  
  38:     from transformers import (
  39          Mamba2ForCausalLM,

  41      )
  42:     from transformers.models.mamba2.modeling_mamba2 import Mamba2Cache, Mamba2Mixer
  43  

train_real_world/transformers_4573/tests/models/marian/test_modeling_marian.py:
  19  
  20: from transformers import MarianConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      require_sentencepiece,

  38  
  39:     from transformers import (
  40          AutoConfig,

  46      )
  47:     from transformers.models.marian.modeling_marian import (
  48          MarianDecoder,

train_real_world/transformers_4573/tests/models/marian/test_tokenization_marian.py:
  19  
  20: from transformers import BatchEncoding, MarianTokenizer
  21: from transformers.testing_utils import get_tests_dir, require_sentencepiece, slow
  22: from transformers.utils import is_sentencepiece_available
  23  

  25  if is_sentencepiece_available():
  26:     from transformers.models.marian.tokenization_marian import VOCAB_FILES_NAMES, save_json
  27  

train_real_world/transformers_4573/tests/models/markuplm/test_feature_extraction_markuplm.py:
  17  
  18: from transformers.testing_utils import require_bs4
  19: from transformers.utils import is_bs4_available
  20  

  24  if is_bs4_available():
  25:     from transformers import MarkupLMFeatureExtractor
  26  

train_real_world/transformers_4573/tests/models/markuplm/test_modeling_markuplm.py:
  18  
  19: from transformers import MarkupLMConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          MarkupLMForQuestionAnswering,

  37  # TODO check dependencies
  38: from transformers import MarkupLMFeatureExtractor, MarkupLMProcessor, MarkupLMTokenizer
  39  

train_real_world/transformers_4573/tests/models/markuplm/test_processing_markuplm.py:
  21  
  22: from transformers import (
  23      MarkupLMProcessor,

  28  )
  29: from transformers.models.markuplm.tokenization_markuplm import VOCAB_FILES_NAMES
  30: from transformers.testing_utils import require_bs4, require_tokenizers, require_torch, slow
  31: from transformers.utils import is_bs4_available, is_tokenizers_available
  32  

  34  if is_bs4_available():
  35:     from transformers import MarkupLMFeatureExtractor
  36  
  37  if is_tokenizers_available():
  38:     from transformers import MarkupLMTokenizerFast
  39  

train_real_world/transformers_4573/tests/models/markuplm/test_tokenization_markuplm.py:
  24  
  25: from transformers import (
  26      AddedToken,

  32  )
  33: from transformers.models.markuplm.tokenization_markuplm import VOCAB_FILES_NAMES, MarkupLMTokenizer
  34: from transformers.testing_utils import require_tokenizers, slow
  35  

train_real_world/transformers_4573/tests/models/mask2former/test_image_processing_mask2former.py:
   21  
   22: from transformers.image_utils import ChannelDimension
   23: from transformers.testing_utils import require_torch, require_vision
   24: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
   25  

   32      if is_vision_available():
   33:         from transformers import Mask2FormerImageProcessor
   34:         from transformers.models.mask2former.image_processing_mask2former import binary_mask_to_rle
   35:         from transformers.models.mask2former.modeling_mask2former import Mask2FormerForUniversalSegmentationOutput
   36  
   37          if is_torchvision_available():
   38:             from transformers import Mask2FormerImageProcessorFast
   39  

  150  
  151: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
  152  def prepare_semantic_single_inputs():

  157  
  158: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  159  def prepare_semantic_batch_inputs():

train_real_world/transformers_4573/tests/models/mask2former/test_modeling_mask2former.py:
  22  from tests.test_modeling_common import floats_tensor
  23: from transformers import AutoModelForImageClassification, Mask2FormerConfig, is_torch_available, is_vision_available
  24: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  25: from transformers.testing_utils import (
  26      Expectations,

  44  
  45:     from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerModel
  46  
  47      if is_vision_available():
  48:         from transformers import Mask2FormerImageProcessor
  49  

train_real_world/transformers_4573/tests/models/maskformer/test_image_processing_maskformer.py:
   21  
   22: from transformers.testing_utils import require_torch, require_vision
   23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
   24  

   31      if is_vision_available():
   32:         from transformers import MaskFormerImageProcessor
   33:         from transformers.models.maskformer.image_processing_maskformer import binary_mask_to_rle
   34:         from transformers.models.maskformer.modeling_maskformer import MaskFormerForInstanceSegmentationOutput
   35  
   36          if is_torchvision_available():
   37:             from transformers import MaskFormerImageProcessorFast
   38  

  149  
  150: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
  151  def prepare_semantic_single_inputs():

  156  
  157: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  158  def prepare_semantic_batch_inputs():

train_real_world/transformers_4573/tests/models/maskformer/test_modeling_maskformer_swin.py:
  18  
  19: from transformers import MaskFormerSwinConfig
  20: from transformers.testing_utils import require_torch, require_torch_multi_gpu, torch_device
  21: from transformers.utils import is_torch_available
  22  

  32  
  33:     from transformers import MaskFormerSwinBackbone
  34:     from transformers.models.maskformer import MaskFormerSwinModel
  35  

train_real_world/transformers_4573/tests/models/maskformer/test_modeling_maskformer.py:
  22  from tests.test_modeling_common import floats_tensor
  23: from transformers import DetrConfig, MaskFormerConfig, SwinConfig, is_torch_available, is_vision_available
  24: from transformers.testing_utils import (
  25      Expectations,

  44  
  45:     from transformers import MaskFormerForInstanceSegmentation, MaskFormerModel
  46  
  47      if is_vision_available():
  48:         from transformers import MaskFormerImageProcessor
  49  

train_real_world/transformers_4573/tests/models/mbart/test_modeling_mbart.py:
  20  
  21: from transformers import MBartConfig, is_torch_available
  22: from transformers.testing_utils import (
  23      require_sentencepiece,

  39  
  40:     from transformers import (
  41          AutoTokenizer,

  48      )
  49:     from transformers.models.mbart.modeling_mbart import MBartDecoder, MBartEncoder
  50  

train_real_world/transformers_4573/tests/models/mbart/test_tokenization_mbart.py:
  17  
  18: from transformers import BatchEncoding, MBartTokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      get_tests_dir,

  33  if is_torch_available():
  34:     from transformers.models.mbart.modeling_mbart import shift_tokens_right
  35  

train_real_world/transformers_4573/tests/models/mbart50/test_tokenization_mbart50.py:
  17  
  18: from transformers import BatchEncoding, MBart50Tokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      get_tests_dir,

  32  if is_torch_available():
  33:     from transformers.models.mbart.modeling_mbart import shift_tokens_right
  34  

train_real_world/transformers_4573/tests/models/megatron_bert/test_modeling_megatron_bert.py:
  20  
  21: from transformers import MegatronBertConfig, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/megatron_gpt2/test_modeling_megatron_gpt2.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  20  

  24  
  25:     from transformers import GPT2LMHeadModel
  26  

train_real_world/transformers_4573/tests/models/metaclip_2/test_modeling_metaclip_2.py:
  23  
  24: from transformers import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig
  25: from transformers.testing_utils import (
  26      require_torch,

  30  )
  31: from transformers.utils import (
  32      is_torch_available,

  51  
  52:     from transformers import (
  53          MetaClip2ForImageClassification,

  63  
  64:     from transformers import CLIPProcessor
  65  

train_real_world/transformers_4573/tests/models/mgp_str/test_modeling_mgp_str.py:
  19  
  20: from transformers import MgpstrConfig
  21: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  32  
  33:     from transformers import MgpstrForSceneTextRecognition, MgpstrModel
  34  

  38  
  39:     from transformers import MgpstrProcessor
  40  

train_real_world/transformers_4573/tests/models/mgp_str/test_processing_mgp_str.py:
  19  
  20: from transformers.models.mgp_str.tokenization_mgp_str import VOCAB_FILES_NAMES
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  31  if is_vision_available():
  32:     from transformers import MgpstrProcessor
  33  

train_real_world/transformers_4573/tests/models/mgp_str/test_tokenization_mgp_str.py:
  19  
  20: from transformers import MgpstrTokenizer
  21: from transformers.models.mgp_str.tokenization_mgp_str import VOCAB_FILES_NAMES
  22: from transformers.testing_utils import require_tokenizers
  23  

train_real_world/transformers_4573/tests/models/mimi/test_modeling_mimi.py:
   24  
   25: from transformers import AutoFeatureExtractor, MimiConfig
   26: from transformers.testing_utils import (
   27      is_flaky,

   42  
   43:     from transformers import MimiModel
   44  
   45  
   46: # Copied from transformers.tests.encodec.test_modeling_encodec.prepare_inputs_dict
   47  def prepare_inputs_dict(

  221  
  222:     # Copied from transformers.tests.encodec.test_modeling_encodec.MimiModelTest.test_determinism
  223      def test_determinism(self):

  248  
  249:     # Copied from transformers.tests.encodec.test_modeling_encodec.MimiModelTest.test_model_outputs_equivalence
  250      def test_model_outputs_equivalence(self):

  286  
  287:     # Copied from transformers.tests.encodec.test_modeling_encodec.MimiModelTest.test_identity_shortcut
  288      def test_identity_shortcut(self):

  334  
  335: # Copied from transformers.tests.encodec.test_modeling_encodec.normalize
  336  def normalize(arr):

  341  
  342: # Copied from transformers.tests.encodec.test_modeling_encodec.compute_rmse
  343  def compute_rmse(arr1, arr2):

train_real_world/transformers_4573/tests/models/minimax/test_modeling_minimax.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  31  
  32:     from transformers import (
  33          MiniMaxForCausalLM,

  35      )
  36:     from transformers.models.minimax.modeling_minimax import MiniMaxCache
  37  from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester

train_real_world/transformers_4573/tests/models/ministral/test_modeling_ministral.py:
   22  
   23: from transformers import AutoTokenizer, BitsAndBytesConfig, GenerationConfig, is_torch_available
   24: from transformers.testing_utils import (
   25      backend_empty_cache,

   38  
   39:     from transformers import (
   40          AutoModelForCausalLM,

  157          # TODO: Exportability is not working
  158:         from transformers.testing_utils import is_torch_greater_or_equal
  159  

  162  
  163:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  164  

train_real_world/transformers_4573/tests/models/ministral3/test_modeling_ministral3.py:
  21  
  22: from transformers import AutoTokenizer, Mistral3ForConditionalGeneration, is_torch_available
  23: from transformers.testing_utils import (
  24      Expectations,

  38  
  39:     from transformers import (
  40          Ministral3Model,

train_real_world/transformers_4573/tests/models/mistral/test_modeling_mistral.py:
  22  
  23: from transformers import AutoTokenizer, BitsAndBytesConfig, DynamicCache, is_torch_available, set_seed
  24: from transformers.cache_utils import DynamicSlidingWindowLayer
  25: from transformers.testing_utils import (
  26      DeviceProperties,

  43  
  44:     from transformers import (
  45          MistralForCausalLM,

train_real_world/transformers_4573/tests/models/mistral3/test_modeling_mistral3.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  25  )
  26: from transformers.testing_utils import (
  27      Expectations,

  45  
  46:     from transformers import (
  47          Mistral3ForConditionalGeneration,

train_real_world/transformers_4573/tests/models/mistral3/test_processing_mistral3.py:
  18  
  19: from transformers import PixtralProcessor
  20: from transformers.testing_utils import require_vision
  21: from transformers.utils import is_torch_available
  22  

train_real_world/transformers_4573/tests/models/mixtral/test_modeling_mixtral.py:
  19  
  20: from transformers import is_torch_available
  21: from transformers.testing_utils import (
  22      Expectations,

  34  
  35:     from transformers import (
  36          MixtralForCausalLM,

train_real_world/transformers_4573/tests/models/mlcd/test_modeling_mlcd.py:
  21  
  22: from transformers import (
  23      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      require_torch,

train_real_world/transformers_4573/tests/models/mllama/test_image_processing_mllama.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  28  
  29:     from transformers import MllamaImageProcessor
  30  
  31      if is_torchvision_available():
  32:         from transformers import MllamaImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/mllama/test_modeling_mllama.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  30  )
  31: from transformers.cache_utils import Cache
  32: from transformers.models.mllama.configuration_mllama import MllamaTextConfig
  33: from transformers.testing_utils import (
  34      Expectations,

train_real_world/transformers_4573/tests/models/mllama/test_processing_mllama.py:
  19  
  20: from transformers import MllamaProcessor
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_vision_available
  23  

train_real_world/transformers_4573/tests/models/mluke/test_tokenization_mluke.py:
  17  
  18: from transformers.models.mluke.tokenization_mluke import MLukeTokenizer
  19: from transformers.testing_utils import get_tests_dir, require_torch, slow
  20: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  21  

train_real_world/transformers_4573/tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py:
  24  
  25: from transformers import (
  26      MMGroundingDinoConfig,

  30  )
  31: from transformers.testing_utils import (
  32      is_flaky,

  48  
  49:     from transformers import MMGroundingDinoConfig, MMGroundingDinoForObjectDetection, MMGroundingDinoModel
  50:     from transformers.pytorch_utils import id_tensor_storage
  51  

  55  
  56:     from transformers import AutoProcessor
  57  

train_real_world/transformers_4573/tests/models/mobilebert/test_modeling_mobilebert.py:
  20  
  21: from transformers import AutoTokenizer, MobileBertConfig, MobileBertForMaskedLM, is_torch_available
  22: from transformers.models.auto import get_values
  23: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/mobilenet_v1/test_image_processing_mobilenet_v1.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import MobileNetV1ImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import MobileNetV1ImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py:
  18  
  19: from transformers import MobileNetV1Config
  20: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  30  
  31:     from transformers import MobileNetV1ForImageClassification, MobileNetV1Model
  32  

  36  
  37:     from transformers import MobileNetV1ImageProcessor
  38  

train_real_world/transformers_4573/tests/models/mobilenet_v2/test_image_processing_mobilenet_v2.py:
  19  
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  31  if is_vision_available():
  32:     from transformers import MobileNetV2ImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import MobileNetV2ImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py:
  18  
  19: from transformers import MobileNetV2Config
  20: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  30  
  31:     from transformers import MobileNetV2ForImageClassification, MobileNetV2ForSemanticSegmentation, MobileNetV2Model
  32  

  36  
  37:     from transformers import MobileNetV2ImageProcessor
  38  

train_real_world/transformers_4573/tests/models/mobilevit/test_image_processing_mobilevit.py:
  19  
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  31  if is_vision_available():
  32:     from transformers import MobileViTImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import MobileViTImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/mobilevit/test_modeling_mobilevit.py:
  18  
  19: from transformers import MobileViTConfig
  20: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  30  
  31:     from transformers import MobileViTForImageClassification, MobileViTForSemanticSegmentation, MobileViTModel
  32  

  36  
  37:     from transformers import MobileViTImageProcessor
  38  

train_real_world/transformers_4573/tests/models/mobilevitv2/test_modeling_mobilevitv2.py:
  18  
  19: from transformers import MobileViTV2Config
  20: from transformers.testing_utils import (
  21      Expectations,

  27  )
  28: from transformers.utils import is_torch_available, is_vision_available
  29  

  37  
  38:     from transformers import MobileViTV2ForImageClassification, MobileViTV2ForSemanticSegmentation, MobileViTV2Model
  39:     from transformers.models.mobilevitv2.modeling_mobilevitv2 import (
  40          make_divisible,

  46  
  47:     from transformers import MobileViTImageProcessor
  48  

train_real_world/transformers_4573/tests/models/modernbert/test_modeling_modernbert.py:
  22  
  23: from transformers import AutoTokenizer, ModernBertConfig, PreTrainedModel, is_torch_available
  24: from transformers.models.auto import get_values
  25: from transformers.testing_utils import (
  26      CaptureLogger,

  41  
  42:     from transformers import (
  43          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py:
  17  
  18: from transformers import AutoTokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      require_torch,

  30  
  31:     from transformers import (
  32          ModernBertDecoderForCausalLM,

train_real_world/transformers_4573/tests/models/moonshine/test_modeling_moonshine.py:
  18  
  19: from transformers import MoonshineConfig, is_torch_available
  20: from transformers.testing_utils import Expectations, cleanup, require_torch, slow, torch_device
  21  

  33  
  34:     from transformers import (
  35          AutoProcessor,

train_real_world/transformers_4573/tests/models/moshi/test_modeling_moshi.py:
  25  
  26: from transformers import (
  27      MoshiConfig,

  29  )
  30: from transformers.integrations.deepspeed import (
  31      is_deepspeed_available,

  33  )
  34: from transformers.testing_utils import (
  35      is_flaky,

  59  
  60:     from transformers import (
  61          AutoFeatureExtractor,

train_real_world/transformers_4573/tests/models/moshi/test_tokenization_moshi.py:
  17  
  18: from transformers import (
  19      SPIECE_UNDERLINE,

  23  )
  24: from transformers.convert_slow_tokenizer import MoshiConverter
  25: from transformers.testing_utils import (
  26      get_tests_dir,

train_real_world/transformers_4573/tests/models/mpnet/test_modeling_mpnet.py:
  17  
  18: from transformers import MPNetConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          MPNetForMaskedLM,

train_real_world/transformers_4573/tests/models/mpnet/test_tokenization_mpnet.py:
  17  
  18: from transformers.models.mpnet.tokenization_mpnet import MPNetTokenizer
  19: from transformers.testing_utils import require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/mpt/test_modeling_mpt.py:
  18  
  19: from transformers import BitsAndBytesConfig, MptConfig, is_torch_available
  20: from transformers.testing_utils import (
  21      Expectations,

  38  
  39:     from transformers import (
  40          AutoTokenizer,

train_real_world/transformers_4573/tests/models/mra/test_modeling_mra.py:
  17  
  18: from transformers import MraConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          MraForMaskedLM,

train_real_world/transformers_4573/tests/models/mt5/test_modeling_mt5.py:
  17  
  18: from transformers import MT5Config, is_torch_available
  19: from transformers.testing_utils import (
  20      require_sentencepiece,

  36  
  37:     from transformers import (
  38          AutoModelForSeq2SeqLM,

train_real_world/transformers_4573/tests/models/musicgen/test_modeling_musicgen.py:
  26  
  27: from transformers import (
  28      EncodecConfig,

  34  )
  35: from transformers.testing_utils import (
  36      Expectations,

  55  
  56:     from transformers import (
  57          MusicgenForCausalLM,

train_real_world/transformers_4573/tests/models/musicgen/test_processing_musicgen.py:
  22  
  23: from transformers import T5Tokenizer, T5TokenizerFast
  24: from transformers.testing_utils import require_sentencepiece, require_torch
  25: from transformers.utils.import_utils import is_speech_available
  26  

  28  if is_speech_available():
  29:     from transformers import EncodecFeatureExtractor, MusicgenProcessor
  30  

train_real_world/transformers_4573/tests/models/musicgen_melody/test_feature_extraction_musicgen_melody.py:
  24  
  25: from transformers.testing_utils import (
  26      check_json_file_has_correct_format,

  29  )
  30: from transformers.utils.import_utils import is_torchaudio_available
  31  

  37  
  38:     from transformers import MusicgenMelodyFeatureExtractor
  39  

train_real_world/transformers_4573/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:
  26  
  27: from transformers import (
  28      EncodecConfig,

  33  )
  34: from transformers.testing_utils import (
  35      Expectations,

  57  
  58:     from transformers import (
  59          MusicgenMelodyForCausalLM,

  65  if is_torchaudio_available():
  66:     from transformers import MusicgenMelodyProcessor
  67  

train_real_world/transformers_4573/tests/models/musicgen_melody/test_processing_musicgen_melody.py:
  22  
  23: from transformers import T5Tokenizer, T5TokenizerFast
  24: from transformers.testing_utils import require_sentencepiece, require_torch, require_torchaudio
  25: from transformers.utils.import_utils import is_torchaudio_available
  26  

  28  if is_torchaudio_available():
  29:     from transformers import MusicgenMelodyFeatureExtractor, MusicgenMelodyProcessor
  30  

train_real_world/transformers_4573/tests/models/mvp/test_modeling_mvp.py:
  22  
  23: from transformers import MvpConfig, is_torch_available
  24: from transformers.testing_utils import (
  25      require_sentencepiece,

  41  
  42:     from transformers import (
  43          MvpForCausalLM,

  49      )
  50:     from transformers.models.mvp.modeling_mvp import MvpDecoder, MvpEncoder, shift_tokens_right
  51  

train_real_world/transformers_4573/tests/models/myt5/test_tokenization_myt5.py:
  16  
  17: from transformers import MyT5Tokenizer
  18: from transformers.testing_utils import slow
  19  

train_real_world/transformers_4573/tests/models/nanochat/test_modeling_nanochat.py:
  17  
  18: from transformers import AutoTokenizer, NanoChatConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      cleanup,

  29  
  30:     from transformers import (
  31          NanoChatForCausalLM,

train_real_world/transformers_4573/tests/models/nemotron/test_modeling_nemotron.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      Expectations,

  34  
  35:     from transformers import (
  36          AutoTokenizer,

train_real_world/transformers_4573/tests/models/nllb/test_tokenization_nllb.py:
  18  
  19: from transformers import (
  20      AddedToken,

  24  )
  25: from transformers.models.nllb.tokenization_nllb import FAIRSEQ_LANGUAGE_CODES
  26: from transformers.testing_utils import (
  27      get_tests_dir,

  32  )
  33: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  34  

  41  if is_torch_available():
  42:     from transformers.models.m2m_100.modeling_m2m_100 import shift_tokens_right
  43  

train_real_world/transformers_4573/tests/models/nllb_moe/test_modeling_nllb_moe.py:
  20  
  21: from transformers import NllbMoeConfig, is_torch_available, set_seed
  22: from transformers.testing_utils import (
  23      require_sentencepiece,

  39  
  40:     from transformers import NllbMoeForConditionalGeneration, NllbMoeModel, NllbTokenizer
  41:     from transformers.models.nllb_moe.modeling_nllb_moe import NllbMoeDecoder, NllbMoeEncoder, NllbMoeTop2Router
  42  

train_real_world/transformers_4573/tests/models/nougat/test_image_processing_nougat.py:
  21  
  22: from transformers.image_utils import SizeDict, load_image
  23: from transformers.testing_utils import require_torch, require_vision
  24: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  25  

  35  
  36:     from transformers import NougatImageProcessor
  37  
  38      if is_torchvision_available():
  39:         from transformers import NougatImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/nougat/test_tokenization_nougat.py:
  16  
  17: from transformers import NougatTokenizer
  18: from transformers.models.nougat.tokenization_nougat import markdown_compatible, normalize_list_like_lines
  19: from transformers.testing_utils import require_levenshtein, require_nltk, require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/nystromformer/test_modeling_nystromformer.py:
  17  
  18: from transformers import AutoTokenizer, NystromformerConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          NystromformerForMaskedLM,

train_real_world/transformers_4573/tests/models/olmo/test_modeling_olmo.py:
   20  
   21: from transformers import OlmoConfig, is_torch_available
   22: from transformers.generation.configuration_utils import GenerationConfig
   23: from transformers.models.auto.tokenization_auto import AutoTokenizer
   24: from transformers.models.gpt_neox.tokenization_gpt_neox import GPTNeoXTokenizer as GPTNeoXTokenizerFast
   25: from transformers.testing_utils import (
   26      require_tokenizers,

   40  
   41:     from transformers import (
   42          OlmoForCausalLM,

  292  
  293:         from transformers.integrations.executorch import (
  294              TorchExportableModuleWithStaticCache,

  341          # Static Cache + export
  342:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  343  

train_real_world/transformers_4573/tests/models/olmo2/test_modeling_olmo2.py:
   20  
   21: from transformers import Olmo2Config, is_torch_available
   22: from transformers.generation.configuration_utils import GenerationConfig
   23: from transformers.models.auto.tokenization_auto import AutoTokenizer
   24: from transformers.testing_utils import (
   25      Expectations,

   41  
   42:     from transformers import (
   43          Olmo2ForCausalLM,

  289  
  290:         from transformers.integrations.executorch import (
  291              TorchExportableModuleWithStaticCache,

train_real_world/transformers_4573/tests/models/olmo3/test_modeling_olmo3.py:
   22  
   23: from transformers import is_torch_available, set_seed
   24: from transformers.generation.configuration_utils import GenerationConfig
   25: from transformers.models.auto.tokenization_auto import AutoTokenizer
   26: from transformers.testing_utils import (
   27      Expectations,

   40  
   41:     from transformers import (
   42          Olmo3ForCausalLM,

   44      )
   45:     from transformers.models.olmo3.modeling_olmo3 import Olmo3RotaryEmbedding
   46  

  224  
  225:         from transformers.integrations.executorch import (
  226              TorchExportableModuleWithStaticCache,

train_real_world/transformers_4573/tests/models/olmoe/test_modeling_olmoe.py:
  17  
  18: from transformers import OlmoeConfig, is_torch_available
  19: from transformers.models.auto.tokenization_auto import AutoTokenizer
  20: from transformers.models.gpt_neox.tokenization_gpt_neox import GPTNeoXTokenizer as GPTNeoXTokenizerFast
  21: from transformers.testing_utils import (
  22      require_tokenizers,

  36  
  37:     from transformers import (
  38          OlmoeForCausalLM,

train_real_world/transformers_4573/tests/models/omdet_turbo/test_modeling_omdet_turbo.py:
  22  
  23: from transformers import OmDetTurboConfig, is_torch_available, is_vision_available
  24: from transformers.feature_extraction_utils import BatchFeature
  25: from transformers.testing_utils import (
  26      require_timm,

  42  
  43:     from transformers import OmDetTurboForObjectDetection
  44  

  48  
  49:     from transformers import AutoProcessor
  50  

train_real_world/transformers_4573/tests/models/omdet_turbo/test_processing_omdet_turbo.py:
  17  
  18: from transformers import OmDetTurboProcessor
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torch_available
  21  

  30  
  31:     from transformers.models.omdet_turbo.modeling_omdet_turbo import OmDetTurboObjectDetectionOutput
  32  

train_real_world/transformers_4573/tests/models/oneformer/test_image_processing_oneformer.py:
   23  
   24: from transformers.testing_utils import require_torch, require_vision
   25: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
   26  

   33      if is_vision_available():
   34:         from transformers import OneFormerImageProcessor
   35  
   36      if is_torchvision_available():
   37:         from transformers import OneFormerImageProcessorFast
   38:         from transformers.models.oneformer.image_processing_oneformer import binary_mask_to_rle, prepare_metadata
   39:         from transformers.models.oneformer.modeling_oneformer import OneFormerForUniversalSegmentationOutput
   40  

  158  
  159: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
  160  def prepare_semantic_single_inputs():

  165  
  166: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  167  def prepare_semantic_batch_inputs():

train_real_world/transformers_4573/tests/models/oneformer/test_modeling_oneformer.py:
  22  from tests.test_modeling_common import floats_tensor
  23: from transformers import AutoModelForImageClassification, OneFormerConfig, is_torch_available, is_vision_available
  24: from transformers.testing_utils import (
  25      Expectations,

  44  
  45:     from transformers import OneFormerForUniversalSegmentation, OneFormerModel
  46  
  47      if is_vision_available():
  48:         from transformers import OneFormerProcessor
  49  

train_real_world/transformers_4573/tests/models/oneformer/test_processing_oneformer.py:
  24  
  25: from transformers.testing_utils import check_json_file_has_correct_format, require_torch, require_vision
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  34      if is_vision_available():
  35:         from transformers import CLIPTokenizer, OneFormerImageProcessor, OneFormerProcessor
  36:         from transformers.models.oneformer.image_processing_oneformer import binary_mask_to_rle
  37:         from transformers.models.oneformer.modeling_oneformer import OneFormerForUniversalSegmentationOutput
  38  

train_real_world/transformers_4573/tests/models/openai/test_modeling_openai.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  29  
  30:     from transformers import (
  31          OpenAIGPTConfig,

train_real_world/transformers_4573/tests/models/openai/test_tokenization_openai.py:
  17  
  18: from transformers import OpenAIGPTTokenizer
  19: from transformers.testing_utils import require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/opt/test_modeling_opt.py:
  21  
  22: from transformers import OPTConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      require_torch,

  39  
  40:     from transformers import (
  41          GPT2Tokenizer,

train_real_world/transformers_4573/tests/models/ovis2/test_image_processing_ovis2.py:
  17  
  18: from transformers.image_utils import SizeDict
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  21  

  28  if is_vision_available():
  29:     from transformers import Ovis2ImageProcessor
  30  
  31      if is_torchvision_available():
  32:         from transformers import Ovis2ImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/ovis2/test_modeling_ovis2.py:
  19  
  20: from transformers import (
  21      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      cleanup,

train_real_world/transformers_4573/tests/models/ovis2/test_processor_ovis2.py:
  18  
  19: from transformers.testing_utils import require_av, require_vision
  20: from transformers.utils import is_vision_available
  21  

  25  if is_vision_available():
  26:     from transformers import (
  27          AutoProcessor,

train_real_world/transformers_4573/tests/models/owlv2/test_image_processing_owlv2.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision, slow
  19: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  20  

  26  
  27:     from transformers import AutoProcessor, Owlv2ForObjectDetection, Owlv2ImageProcessor
  28  

  31  
  32:     from transformers import Owlv2ImageProcessorFast
  33  

train_real_world/transformers_4573/tests/models/owlv2/test_modeling_owlv2.py:
  22  
  23: from transformers import Owlv2Config, Owlv2TextConfig, Owlv2VisionConfig
  24: from transformers.testing_utils import (
  25      require_torch,

  31  )
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  47  
  48:     from transformers import Owlv2ForObjectDetection, Owlv2Model, Owlv2TextModel, Owlv2VisionModel
  49  

  53  
  54:     from transformers import OwlViTImageProcessor, OwlViTProcessor
  55  

train_real_world/transformers_4573/tests/models/owlv2/test_processing_owlv2.py:
  2  
  3: from transformers import Owlv2Processor
  4: from transformers.testing_utils import require_scipy
  5  

train_real_world/transformers_4573/tests/models/owlvit/test_image_processing_owlvit.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import OwlViTImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import OwlViTImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/owlvit/test_modeling_owlvit.py:
  22  
  23: from transformers import OwlViTConfig, OwlViTTextConfig, OwlViTVisionConfig
  24: from transformers.testing_utils import (
  25      require_torch,

  31  )
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  47  
  48:     from transformers import OwlViTForObjectDetection, OwlViTModel, OwlViTTextModel, OwlViTVisionModel
  49  

  53  
  54:     from transformers import OwlViTProcessor
  55  

train_real_world/transformers_4573/tests/models/owlvit/test_processing_owlvit.py:
  20  
  21: from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES
  22: from transformers.testing_utils import require_vision
  23: from transformers.utils import is_vision_available
  24  

  28  if is_vision_available():
  29:     from transformers import OwlViTProcessor
  30  

train_real_world/transformers_4573/tests/models/paddleocr_vl/test_modeling_paddleocr_vl.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  28  )
  29: from transformers.testing_utils import (
  30      backend_empty_cache,

train_real_world/transformers_4573/tests/models/paligemma/test_modeling_paligemma.py:
  20  
  21: from transformers import (
  22      PaliGemmaConfig,

  28  )
  29: from transformers.testing_utils import (
  30      Expectations,

train_real_world/transformers_4573/tests/models/paligemma/test_processing_paligemma.py:
  16  
  17: from transformers import PaliGemmaProcessor
  18: from transformers.testing_utils import get_tests_dir, require_torch, require_vision
  19  

train_real_world/transformers_4573/tests/models/paligemma2/test_modeling_paligemma2.py:
  18  
  19: from transformers import (
  20      PaliGemmaConfig,

  23  )
  24: from transformers.testing_utils import (
  25      require_torch,

train_real_world/transformers_4573/tests/models/parakeet/test_feature_extraction_parakeet.py:
  21  
  22: from transformers import ParakeetFeatureExtractor
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils import is_datasets_available, is_torch_available
  25  

train_real_world/transformers_4573/tests/models/parakeet/test_modeling_parakeet.py:
  20  
  21: from transformers import is_datasets_available, is_torch_available
  22: from transformers.testing_utils import cleanup, require_torch, slow, torch_device
  23  

  33  
  34:     from transformers import (
  35          AutoProcessor,

train_real_world/transformers_4573/tests/models/parakeet/test_processing_parakeet.py:
  16  
  17: from transformers import ParakeetProcessor
  18: from transformers.testing_utils import require_torch, require_torchaudio
  19  

train_real_world/transformers_4573/tests/models/parakeet/test_tokenization_parakeet.py:
  17  
  18: from transformers.models.parakeet import ParakeetTokenizerFast
  19  

train_real_world/transformers_4573/tests/models/patchtsmixer/test_modeling_patchtsmixer.py:
  25  
  26: from transformers import is_torch_available
  27: from transformers.models.auto import get_values
  28: from transformers.testing_utils import is_flaky, require_torch, slow, torch_device
  29: from transformers.utils import check_torch_load_is_safe
  30  

  40  
  41:     from transformers import (
  42          MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING,

  50      )
  51:     from transformers.models.patchtsmixer.modeling_patchtsmixer import (
  52          PatchTSMixerEncoder,

train_real_world/transformers_4573/tests/models/patchtst/test_modeling_patchtst.py:
  22  
  23: from transformers import is_torch_available
  24: from transformers.models.auto import get_values
  25: from transformers.testing_utils import is_flaky, require_read_token, require_torch, slow, torch_device
  26: from transformers.utils import check_torch_load_is_safe
  27  

  37  
  38:     from transformers import (
  39          MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING,

train_real_world/transformers_4573/tests/models/pe_audio/test_modeling_pe_audio.py:
  15  
  16: from transformers import PeAudioConfig, PeAudioEncoderConfig
  17: from transformers.audio_utils import load_audio
  18: from transformers.testing_utils import (
  19      require_torch,

  22  )
  23: from transformers.utils import is_torch_available
  24  

  36  
  37:     from transformers import (
  38          ModernBertConfig,

train_real_world/transformers_4573/tests/models/pe_audio_video/test_modeling_pe_audio_video.py:
  17  
  18: from transformers import PeAudioVideoEncoderConfig, PeAudioVideoProcessor
  19: from transformers.testing_utils import (
  20      cleanup,

  24  )
  25: from transformers.utils import is_torch_available
  26  

  37  
  38:     from transformers import (
  39          PeAudioVideoEncoder,

train_real_world/transformers_4573/tests/models/pe_video/test_modeling_pe_video.py:
  15  
  16: from transformers import PeVideoConfig, PeVideoEncoderConfig
  17: from transformers.testing_utils import (
  18      require_torch,

  21  )
  22: from transformers.utils import is_torch_available
  23  

  35  
  36:     from transformers import (
  37          ModernBertConfig,

train_real_world/transformers_4573/tests/models/pegasus/test_modeling_pegasus.py:
  19  
  20: from transformers import PegasusConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      require_sentencepiece,

  39  
  40:     from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusModel
  41:     from transformers.models.pegasus.modeling_pegasus import PegasusDecoder, PegasusEncoder, PegasusForCausalLM
  42  

train_real_world/transformers_4573/tests/models/pegasus/test_tokenization_pegasus.py:
  17  
  18: from transformers import PegasusTokenizer
  19: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, require_torch, slow
  20: from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor
  21  

train_real_world/transformers_4573/tests/models/pegasus_x/test_modeling_pegasus_x.py:
  21  
  22: from transformers import is_torch_available
  23: from transformers.testing_utils import (
  24      require_sentencepiece,

  40  
  41:     from transformers import PegasusTokenizer, PegasusXConfig, PegasusXForConditionalGeneration, PegasusXModel
  42:     from transformers.models.pegasus_x.modeling_pegasus_x import PegasusXDecoder, PegasusXEncoder
  43  

train_real_world/transformers_4573/tests/models/perceiver/test_image_processing_perceiver.py:
  20  
  21: from transformers.image_utils import PILImageResampling
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  30  
  31:     from transformers import PerceiverImageProcessor
  32  
  33      if is_torchvision_available():
  34:         from transformers import PerceiverImageProcessorFast
  35  

train_real_world/transformers_4573/tests/models/perceiver/test_modeling_perceiver.py:
  25  
  26: from transformers import PerceiverConfig
  27: from transformers.testing_utils import (
  28      IS_ROCM_SYSTEM,

  34  )
  35: from transformers.utils import is_torch_available, is_vision_available
  36  

  45  
  46:     from transformers import (
  47          PerceiverForImageClassificationConvProcessing,

  56      )
  57:     from transformers.models.auto.modeling_auto import (
  58          MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,

  68  
  69:     from transformers import PerceiverImageProcessor
  70  

train_real_world/transformers_4573/tests/models/perceiver/test_tokenization_perceiver.py:
  20  
  21: from transformers import BatchEncoding, PerceiverTokenizer
  22  

train_real_world/transformers_4573/tests/models/perception_lm/test_image_processing_perception_lm.py:
  18  
  19: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  32      if is_torchvision_available():
  33:         from transformers import PerceptionLMImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/perception_lm/test_modeling_perception_lm.py:
  19  
  20: from transformers import (
  21      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      cleanup,

train_real_world/transformers_4573/tests/models/perception_lm/test_processing_perception_lm.py:
  16  
  17: from transformers import (
  18      PerceptionLMProcessor,
  19  )
  20: from transformers.testing_utils import require_read_token, require_vision
  21: from transformers.utils import is_torch_available
  22  

train_real_world/transformers_4573/tests/models/perception_lm/test_video_processing_perception_lm.py:
  17  
  18: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import PerceptionLMVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/persimmon/test_modeling_persimmon.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

  33  
  34:     from transformers import (
  35          AutoTokenizer,

train_real_world/transformers_4573/tests/models/phi/test_modeling_phi.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      require_torch,

  31  
  32:     from transformers import (
  33          AutoTokenizer,

train_real_world/transformers_4573/tests/models/phi3/test_modeling_phi3.py:
   20  
   21: from transformers import StaticCache, is_torch_available
   22: from transformers.models.auto.configuration_auto import AutoConfig
   23: from transformers.testing_utils import (
   24      Expectations,

   35  
   36:     from transformers import (
   37          AutoTokenizer,

  323      def test_export_static_cache(self):
  324:         from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  325  

  328  
  329:         from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
  330:         from transformers.integrations.executorch import (
  331              TorchExportableModuleWithStaticCache,

  388          # Static Cache + export
  389:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  390  

train_real_world/transformers_4573/tests/models/phi4_multimodal/test_feature_extraction_phi4_multimodal.py:
  24  
  25: from transformers import Phi4MultimodalFeatureExtractor
  26: from transformers.testing_utils import check_json_file_has_correct_format, require_torch
  27: from transformers.utils.import_utils import is_torch_available
  28  

train_real_world/transformers_4573/tests/models/phi4_multimodal/test_image_processing_phi4_multimodal.py:
  25  
  26: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  27: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  28  

  38      if is_torchvision_available():
  39:         from transformers import Phi4MultimodalImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py:
  20  
  21: from transformers import (
  22      AutoModelForCausalLM,

  32  )
  33: from transformers.testing_utils import (
  34      Expectations,

  41  )
  42: from transformers.utils import is_torchcodec_available
  43  

train_real_world/transformers_4573/tests/models/phimoe/test_modeling_phimoe.py:
  20  
  21: from transformers import StaticCache, is_torch_available
  22: from transformers.testing_utils import (
  23      cleanup,

  34  
  35:     from transformers import (
  36          AutoTokenizer,

train_real_world/transformers_4573/tests/models/phobert/test_tokenization_phobert.py:
  17  
  18: from transformers.models.phobert.tokenization_phobert import VOCAB_FILES_NAMES, PhobertTokenizer
  19  

train_real_world/transformers_4573/tests/models/pix2struct/test_image_processing_pix2struct.py:
  20  
  21: from transformers.image_utils import load_image
  22: from transformers.testing_utils import require_torch, require_torch_accelerator, require_vision, slow, torch_device
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  34  
  35:     from transformers import Pix2StructImageProcessor
  36  
  37      if is_torchvision_available():
  38:         from transformers import Pix2StructImageProcessorFast
  39  

train_real_world/transformers_4573/tests/models/pix2struct/test_modeling_pix2struct.py:
  23  
  24: from transformers import Pix2StructConfig, Pix2StructTextConfig, Pix2StructVisionConfig
  25: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  42  
  43:     from transformers import (
  44          Pix2StructForConditionalGeneration,

train_real_world/transformers_4573/tests/models/pix2struct/test_processing_pix2struct.py:
  15  
  16: from transformers.testing_utils import require_torch, require_vision
  17: from transformers.utils import is_vision_available
  18  

  22  if is_vision_available():
  23:     from transformers import (
  24          Pix2StructProcessor,

train_real_world/transformers_4573/tests/models/pixio/test_modeling_pixio.py:
  18  
  19: from transformers import PixioConfig
  20: from transformers.testing_utils import (
  21      require_torch,

  25  )
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  37  
  38:     from transformers import PixioBackbone, PixioModel
  39  

  43  
  44:     from transformers import AutoImageProcessor
  45  

train_real_world/transformers_4573/tests/models/pixtral/test_image_processing_pixtral.py:
  20  
  21: from transformers.image_utils import load_image
  22: from transformers.testing_utils import (
  23      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  30  

  40  
  41:     from transformers import PixtralImageProcessor
  42  
  43      if is_torchvision_available():
  44:         from transformers import PixtralImageProcessorFast
  45  

train_real_world/transformers_4573/tests/models/pixtral/test_modeling_pixtral.py:
  17  
  18: from transformers import (
  19      PixtralVisionConfig,

  22  )
  23: from transformers.testing_utils import (
  24      require_torch,

train_real_world/transformers_4573/tests/models/pixtral/test_processing_pixtral.py:
  19  
  20: from transformers.testing_utils import require_vision
  21: from transformers.utils import is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import PixtralProcessor
  28  

train_real_world/transformers_4573/tests/models/plbart/test_modeling_plbart.py:
  20  
  21: from transformers import PLBartConfig, is_torch_available
  22: from transformers.testing_utils import (
  23      require_sentencepiece,

  39  
  40:     from transformers import (
  41          AutoTokenizer,

  46      )
  47:     from transformers.models.plbart.modeling_plbart import PLBartDecoder, PLBartEncoder
  48  

train_real_world/transformers_4573/tests/models/plbart/test_tokenization_plbart.py:
  17  
  18: from transformers import SPIECE_UNDERLINE, BatchEncoding, PLBartTokenizer, is_torch_available
  19: from transformers.testing_utils import (
  20      get_tests_dir,

  33  if is_torch_available():
  34:     from transformers.models.plbart.modeling_plbart import shift_tokens_right
  35  

train_real_world/transformers_4573/tests/models/poolformer/test_image_processing_poolformer.py:
  16  
  17: from transformers.testing_utils import require_torch, require_vision
  18: from transformers.utils import is_torchvision_available, is_vision_available
  19  

  23  if is_vision_available():
  24:     from transformers import PoolFormerImageProcessor
  25  
  26      if is_torchvision_available():
  27:         from transformers import PoolFormerImageProcessorFast
  28  

train_real_world/transformers_4573/tests/models/poolformer/test_modeling_poolformer.py:
  17  
  18: from transformers import is_torch_available, is_vision_available
  19: from transformers.models.auto import get_values
  20: from transformers.testing_utils import Expectations, require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import MODEL_MAPPING, PoolFormerConfig, PoolFormerForImageClassification, PoolFormerModel
  31  

  35  
  36:     from transformers import PoolFormerImageProcessor
  37  

train_real_world/transformers_4573/tests/models/pop2piano/test_feature_extraction_pop2piano.py:
  21  
  22: from transformers.testing_utils import (
  23      check_json_file_has_correct_format,

  28  )
  29: from transformers.utils.import_utils import (
  30      is_essentia_available,

  45  
  46:     from transformers import Pop2PianoFeatureExtractor
  47  

train_real_world/transformers_4573/tests/models/pop2piano/test_modeling_pop2piano.py:
   20  
   21: from transformers import Pop2PianoConfig
   22: from transformers.feature_extraction_utils import BatchFeature
   23: from transformers.testing_utils import (
   24      require_essentia,

   30  )
   31: from transformers.utils import is_essentia_available, is_librosa_available, is_scipy_available, is_torch_available
   32  

   40  
   41:     from transformers import Pop2PianoForConditionalGeneration
   42  

  580          if is_librosa_available() and is_scipy_available() and is_essentia_available() and is_torch_available():
  581:             from transformers import Pop2PianoProcessor
  582  

  608          if is_librosa_available() and is_scipy_available() and is_essentia_available() and is_torch_available():
  609:             from transformers import Pop2PianoFeatureExtractor, Pop2PianoTokenizer
  610  

train_real_world/transformers_4573/tests/models/pop2piano/test_processing_pop2piano.py:
  22  
  23: from transformers.testing_utils import (
  24      require_essentia,

  29  )
  30: from transformers.tokenization_python import BatchEncoding
  31: from transformers.utils.import_utils import (
  32      is_essentia_available,

  50  
  51:     from transformers import (
  52          Pop2PianoFeatureExtractor,

train_real_world/transformers_4573/tests/models/pop2piano/test_tokenization_pop2piano.py:
  21  
  22: from transformers.feature_extraction_utils import BatchFeature
  23: from transformers.testing_utils import (
  24      is_pretty_midi_available,

  28  )
  29: from transformers.tokenization_python import BatchEncoding
  30  

  39  
  40:     from transformers import Pop2PianoTokenizer
  41  

train_real_world/transformers_4573/tests/models/prompt_depth_anything/test_image_processing_prompt_depth_anything.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torchvision_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import PromptDepthAnythingImageProcessor
  28  
  29      if is_torchvision_available():
  30:         from transformers import PromptDepthAnythingImageProcessorFast
  31  

train_real_world/transformers_4573/tests/models/prompt_depth_anything/test_modeling_prompt_depth_anything.py:
  20  
  21: from transformers import Dinov2Config, PromptDepthAnythingConfig
  22: from transformers.file_utils import is_torch_available, is_vision_available
  23: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  24: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  25: from transformers.utils.import_utils import get_torch_major_and_minor_version
  26  

  34  
  35:     from transformers import PromptDepthAnythingForDepthEstimation
  36  

  40  
  41:     from transformers import AutoImageProcessor
  42  

train_real_world/transformers_4573/tests/models/prophetnet/test_modeling_prophetnet.py:
  17  
  18: from transformers import ProphetNetConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  29  
  30:     from transformers import (
  31          ProphetNetDecoder,

  37      )
  38:     from transformers.modeling_outputs import BaseModelOutput
  39  

train_real_world/transformers_4573/tests/models/prophetnet/test_tokenization_prophetnet.py:
  18  
  19: from transformers import BatchEncoding
  20: from transformers.models.bert.tokenization_bert_legacy import (
  21      BasicTokenizer,

  26  )
  27: from transformers.models.prophetnet.tokenization_prophetnet import VOCAB_FILES_NAMES, ProphetNetTokenizer
  28: from transformers.testing_utils import require_torch, slow
  29  

train_real_world/transformers_4573/tests/models/pvt/test_image_processing_pvt.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import PvtImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import PvtImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/pvt/test_modeling_pvt.py:
  17  
  18: from transformers import is_torch_available, is_vision_available
  19: from transformers.testing_utils import (
  20      Expectations,

  36  
  37:     from transformers import PvtConfig, PvtForImageClassification, PvtImageProcessor, PvtModel
  38:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  39  

train_real_world/transformers_4573/tests/models/pvt_v2/test_modeling_pvt_v2.py:
  19  
  20: from transformers import PvtV2Backbone, PvtV2Config, is_torch_available, is_vision_available
  21: from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  22: from transformers.testing_utils import (
  23      require_accelerate,

  39  
  40:     from transformers import AutoImageProcessor, PvtV2ForImageClassification, PvtV2Model
  41  

train_real_world/transformers_4573/tests/models/qwen2/test_modeling_qwen2.py:
   21  
   22: from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed
   23: from transformers.generation.configuration_utils import GenerationConfig
   24: from transformers.testing_utils import (
   25      Expectations,

   37  
   38:     from transformers import (
   39          Qwen2ForCausalLM,

  204  
  205:         from transformers.integrations.executorch import (
  206              TorchExportableModuleWithStaticCache,

  260          # Static Cache + export
  261:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  262  

train_real_world/transformers_4573/tests/models/qwen2/test_tokenization_qwen2.py:
  18  from tests.test_tokenization_common import TokenizerTesterMixin
  19: from transformers.models.qwen2.tokenization_qwen2 import Qwen2Tokenizer
  20: from transformers.testing_utils import (
  21      require_tokenizers,

train_real_world/transformers_4573/tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py:
  26  
  27: from transformers import (
  28      AutoProcessor,

  34  )
  35: from transformers.testing_utils import (
  36      Expectations,

train_real_world/transformers_4573/tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py:
  21  
  22: from transformers import (
  23      Qwen2_5OmniProcessor,
  24  )
  25: from transformers.testing_utils import (
  26      require_av,

  32  )
  33: from transformers.utils import is_torch_available
  34  

train_real_world/transformers_4573/tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py:
  22  
  23: from transformers import (
  24      AutoProcessor,

  30  )
  31: from transformers.image_utils import load_image
  32: from transformers.testing_utils import (
  33      Expectations,

  41  )
  42: from transformers.utils import is_cv2_available
  43  

train_real_world/transformers_4573/tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py:
  19  
  20: from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import Qwen2_5_VLProcessor
  28  

train_real_world/transformers_4573/tests/models/qwen2_audio/test_modeling_qwen2_audio.py:
  23  
  24: from transformers import (
  25      AutoProcessor,

  29  )
  30: from transformers.testing_utils import (
  31      cleanup,

train_real_world/transformers_4573/tests/models/qwen2_audio/test_processing_qwen2_audio.py:
  15  
  16: from transformers import AutoProcessor, AutoTokenizer, Qwen2AudioProcessor
  17: from transformers.testing_utils import require_torch, require_torchaudio
  18  

train_real_world/transformers_4573/tests/models/qwen2_moe/test_modeling_qwen2_moe.py:
  19  
  20: from transformers import AutoTokenizer, is_torch_available, set_seed
  21: from transformers.testing_utils import (
  22      cleanup,

  36  
  37:     from transformers import (
  38          Qwen2MoeForCausalLM,

train_real_world/transformers_4573/tests/models/qwen2_vl/test_image_processing_qwen2_vl.py:
  20  
  21: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, load_image
  22: from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize
  23: from transformers.testing_utils import require_torch, require_vision
  24: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  25  

  35  
  36:     from transformers import Qwen2VLImageProcessor
  37  
  38      if is_torchvision_available():
  39:         from transformers import Qwen2VLImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/qwen2_vl/test_modeling_qwen2_vl.py:
  23  
  24: from transformers import (
  25      AutoProcessor,

  31  )
  32: from transformers.testing_utils import (
  33      Expectations,

train_real_world/transformers_4573/tests/models/qwen2_vl/test_processing_qwen2_vl.py:
  19  
  20: from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import Qwen2VLProcessor
  28  

train_real_world/transformers_4573/tests/models/qwen2_vl/test_video_processing_qwen2_vl.py:
  19  
  20: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  32  
  33:     from transformers.image_utils import get_image_size
  34:     from transformers.models.qwen2_vl.video_processing_qwen2_vl import smart_resize
  35  
  36      if is_torchvision_available():
  37:         from transformers import Qwen2VLVideoProcessor
  38  

train_real_world/transformers_4573/tests/models/qwen3/test_modeling_qwen3.py:
   20  
   21: from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed
   22: from transformers.generation.configuration_utils import GenerationConfig
   23: from transformers.testing_utils import (
   24      Expectations,

   36  
   37:     from transformers import (
   38          Qwen3ForCausalLM,

  198  
  199:         from transformers.integrations.executorch import (
  200              TorchExportableModuleWithStaticCache,

  254          # Static Cache + export
  255:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  256  

train_real_world/transformers_4573/tests/models/qwen3_moe/test_modeling_qwen3_moe.py:
  19  
  20: from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed
  21: from transformers.testing_utils import (
  22      cleanup,

  36  
  37:     from transformers import (
  38          Qwen3MoeForCausalLM,

train_real_world/transformers_4573/tests/models/qwen3_next/test_modeling_qwen3_next.py:
  20  
  21: from transformers import is_torch_available
  22: from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device
  23  

  27  
  28:     from transformers import (
  29          Cache,

  31      )
  32:     from transformers.models.qwen3_next.modeling_qwen3_next import Qwen3NextDynamicCache
  33  

train_real_world/transformers_4573/tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py:
  26  
  27: from transformers import (
  28      AutoProcessor,

  34  )
  35: from transformers.testing_utils import (
  36      Expectations,

train_real_world/transformers_4573/tests/models/qwen3_omni_moe/test_processing_qwen3_omni_moe.py:
  22  
  23: from transformers import (
  24      Qwen3OmniMoeProcessor,
  25  )
  26: from transformers.testing_utils import (
  27      require_av,

  33  )
  34: from transformers.utils import is_torch_available
  35  

train_real_world/transformers_4573/tests/models/qwen3_vl/test_modeling_qwen3_vl.py:
  18  
  19: from transformers import (
  20      Qwen3VLConfig,

  24  )
  25: from transformers.testing_utils import (
  26      require_torch,

train_real_world/transformers_4573/tests/models/qwen3_vl/test_processing_qwen3_vl.py:
  19  
  20: from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  26  if is_vision_available():
  27:     from transformers import Qwen3VLProcessor
  28  

train_real_world/transformers_4573/tests/models/qwen3_vl/test_video_processing_qwen3_vl.py:
  19  
  20: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  30  if is_vision_available() and is_torchvision_available():
  31:     from transformers import Qwen3VLVideoProcessor
  32:     from transformers.models.qwen3_vl.video_processing_qwen3_vl import smart_resize
  33  

train_real_world/transformers_4573/tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py:
  20  
  21: from transformers import (
  22      AutoProcessor,

  27  )
  28: from transformers.testing_utils import (
  29      Expectations,

train_real_world/transformers_4573/tests/models/rag/test_modeling_rag.py:
  25  
  26: from transformers import BartTokenizer, T5Tokenizer
  27: from transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer
  28: from transformers.testing_utils import (
  29      cleanup,

  37  )
  38: from transformers.utils import is_datasets_available, is_faiss_available, is_torch_available
  39  

  52  
  53:     from transformers import (
  54          AutoConfig,

  64      )
  65:     from transformers.modeling_outputs import BaseModelOutput
  66  

train_real_world/transformers_4573/tests/models/rag/test_retrieval_rag.py:
  23  
  24: from transformers import is_faiss_available
  25: from transformers.models.bart.configuration_bart import BartConfig
  26: from transformers.models.dpr.configuration_dpr import DPRConfig
  27: from transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer
  28: from transformers.models.rag.configuration_rag import RagConfig
  29: from transformers.models.rag.retrieval_rag import CustomHFIndex, RagRetriever
  30: from transformers.models.roberta.tokenization_roberta import RobertaTokenizer as BartTokenizer
  31: from transformers.testing_utils import require_faiss, require_sentencepiece, require_tokenizers, require_torch
  32  

train_real_world/transformers_4573/tests/models/rag/test_tokenization_rag.py:
  20  
  21: from transformers import DPRQuestionEncoderTokenizer, RobertaTokenizer
  22: from transformers.models.bart.configuration_bart import BartConfig
  23: from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES as DPR_VOCAB_FILES_NAMES
  24: from transformers.models.dpr.configuration_dpr import DPRConfig
  25: from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES
  26: from transformers.testing_utils import require_faiss, require_tokenizers, require_torch, slow
  27: from transformers.utils import is_datasets_available, is_faiss_available, is_torch_available
  28  

  30  if is_torch_available() and is_datasets_available() and is_faiss_available():
  31:     from transformers.models.rag.configuration_rag import RagConfig
  32:     from transformers.models.rag.tokenization_rag import RagTokenizer
  33  

train_real_world/transformers_4573/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:
  20  
  21: from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed
  22: from transformers.testing_utils import (
  23      Expectations,

  35  
  36:     from transformers import RecurrentGemmaModel
  37  

train_real_world/transformers_4573/tests/models/reformer/test_modeling_reformer.py:
  16  
  17: from transformers import ReformerConfig, is_torch_available
  18: from transformers.testing_utils import (
  19      require_sentencepiece,

  37  
  38:     from transformers import (
  39          ReformerForMaskedLM,

  45      )
  46:     from transformers.models.reformer.modeling_reformer import ReformerDynamicCache, ReformerLayer
  47  

train_real_world/transformers_4573/tests/models/reformer/test_tokenization_reformer.py:
  3  from tests.test_tokenization_common import TokenizerTesterMixin
  4: from transformers.models.reformer.tokenization_reformer import ReformerTokenizer
  5: from transformers.testing_utils import (
  6      require_sentencepiece,

train_real_world/transformers_4573/tests/models/regnet/test_modeling_regnet.py:
  18  
  19: from transformers import RegNetConfig
  20: from transformers.file_utils import is_torch_available, is_vision_available
  21: from transformers.testing_utils import Expectations, is_flaky, require_torch, require_vision, slow, torch_device
  22  

  30  
  31:     from transformers import RegNetForImageClassification, RegNetModel
  32  

  36  
  37:     from transformers import AutoImageProcessor
  38  

train_real_world/transformers_4573/tests/models/rembert/test_modeling_rembert.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          RemBertConfig,

train_real_world/transformers_4573/tests/models/rembert/test_tokenization_rembert.py:
  18  from tests.test_tokenization_common import TokenizerTesterMixin
  19: from transformers import RemBertTokenizer
  20: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  21  

train_real_world/transformers_4573/tests/models/resnet/test_modeling_resnet.py:
  18  
  19: from transformers import ResNetConfig
  20: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import ResNetBackbone, ResNetForImageClassification, ResNetModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/roberta/test_modeling_roberta.py:
  20  
  21: from transformers import AutoTokenizer, RobertaConfig, is_torch_available
  22: from transformers.testing_utils import TestCasePlus, require_torch, slow, torch_device
  23  

  32  
  33:     from transformers import (
  34          DataCollatorWithFlattening,

  42      )
  43:     from transformers.models.roberta.modeling_roberta import RobertaEmbeddings
  44:     from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
  45  

train_real_world/transformers_4573/tests/models/roberta/test_tokenization_roberta.py:
  18  
  19: from transformers import AutoTokenizer, RobertaTokenizer
  20: from transformers.testing_utils import require_tokenizers
  21  

train_real_world/transformers_4573/tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py:
  19  
  20: from transformers import RobertaPreLayerNormConfig, is_torch_available
  21: from transformers.testing_utils import TestCasePlus, require_torch, slow, torch_device
  22  

  31  
  32:     from transformers import (
  33          DataCollatorWithFlattening,

  41      )
  42:     from transformers.models.roberta_prelayernorm.modeling_roberta_prelayernorm import RobertaPreLayerNormEmbeddings
  43  

train_real_world/transformers_4573/tests/models/roc_bert/test_modeling_roc_bert.py:
  19  
  20: from transformers import RoCBertConfig, is_torch_available
  21: from transformers.models.auto import get_values
  22: from transformers.testing_utils import require_torch, slow, torch_device
  23  

  31  
  32:     from transformers import (
  33          MODEL_FOR_PRETRAINING_MAPPING,

train_real_world/transformers_4573/tests/models/roc_bert/test_tokenization_roc_bert.py:
  19  
  20: from transformers.models.roc_bert.tokenization_roc_bert import (
  21      VOCAB_FILES_NAMES,

  28  )
  29: from transformers.testing_utils import require_tokenizers, slow
  30  

train_real_world/transformers_4573/tests/models/roformer/test_modeling_roformer.py:
  18  
  19: from transformers import RoFormerConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          RoFormerForCausalLM,

  38      )
  39:     from transformers.models.roformer.modeling_roformer import (
  40          RoFormerSelfAttention,

train_real_world/transformers_4573/tests/models/roformer/test_tokenization_roformer.py:
  17  
  18: from transformers import RoFormerTokenizer, RoFormerTokenizerFast
  19: from transformers.testing_utils import require_rjieba, require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/rt_detr/test_image_processing_rt_detr.py:
  16  
  17: from transformers.image_utils import load_image
  18: from transformers.testing_utils import (
  19      require_torch,

  25  )
  26: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  27  

  34  
  35:     from transformers import RTDetrImageProcessor, RTDetrImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/rt_detr/test_modeling_rt_detr_resnet.py:
  16  
  17: from transformers import RTDetrResNetConfig
  18: from transformers.testing_utils import require_torch
  19: from transformers.utils.import_utils import is_torch_available
  20  

  25  if is_torch_available():
  26:     from transformers import RTDetrResNetBackbone
  27  

train_real_world/transformers_4573/tests/models/rt_detr/test_modeling_rt_detr.py:
  24  
  25: from transformers import (
  26      RTDetrConfig,

  31  )
  32: from transformers.testing_utils import (
  33      Expectations,

  48  
  49:     from transformers import RTDetrForObjectDetection, RTDetrModel
  50  

train_real_world/transformers_4573/tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py:
  24  
  25: from transformers import (
  26      RTDetrResNetConfig,

  30  )
  31: from transformers.testing_utils import (
  32      Expectations,

  47  
  48:     from transformers import RTDetrV2ForObjectDetection, RTDetrV2Model
  49  

  52  
  53:     from transformers import RTDetrImageProcessor
  54  

train_real_world/transformers_4573/tests/models/rwkv/test_modeling_rwkv.py:
  18  
  19: from transformers import AutoTokenizer, RwkvConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  30  
  31:     from transformers import (
  32          RwkvForCausalLM,

train_real_world/transformers_4573/tests/models/sam/test_image_processing_sam.py:
   19  
   20: from transformers.file_utils import is_torch_available, is_vision_available
   21: from transformers.testing_utils import require_torch, require_vision
   22: from transformers.utils import is_torchvision_available
   23  

   30  if is_vision_available():
   31:     from transformers import SamImageProcessor
   32  
   33      if is_torchvision_available():
   34:         from transformers import SamImageProcessorFast
   35  

  103  
  104: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
  105  def prepare_semantic_single_inputs():

  110  
  111: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  112  def prepare_semantic_batch_inputs():

train_real_world/transformers_4573/tests/models/sam/test_modeling_sam.py:
  21  
  22: from transformers import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig, pipeline
  23: from transformers.testing_utils import Expectations, cleanup, require_torch, slow, torch_device
  24: from transformers.utils import is_torch_available, is_vision_available
  25  

  34  
  35:     from transformers import SamModel, SamProcessor, SamVisionModel
  36  

train_real_world/transformers_4573/tests/models/sam/test_processing_sam.py:
  17  
  18: from transformers.testing_utils import require_torch, require_torchvision, require_vision
  19: from transformers.utils import is_torch_available, is_vision_available
  20  

  26  
  27:     from transformers import SamProcessor
  28  

  31  
  32:     from transformers.models.sam.image_processing_sam import _mask_to_rle_pytorch
  33  

train_real_world/transformers_4573/tests/models/sam2/test_image_processing_sam2.py:
  19  
  20: from transformers.file_utils import is_torch_available, is_vision_available
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torchvision_available
  23  

  30  if is_vision_available() and is_torchvision_available():
  31:     from transformers import Sam2ImageProcessorFast
  32  

  89  
  90: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs
  91  def prepare_semantic_single_inputs():

  96  
  97: # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs
  98  def prepare_semantic_batch_inputs():

train_real_world/transformers_4573/tests/models/sam2/test_modeling_sam2.py:
  22  
  23: from transformers import (
  24      Sam2Config,

  31  )
  32: from transformers.testing_utils import (
  33      backend_empty_cache,

  37  )
  38: from transformers.utils import is_torch_available, is_vision_available
  39: from transformers.video_utils import load_video
  40  

  49  
  50:     from transformers import Sam2Model, Sam2Processor, Sam2VisionModel
  51  

train_real_world/transformers_4573/tests/models/sam2/test_processor_sam2.py:
  17  
  18: from transformers.testing_utils import (
  19      require_torch,

  22  )
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  28  if is_vision_available():
  29:     from transformers import Sam2Processor
  30  

train_real_world/transformers_4573/tests/models/sam2_video/test_modeling_sam2_video.py:
  21  
  22: from transformers.testing_utils import (
  23      backend_empty_cache,

  26  )
  27: from transformers.utils import is_torch_available, is_vision_available
  28: from transformers.video_utils import load_video
  29  

  33  
  34:     from transformers import Sam2VideoModel, Sam2VideoProcessor
  35  

train_real_world/transformers_4573/tests/models/sam2_video/test_processor_sam2_video.py:
  17  
  18: from transformers.testing_utils import (
  19      require_torch,

  22  )
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  28  if is_vision_available():
  29:     from transformers import Sam2VideoProcessor
  30  

train_real_world/transformers_4573/tests/models/sam3/test_modeling_sam3.py:
  22  
  23: from transformers.testing_utils import (
  24      backend_empty_cache,

  29  )
  30: from transformers.utils import is_torch_available, is_vision_available
  31  

  40  
  41:     from transformers.models.sam3.configuration_sam3 import (
  42          Sam3Config,

  49      )
  50:     from transformers.models.sam3.modeling_sam3 import Sam3Model, Sam3VisionModel
  51:     from transformers.models.sam3.processing_sam3 import Sam3Processor
  52  

train_real_world/transformers_4573/tests/models/sam3_tracker/test_modeling_sam3_tracker.py:
  22  
  23: from transformers import (
  24      Sam3TrackerConfig,

  28  )
  29: from transformers.testing_utils import (
  30      backend_empty_cache,

  34  )
  35: from transformers.utils import is_torch_available, is_vision_available
  36: from transformers.video_utils import load_video
  37  

  46  
  47:     from transformers import Sam3TrackerModel, Sam3TrackerProcessor, Sam3VisionConfig, Sam3ViTConfig
  48  

train_real_world/transformers_4573/tests/models/sam3_tracker_video/test_modeling_sam3_tracker_video.py:
  21  
  22: from transformers.testing_utils import (
  23      backend_empty_cache,

  26  )
  27: from transformers.utils import is_torch_available, is_vision_available
  28: from transformers.video_utils import load_video
  29  

  33  
  34:     from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor
  35  

train_real_world/transformers_4573/tests/models/sam3_video/test_modeling_sam3_video.py:
   19  
   20: from transformers.testing_utils import (
   21      backend_empty_cache,

   24  )
   25: from transformers.utils import is_torch_available
   26: from transformers.video_utils import load_video
   27  

   31  
   32:     from transformers import Sam3VideoModel, Sam3VideoProcessor
   33  

  537          """Test that custom image size can be set and propagates correctly to detector and tracker configs."""
  538:         from transformers import Sam3VideoConfig
  539  

train_real_world/transformers_4573/tests/models/sam_hq/test_modeling_sam_hq.py:
  22  
  23: from transformers import (
  24      SamHQConfig,

  30  )
  31: from transformers.testing_utils import Expectations, cleanup, require_torch, slow, torch_device
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  42  
  43:     from transformers import SamHQModel, SamHQProcessor
  44  

train_real_world/transformers_4573/tests/models/sam_hq/test_processing_sam_hq.py:
  17  
  18: from transformers.testing_utils import require_torch, require_torchvision, require_vision
  19: from transformers.utils import is_torch_available, is_vision_available
  20  

  26  
  27:     from transformers import SamHQProcessor
  28  

train_real_world/transformers_4573/tests/models/seamless_m4t/test_feature_extraction_seamless_m4t.py:
  24  
  25: from transformers import SeamlessM4TFeatureExtractor, is_speech_available
  26: from transformers.testing_utils import check_json_file_has_correct_format, require_torch
  27: from transformers.utils.import_utils import is_torch_available
  28  

train_real_world/transformers_4573/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:
  20  
  21: from transformers import SeamlessM4TConfig, is_speech_available, is_torch_available
  22: from transformers.testing_utils import require_speech, require_torch, slow, torch_device
  23: from transformers.trainer_utils import set_seed
  24  

  37  
  38:     from transformers import (
  39          SeamlessM4TForSpeechToSpeech,

  46  if is_speech_available():
  47:     from transformers import SeamlessM4TProcessor
  48  

train_real_world/transformers_4573/tests/models/seamless_m4t/test_processing_seamless_m4t.py:
  18  
  19: from transformers import SeamlessM4TFeatureExtractor, SeamlessM4TProcessor
  20: from transformers.models.seamless_m4t import (
  21      SeamlessM4TTokenizer,

  23  )
  24: from transformers.testing_utils import require_torch
  25  

train_real_world/transformers_4573/tests/models/seamless_m4t/test_tokenization_seamless_m4t.py:
  16  
  17: from transformers import (
  18      AddedToken,

  22  )
  23: from transformers.testing_utils import (
  24      get_tests_dir,

  37  if is_torch_available():
  38:     from transformers.models.m2m_100.modeling_m2m_100 import shift_tokens_right
  39  

train_real_world/transformers_4573/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:
   20  
   21: from transformers import SeamlessM4Tv2Config, is_speech_available, is_torch_available
   22: from transformers.testing_utils import require_speech, require_torch, slow, torch_device
   23: from transformers.trainer_utils import set_seed
   24  

   36  
   37:     from transformers import (
   38          SeamlessM4Tv2ForSpeechToSpeech,

   45  if is_speech_available():
   46:     from transformers import SeamlessM4TProcessor
   47  

  746  
  747:         from transformers.testing_utils import set_config_for_less_flaky_test, set_model_for_less_flaky_test
  748  

train_real_world/transformers_4573/tests/models/seed_oss/test_modeling_seed_oss.py:
  19  
  20: from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available
  21: from transformers.testing_utils import (
  22      cleanup,

  35  
  36:     from transformers import (
  37          SeedOssModel,

train_real_world/transformers_4573/tests/models/segformer/test_image_processing_segformer.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  29  if is_vision_available():
  30:     from transformers import SegformerImageProcessor
  31  
  32      if is_torchvision_available():
  33:         from transformers import SegformerImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/segformer/test_modeling_segformer.py:
  17  
  18: from transformers import SegformerConfig, is_torch_available, is_vision_available
  19: from transformers.testing_utils import Expectations, require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          SegformerForImageClassification,

  33      )
  34:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  35  

  39  
  40:     from transformers import SegformerImageProcessor
  41  

train_real_world/transformers_4573/tests/models/seggpt/test_image_processing_seggpt.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision, slow
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  28  
  29:     from transformers.models.seggpt.modeling_seggpt import SegGptImageSegmentationOutput
  30  

  33  
  34:     from transformers import SegGptImageProcessor
  35  

train_real_world/transformers_4573/tests/models/seggpt/test_modeling_seggpt.py:
  22  
  23: from transformers import SegGptConfig
  24: from transformers.testing_utils import (
  25      Expectations,

  30  )
  31: from transformers.utils import is_torch_available, is_vision_available
  32  

  41  
  42:     from transformers import SegGptForImageSegmentation, SegGptModel
  43:     from transformers.models.seggpt.modeling_seggpt import SegGptLoss
  44  

  46  if is_vision_available():
  47:     from transformers import SegGptImageProcessor
  48  

train_real_world/transformers_4573/tests/models/sew/test_modeling_sew.py:
  20  
  21: from transformers import SEWConfig, is_torch_available
  22: from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device
  23  

  36  
  37:     from transformers import (
  38          SEWForCTC,

  43      )
  44:     from transformers.models.hubert.modeling_hubert import _compute_mask_indices
  45  

train_real_world/transformers_4573/tests/models/sew_d/test_modeling_sew_d.py:
  20  
  21: from transformers import SEWDConfig, is_torch_available
  22: from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device
  23  

  36  
  37:     from transformers import (
  38          SEWDForCTC,

  43      )
  44:     from transformers.models.hubert.modeling_hubert import _compute_mask_indices
  45  

train_real_world/transformers_4573/tests/models/shieldgemma2/test_modeling_shieldgemma2.py:
  21  
  22: from transformers import BitsAndBytesConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      cleanup,

  32  if is_torch_available():
  33:     from transformers import ShieldGemma2ForImageClassification, ShieldGemma2Processor
  34  

train_real_world/transformers_4573/tests/models/shieldgemma2/test_processing_shieldgemma2.py:
  21  
  22: from transformers import ShieldGemma2Processor
  23: from transformers.testing_utils import get_tests_dir, require_vision
  24  

train_real_world/transformers_4573/tests/models/siglip/test_image_processing_siglip.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import SiglipImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import SiglipImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/siglip/test_modeling_siglip.py:
  23  
  24: from transformers import SiglipConfig, SiglipTextConfig, SiglipVisionConfig
  25: from transformers.testing_utils import (
  26      require_torch,

  30  )
  31: from transformers.utils import (
  32      is_torch_available,

  50  
  51:     from transformers import SiglipForImageClassification, SiglipModel, SiglipTextModel, SiglipVisionModel
  52:     from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer
  53  

  56  
  57:     from transformers import SiglipProcessor
  58  

train_real_world/transformers_4573/tests/models/siglip/test_tokenization_siglip.py:
  17  
  18: from transformers import SPIECE_UNDERLINE, AddedToken, BatchEncoding, SiglipTokenizer
  19: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow
  20  

train_real_world/transformers_4573/tests/models/siglip2/test_image_processing_siglip2.py:
  17  
  18: from transformers.image_utils import load_image
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  28  
  29:     from transformers import Siglip2ImageProcessor
  30  

  32  if is_torchvision_available():
  33:     from transformers import Siglip2ImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/siglip2/test_modeling_siglip2.py:
  23  
  24: from transformers import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig
  25: from transformers.testing_utils import (
  26      Expectations,

  34  )
  35: from transformers.utils import (
  36      is_torch_available,

  54  
  55:     from transformers import Siglip2ForImageClassification, Siglip2Model, Siglip2TextModel, Siglip2VisionModel
  56  

  59  
  60:     from transformers import Siglip2Processor
  61  

train_real_world/transformers_4573/tests/models/smollm3/test_modeling_smollm3.py:
   22  
   23: from transformers import AutoTokenizer, BitsAndBytesConfig, SmolLM3Config, is_torch_available
   24: from transformers.generation.configuration_utils import GenerationConfig
   25: from transformers.testing_utils import (
   26      backend_empty_cache,

   33  )
   34: from transformers.utils.import_utils import is_torch_greater_or_equal
   35  

   39  
   40:     from transformers import (
   41          SmolLM3ForCausalLM,

  154  
  155:         from transformers.integrations.executorch import (
  156              TorchExportableModuleWithStaticCache,

train_real_world/transformers_4573/tests/models/smolvlm/test_image_processing_smolvlm.py:
  20  
  21: from transformers.image_utils import PILImageResampling, load_image
  22: from transformers.testing_utils import require_torch, require_vision
  23: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  24  

  31  
  32:     from transformers import SmolVLMImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import SmolVLMImageProcessorFast
  36  

train_real_world/transformers_4573/tests/models/smolvlm/test_modeling_smolvlm.py:
   23  
   24: from transformers import (
   25      AutoProcessor,

   28  )
   29: from transformers.testing_utils import (
   30      Expectations,

   46  
   47:     from transformers import (
   48          GenerationConfig,

  597      def test_export_smolvlm_vision_encoder(self):
  598:         from transformers import AutoConfig
  599:         from transformers.integrations.executorch import TorchExportableModuleForVLM
  600  

  620      def test_export_smolvlm_connector(self):
  621:         from transformers import AutoConfig
  622:         from transformers.integrations.executorch import TorchExportableModuleForVLM
  623  

  646      def test_export_smolvlm_text_decoder(self):
  647:         from transformers import AutoConfig
  648:         from transformers.integrations.executorch import TorchExportableModuleForVLM
  649  

train_real_world/transformers_4573/tests/models/smolvlm/test_processing_smolvlm.py:
  18  
  19: from transformers import SmolVLMProcessor
  20: from transformers.image_utils import load_image
  21: from transformers.testing_utils import require_av, require_torch, require_vision
  22  

train_real_world/transformers_4573/tests/models/smolvlm/test_video_processing_smolvlm.py:
  17  
  18: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import SmolVLMVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import (
  21      require_deterministic_for_xpu,

  36  
  37:     from transformers import (
  38          BertLMHeadModel,

  42      )
  43:     from transformers.modeling_outputs import BaseModelOutput
  44:     from transformers.models.speech_to_text.modeling_speech_to_text import Speech2TextEncoder
  45  

train_real_world/transformers_4573/tests/models/speech_to_text/test_feature_extraction_speech_to_text.py:
   23  
   24: from transformers import Speech2TextFeatureExtractor
   25: from transformers.testing_utils import check_json_file_has_correct_format, require_torch, require_torchaudio
   26  

  353  
  354:         from transformers.models.speech_to_text.feature_extraction_speech_to_text import is_speech_available
  355  

train_real_world/transformers_4573/tests/models/speech_to_text/test_modeling_speech_to_text.py:
  22  
  23: from transformers import Speech2TextConfig
  24: from transformers.testing_utils import (
  25      is_torch_available,

  43  
  44:     from transformers import Speech2TextForConditionalGeneration, Speech2TextModel, Speech2TextProcessor
  45:     from transformers.models.speech_to_text.modeling_speech_to_text import Speech2TextDecoder, Speech2TextEncoder
  46  

train_real_world/transformers_4573/tests/models/speech_to_text/test_processing_speech_to_text.py:
  20  
  21: from transformers import Speech2TextFeatureExtractor, Speech2TextProcessor, Speech2TextTokenizer
  22: from transformers.models.speech_to_text.tokenization_speech_to_text import VOCAB_FILES_NAMES, save_json
  23: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_torch, require_torchaudio
  24  

train_real_world/transformers_4573/tests/models/speech_to_text/test_tokenization_speech_to_text.py:
  19  
  20: from transformers import SPIECE_UNDERLINE, is_sentencepiece_available
  21: from transformers.models.speech_to_text import Speech2TextTokenizer
  22: from transformers.models.speech_to_text.tokenization_speech_to_text import VOCAB_FILES_NAMES, save_json
  23: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow
  24  

train_real_world/transformers_4573/tests/models/speecht5/test_feature_extraction_speecht5.py:
  21  
  22: from transformers import BatchFeature, SpeechT5FeatureExtractor
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils.import_utils import is_torch_available
  25  

train_real_world/transformers_4573/tests/models/speecht5/test_modeling_speecht5.py:
  21  
  22: from transformers import SpeechT5Config, SpeechT5HifiGanConfig
  23: from transformers.testing_utils import (
  24      is_torch_available,

  31  )
  32: from transformers.trainer_utils import set_seed
  33  

  47  
  48:     from transformers import (
  49          SpeechT5ForSpeechToSpeech,

train_real_world/transformers_4573/tests/models/speecht5/test_processing_speecht5.py:
  19  
  20: from transformers import is_speech_available, is_torch_available
  21: from transformers.models.speecht5 import SpeechT5Tokenizer
  22: from transformers.testing_utils import get_tests_dir, require_speech, require_torch
  23  

  25  if is_speech_available() and is_torch_available():
  26:     from transformers import SpeechT5FeatureExtractor, SpeechT5Processor
  27  

train_real_world/transformers_4573/tests/models/speecht5/test_tokenization_speecht5.py:
  17  
  18: from transformers import SPIECE_UNDERLINE
  19: from transformers.models.speecht5 import SpeechT5Tokenizer
  20: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow
  21: from transformers.tokenization_python import AddedToken
  22  

train_real_world/transformers_4573/tests/models/splinter/test_modeling_splinter.py:
  18  
  19: from transformers import is_torch_available
  20: from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device
  21  

  29  
  30:     from transformers import SplinterConfig, SplinterForPreTraining, SplinterForQuestionAnswering, SplinterModel
  31  

train_real_world/transformers_4573/tests/models/splinter/test_tokenization_splinter.py:
  16  from tests.test_tokenization_common import TokenizerTesterMixin
  17: from transformers.models.splinter.tokenization_splinter import SplinterTokenizer
  18  

train_real_world/transformers_4573/tests/models/squeezebert/test_modeling_squeezebert.py:
  17  
  18: from transformers import SqueezeBertConfig, is_torch_available
  19: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          SqueezeBertForMaskedLM,

train_real_world/transformers_4573/tests/models/stablelm/test_modeling_stablelm.py:
  19  
  20: from transformers import BitsAndBytesConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      require_bitsandbytes,

  32  
  33:     from transformers import (
  34          AutoTokenizer,

train_real_world/transformers_4573/tests/models/starcoder2/test_modeling_starcoder2.py:
  19  
  20: from transformers import BitsAndBytesConfig, is_torch_available
  21: from transformers.testing_utils import (
  22      Expectations,

  34  
  35:     from transformers import (
  36          AutoTokenizer,

train_real_world/transformers_4573/tests/models/superglue/test_image_processing_superglue.py:
  21  
  22: from transformers.testing_utils import (
  23      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  30  

  37  
  38:     from transformers.models.superglue.modeling_superglue import SuperGlueKeypointMatchingOutput
  39  
  40  if is_vision_available():
  41:     from transformers import SuperGlueImageProcessor
  42  
  43      if is_torchvision_available():
  44:         from transformers import SuperGlueImageProcessorFast
  45  

train_real_world/transformers_4573/tests/models/superglue/test_modeling_superglue.py:
  19  
  20: from transformers.models.superglue.configuration_superglue import SuperGlueConfig
  21: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  30  
  31:     from transformers import SuperGlueForKeypointMatching
  32  
  33  if is_vision_available():
  34:     from transformers import AutoImageProcessor
  35  

train_real_world/transformers_4573/tests/models/superpoint/test_image_processing_superpoint.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  20  

  26  
  27:     from transformers.models.superpoint.modeling_superpoint import SuperPointKeypointDescriptionOutput
  28  
  29  if is_vision_available():
  30:     from transformers import SuperPointImageProcessor
  31  
  32      if is_torchvision_available():
  33:         from transformers import SuperPointImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/superpoint/test_modeling_superpoint.py:
  17  
  18: from transformers.models.superpoint.configuration_superpoint import SuperPointConfig
  19: from transformers.testing_utils import is_flaky, require_torch, require_vision, slow, torch_device
  20: from transformers.utils import is_torch_available, is_vision_available
  21  

  28  
  29:     from transformers import (
  30          SuperPointForKeypointDetection,

  35  
  36:     from transformers import AutoImageProcessor
  37  

train_real_world/transformers_4573/tests/models/swiftformer/test_modeling_swiftformer.py:
  18  
  19: from transformers import SwiftFormerConfig
  20: from transformers.testing_utils import (
  21      require_torch,

  25  )
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  36  
  37:     from transformers import SwiftFormerForImageClassification, SwiftFormerModel
  38  

  42  
  43:     from transformers import ViTImageProcessor
  44  

train_real_world/transformers_4573/tests/models/swin/test_modeling_swin.py:
  19  
  20: from transformers import SwinConfig
  21: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  22: from transformers.utils import is_torch_available, is_vision_available
  23  

  33  
  34:     from transformers import SwinBackbone, SwinForImageClassification, SwinForMaskedImageModeling, SwinModel
  35  

  39  
  40:     from transformers import AutoImageProcessor
  41  

train_real_world/transformers_4573/tests/models/swin2sr/test_image_processing_swin2sr.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  22  

  31  
  32:     from transformers import Swin2SRImageProcessor
  33  
  34      if is_torchvision_available():
  35:         from transformers import Swin2SRImageProcessorFast
  36:     from transformers.image_transforms import get_image_size
  37  

train_real_world/transformers_4573/tests/models/swin2sr/test_modeling_swin2sr.py:
  17  
  18: from transformers import Swin2SRConfig
  19: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  20: from transformers.utils import is_torch_available, is_vision_available
  21  

  30  
  31:     from transformers import Swin2SRForImageSuperResolution, Swin2SRModel
  32  

  35  
  36:     from transformers import Swin2SRImageProcessor
  37  

train_real_world/transformers_4573/tests/models/swinv2/test_modeling_swinv2.py:
  20  
  21: from transformers import Swinv2Config
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  34  
  35:     from transformers import Swinv2Backbone, Swinv2ForImageClassification, Swinv2ForMaskedImageModeling, Swinv2Model
  36  

  39  
  40:     from transformers import AutoImageProcessor
  41  

train_real_world/transformers_4573/tests/models/switch_transformers/test_modeling_switch_transformers.py:
  17  
  18: from transformers import SwitchTransformersConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  38  
  39:     from transformers import (
  40          AutoTokenizer,

  46      )
  47:     from transformers.models.switch_transformers.modeling_switch_transformers import (
  48          load_balancing_loss_func,

train_real_world/transformers_4573/tests/models/t5/test_modeling_t5.py:
    21  
    22: from transformers import T5Config, is_torch_available
    23: from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4
    24: from transformers.testing_utils import (
    25      Expectations,

    45  
    46:     from transformers import (
    47          AutoTokenizer,

  1496  
  1497:         from transformers.integrations.executorch import Seq2SeqLMEncoderExportableModule
  1498  

  1533  
  1534:         from transformers import AutoModelForSeq2SeqLM, T5ForConditionalGeneration
  1535:         from transformers.integrations.executorch import Seq2SeqLMDecoderExportableModuleWithStaticCache
  1536  

  1595  
  1596:         from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration
  1597:         from transformers.integrations.executorch import Seq2SeqLMExportableModule
  1598  

train_real_world/transformers_4573/tests/models/t5/test_tokenization_t5.py:
  15  
  16: from transformers import T5Tokenizer
  17: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  18  

train_real_world/transformers_4573/tests/models/t5gemma/test_modeling_t5gemma.py:
  23  
  24: from transformers import T5GemmaConfig, T5GemmaModuleConfig, is_torch_available
  25: from transformers.testing_utils import (
  26      require_flash_attn,

  41  
  42:     from transformers import (
  43          T5GemmaEncoderModel,

train_real_world/transformers_4573/tests/models/t5gemma2/test_modeling_t5gemma2.py:
  20  
  21: from transformers import (
  22      T5Gemma2Config,

  27  )
  28: from transformers.testing_utils import (
  29      require_torch,

  41  
  42:     from transformers import (
  43          T5Gemma2ForConditionalGeneration,

train_real_world/transformers_4573/tests/models/table_transformer/test_modeling_table_transformer.py:
  21  
  22: from transformers import ResNetConfig, TableTransformerConfig, is_torch_available, is_vision_available
  23: from transformers.testing_utils import Expectations, require_timm, require_torch, require_vision, slow, torch_device
  24  

  32  
  33:     from transformers import TableTransformerForObjectDetection, TableTransformerModel
  34  

  38  
  39:     from transformers import AutoImageProcessor
  40  

train_real_world/transformers_4573/tests/models/tapas/test_modeling_tapas.py:
  21  
  22: from transformers import (
  23      MODEL_FOR_CAUSAL_LM_MAPPING,

  33  )
  34: from transformers.models.auto import get_values
  35: from transformers.testing_utils import require_torch, slow, torch_device
  36  

  44  
  45:     from transformers import (
  46          TapasForMaskedLM,

  51      )
  52:     from transformers.models.tapas.modeling_tapas import (
  53          IndexMap,

train_real_world/transformers_4573/tests/models/tapas/test_tokenization_tapas.py:
  23  
  24: from transformers import AddedToken, is_mlx_available, is_torch_available
  25: from transformers.models.tapas.tokenization_tapas import (
  26      VOCAB_FILES_NAMES,

  33  )
  34: from transformers.testing_utils import (
  35      require_pandas,

train_real_world/transformers_4573/tests/models/textnet/test_image_processing_textnet.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import TextNetImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import TextNetImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/textnet/test_modeling_textnet.py:
  20  
  21: from transformers import TextNetConfig
  22: from transformers.models.textnet.image_processing_textnet import TextNetImageProcessor
  23: from transformers.testing_utils import (
  24      require_torch,

  28  )
  29: from transformers.utils import is_torch_available
  30  

  39  
  40:     from transformers import TextNetBackbone, TextNetForImageClassification, TextNetModel
  41  

train_real_world/transformers_4573/tests/models/time_series_transformer/test_modeling_time_series_transformer.py:
  22  
  23: from transformers import is_torch_available
  24: from transformers.testing_utils import is_flaky, require_torch, slow, torch_device
  25: from transformers.utils import check_torch_load_is_safe
  26  

  36  
  37:     from transformers import (
  38          TimeSeriesTransformerConfig,

  41      )
  42:     from transformers.models.time_series_transformer.modeling_time_series_transformer import (
  43          TimeSeriesTransformerDecoder,

train_real_world/transformers_4573/tests/models/timesfm/test_modeling_timesfm.py:
  21  
  22: from transformers import TimesFmConfig, is_torch_available
  23: from transformers.testing_utils import require_torch, slow, torch_device
  24  

  29  if is_torch_available():
  30:     from transformers import TimesFmModelForPrediction
  31  

train_real_world/transformers_4573/tests/models/timesformer/test_modeling_timesformer.py:
  22  
  23: from transformers import TimesformerConfig
  24: from transformers.models.auto import get_values
  25: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  26: from transformers.utils import is_torch_available, is_vision_available
  27  

  36  
  37:     from transformers import (
  38          MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING,

  44  if is_vision_available():
  45:     from transformers import VideoMAEImageProcessor
  46  

train_real_world/transformers_4573/tests/models/timm_backbone/test_modeling_timm_backbone.py:
  18  
  19: from transformers import AutoBackbone
  20: from transformers.testing_utils import require_timm, require_torch, torch_device
  21: from transformers.utils.import_utils import is_torch_available
  22  

  28  if is_torch_available():
  29:     from transformers import TimmBackbone, TimmBackboneConfig
  30  

train_real_world/transformers_4573/tests/models/timm_wrapper/test_image_processing_timm_wrapper.py:
  19  
  20: from transformers.testing_utils import require_torch, require_torchvision, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  29  
  30:     from transformers import TimmWrapperConfig, TimmWrapperImageProcessor
  31  

train_real_world/transformers_4573/tests/models/timm_wrapper/test_modeling_timm_wrapper.py:
   18  
   19: from transformers import pipeline
   20: from transformers.testing_utils import (
   21      Expectations,

   28  )
   29: from transformers.utils.import_utils import is_timm_available, is_torch_available, is_vision_available
   30  

   38  
   39:     from transformers import TimmWrapperConfig, TimmWrapperForImageClassification, TimmWrapperModel
   40  

   48  
   49:     from transformers import TimmWrapperImageProcessor
   50  

  323      def test_inference_image_classification_quantized(self):
  324:         from transformers import BitsAndBytesConfig
  325  

train_real_world/transformers_4573/tests/models/trocr/test_modeling_trocr.py:
  17  
  18: from transformers import TrOCRConfig
  19: from transformers.testing_utils import is_torch_available, require_torch, torch_device
  20  

  29  
  30:     from transformers.models.trocr.modeling_trocr import TrOCRDecoder, TrOCRForCausalLM
  31  

train_real_world/transformers_4573/tests/models/trocr/test_processing_trocr.py:
   4  
   5: from transformers.testing_utils import (
   6      require_sentencepiece,

   9  )
  10: from transformers.utils import is_vision_available
  11  

  15  if is_vision_available():
  16:     from transformers import TrOCRProcessor
  17  

train_real_world/transformers_4573/tests/models/tvp/test_image_processing_tvp.py:
  19  
  20: from transformers.image_transforms import PaddingMode
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  32  
  33:     from transformers import TvpImageProcessor, TvpImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/tvp/test_modeling_tvp.py:
  18  
  19: from transformers import ResNetConfig, TimmBackboneConfig, TvpConfig
  20: from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  34  
  35:     from transformers import TvpForVideoGrounding, TvpModel
  36  

  39  
  40:     from transformers import TvpImageProcessor
  41  

train_real_world/transformers_4573/tests/models/udop/test_modeling_udop.py:
  21  
  22: from transformers import UdopConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      require_sentencepiece,

  41  
  42:     from transformers import UdopEncoderModel, UdopForConditionalGeneration, UdopModel, UdopProcessor
  43  

train_real_world/transformers_4573/tests/models/udop/test_processing_udop.py:
  17  
  18: from transformers import (
  19      UdopProcessor,

  22  )
  23: from transformers.testing_utils import (
  24      require_pytesseract,

  29  )
  30: from transformers.utils import is_pytesseract_available, is_torch_available
  31  

  39  if is_pytesseract_available():
  40:     from transformers import LayoutLMv3ImageProcessor
  41  

train_real_world/transformers_4573/tests/models/udop/test_tokenization_udop.py:
   21  
   22: from transformers import (
   23      AddedToken,

   29  )
   30: from transformers.testing_utils import (
   31      get_tests_dir,

  129      #     # Create tokenizer from SentencePiece model using converter
  130:     #     from transformers.convert_slow_tokenizer import UdopConverter
  131:     #     from transformers.tokenization_utils import PreTrainedTokenizer
  132  

train_real_world/transformers_4573/tests/models/umt5/test_modeling_umt5.py:
  17  
  18: from transformers import UMT5Config, is_torch_available
  19: from transformers.testing_utils import (
  20      require_sentencepiece,

  36  
  37:     from transformers import (
  38          AutoTokenizer,

train_real_world/transformers_4573/tests/models/unispeech/test_modeling_unispeech.py:
  22  
  23: from transformers import UniSpeechConfig, is_torch_available
  24: from transformers.testing_utils import is_flaky, require_torch, require_torchcodec, slow, torch_device
  25  

  38  
  39:     from transformers import (
  40          UniSpeechForCTC,

train_real_world/transformers_4573/tests/models/unispeech_sat/test_modeling_unispeech_sat.py:
  22  
  23: from transformers import UniSpeechSatConfig, is_torch_available
  24: from transformers.testing_utils import require_torch, require_torchcodec, slow, torch_device
  25  

  38  
  39:     from transformers import (
  40          UniSpeechSatForAudioFrameClassification,

train_real_world/transformers_4573/tests/models/univnet/test_feature_extraction_univnet.py:
  23  
  24: from transformers import UnivNetFeatureExtractor
  25: from transformers.testing_utils import check_json_file_has_correct_format, require_torch, slow
  26: from transformers.utils.import_utils import is_torch_available
  27  

train_real_world/transformers_4573/tests/models/univnet/test_modeling_univnet.py:
  20  
  21: from transformers import UnivNetConfig, UnivNetFeatureExtractor
  22: from transformers.testing_utils import (
  23      cleanup,

  40  
  41:     from transformers import UnivNetModel
  42  

train_real_world/transformers_4573/tests/models/upernet/test_modeling_upernet.py:
  19  
  20: from transformers import ConvNextConfig, UperNetConfig
  21: from transformers.testing_utils import (
  22      require_timm,

  28  )
  29: from transformers.utils import is_torch_available, is_vision_available
  30: from transformers.utils.import_utils import get_torch_major_and_minor_version
  31  

  39  
  40:     from transformers import UperNetForSemanticSegmentation
  41  

  43  if is_vision_available():
  44:     from transformers import AutoImageProcessor
  45  

train_real_world/transformers_4573/tests/models/vaultgemma/test_modeling_vaultgemma.py:
   22  
   23: from transformers import (
   24      AutoModelForCausalLM,

   29  )
   30: from transformers.cache_utils import DynamicLayer, DynamicSlidingWindowLayer
   31: from transformers.generation.configuration_utils import GenerationConfig
   32: from transformers.testing_utils import (
   33      Expectations,

   48  
   49:     from transformers import (
   50          VaultGemmaModel,

  121  
  122:         from transformers.integrations.executorch import (
  123              TorchExportableModuleWithStaticCache,

  165          # Static Cache + export
  166:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  167  

train_real_world/transformers_4573/tests/models/video_llama_3/test_image_processing_video_llama_3.py:
  21  
  22: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  23: from transformers.models.video_llama_3.image_processing_video_llama_3 import smart_resize
  24: from transformers.testing_utils import require_torch, require_vision
  25: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  26  

  35  
  36:     from transformers import VideoLlama3ImageProcessor
  37  
  38      if is_torchvision_available():
  39:         from transformers import VideoLlama3ImageProcessorFast
  40  

train_real_world/transformers_4573/tests/models/video_llama_3/test_modeling_video_llama_3.py:
  28  
  29: from transformers import (
  30      AutoProcessor,

  37  )
  38: from transformers.testing_utils import (
  39      Expectations,

  48  )
  49: from transformers.utils import (
  50      is_torch_bf16_available_on_device,

train_real_world/transformers_4573/tests/models/video_llama_3/test_processing_video_llama_3.py:
  21  
  22: from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  28  if is_vision_available():
  29:     from transformers import VideoLlama3Processor
  30  if is_torch_available():

train_real_world/transformers_4573/tests/models/video_llama_3/test_video_processing_video_llama_3.py:
  19  
  20: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  32  
  33:     from transformers.image_utils import get_image_size
  34:     from transformers.models.video_llama_3.video_processing_video_llama_3 import smart_resize
  35  
  36      if is_torchvision_available():
  37:         from transformers import VideoLlama3VideoProcessor
  38  

train_real_world/transformers_4573/tests/models/video_llava/test_modeling_video_llava.py:
  23  
  24: from transformers import (
  25      BitsAndBytesConfig,

  32  )
  33: from transformers.testing_utils import (
  34      cleanup,

train_real_world/transformers_4573/tests/models/video_llava/test_video_processing_video_llava.py:
  17  
  18: from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
  19: from transformers.testing_utils import require_torch, require_vision
  20: from transformers.utils import is_torchvision_available, is_vision_available
  21  

  26      if is_torchvision_available():
  27:         from transformers import VideoLlavaVideoProcessor
  28  

train_real_world/transformers_4573/tests/models/videomae/test_image_processing_videomae.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import VideoMAEImageProcessor
  33  

train_real_world/transformers_4573/tests/models/videomae/test_modeling_videomae.py:
  24  
  25: from transformers import VideoMAEConfig
  26: from transformers.models.auto import get_values
  27: from transformers.testing_utils import (
  28      Expectations,

  36  )
  37: from transformers.utils import check_torch_load_is_safe, is_torch_available, is_vision_available
  38  

  47  
  48:     from transformers import (
  49          MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING,

  56  if is_vision_available():
  57:     from transformers import VideoMAEImageProcessor
  58  

train_real_world/transformers_4573/tests/models/videomae/test_video_processing_videomae.py:
  20  
  21: from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD
  22: from transformers.testing_utils import require_torch, require_torchvision, require_vision
  23: from transformers.utils import is_torchvision_available, is_vision_available
  24  

  29      if is_torchvision_available():
  30:         from transformers import VideoMAEImageProcessor, VideoMAEVideoProcessor
  31  

train_real_world/transformers_4573/tests/models/vilt/test_image_processing_vilt.py:
  20  
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torchvision_available, is_vision_available
  23  

  29  
  30:     from transformers import ViltImageProcessor
  31  
  32      if is_torchvision_available():
  33:         from transformers import ViltImageProcessorFast
  34  

train_real_world/transformers_4573/tests/models/vilt/test_modeling_vilt.py:
  21  
  22: from transformers import ViltConfig, is_torch_available, is_vision_available
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24  

  32  
  33:     from transformers import (
  34          ViltForImageAndTextRetrieval,

  40      )
  41:     from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES
  42  

  46  
  47:     from transformers import ViltProcessor
  48  

train_real_world/transformers_4573/tests/models/vipllava/test_modeling_vipllava.py:
   21  
   22: from transformers import (
   23      AutoProcessor,

   30  )
   31: from transformers.testing_utils import (
   32      cleanup,

   50  
   51: # Copied from transformers.tests.models.llava.test_modeling_llava.LlavaVisionText2TextModelTester with Llava->VipLlava
   52  class VipLlavaVisionText2TextModelTester:

  164  @require_torch
  165: # Copied from transformers.tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest with Llava->VipLlava
  166  class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):

train_real_world/transformers_4573/tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py:
  23  
  24: from transformers import DonutProcessor, NougatProcessor, TrOCRProcessor
  25: from transformers.testing_utils import (
  26      require_levenshtein,

  34  )
  35: from transformers.utils import (
  36      is_torch_available,

  55  
  56:     from transformers import (
  57          AutoTokenizer,

  69      )
  70:     from transformers.modeling_outputs import BaseModelOutput
  71  

  76  
  77:     from transformers import ViTImageProcessor
  78  

train_real_world/transformers_4573/tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py:
  21  
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23: from transformers.utils import is_torch_available, is_vision_available
  24  

  35  
  36:     from transformers import (
  37          BertModel,

  49  
  50:     from transformers import VisionTextDualEncoderProcessor
  51  

train_real_world/transformers_4573/tests/models/vision_text_dual_encoder/test_processing_vision_text_dual_encoder.py:
  17  
  18: from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES, BertTokenizer
  19: from transformers.testing_utils import require_tokenizers, require_vision
  20: from transformers.utils import is_vision_available
  21  

  25  if is_vision_available():
  26:     from transformers import VisionTextDualEncoderProcessor, ViTImageProcessorFast
  27  

train_real_world/transformers_4573/tests/models/visual_bert/test_modeling_visual_bert.py:
  18  
  19: from transformers import VisualBertConfig, is_torch_available
  20: from transformers.testing_utils import require_torch, slow, torch_device
  21  

  29  
  30:     from transformers import (
  31          VisualBertForMultipleChoice,

train_real_world/transformers_4573/tests/models/vit/test_image_processing_vit.py:
  17  
  18: from transformers.testing_utils import require_torch, require_vision
  19: from transformers.utils import is_torchvision_available, is_vision_available
  20  

  24  if is_vision_available():
  25:     from transformers import ViTImageProcessor
  26  
  27      if is_torchvision_available():
  28:         from transformers import ViTImageProcessorFast
  29  

train_real_world/transformers_4573/tests/models/vit/test_modeling_vit.py:
  18  
  19: from transformers import ViTConfig
  20: from transformers.testing_utils import (
  21      require_accelerate,

  28  )
  29: from transformers.utils import is_torch_available, is_vision_available
  30  

  39  
  40:     from transformers import ViTForImageClassification, ViTForMaskedImageModeling, ViTModel
  41  

  45  
  46:     from transformers import ViTImageProcessor
  47  

train_real_world/transformers_4573/tests/models/vit_mae/test_modeling_vit_mae.py:
  23  
  24: from transformers import ViTMAEConfig
  25: from transformers.testing_utils import (
  26      is_flaky,

  33  )
  34: from transformers.utils import is_torch_available, is_vision_available
  35  

  44  
  45:     from transformers import ViTMAEForPreTraining, ViTMAEModel
  46  

  50  
  51:     from transformers import ViTImageProcessor
  52  

train_real_world/transformers_4573/tests/models/vit_msn/test_modeling_vit_msn.py:
  18  
  19: from transformers import ViTMSNConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import ViTMSNForImageClassification, ViTMSNModel
  33  

  37  
  38:     from transformers import ViTImageProcessor
  39  

train_real_world/transformers_4573/tests/models/vitdet/test_modeling_vitdet.py:
  17  
  18: from transformers import VitDetConfig
  19: from transformers.testing_utils import require_torch, torch_device
  20: from transformers.utils import is_torch_available
  21  

  31  
  32:     from transformers import VitDetBackbone, VitDetModel
  33  

train_real_world/transformers_4573/tests/models/vitmatte/test_image_processing_vitmatte.py:
  23  
  24: from transformers.image_utils import load_image
  25: from transformers.testing_utils import (
  26      require_torch,

  31  )
  32: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  33  

  44  
  45:     from transformers import VitMatteImageProcessor
  46  
  47      if is_torchvision_available():
  48:         from transformers import VitMatteImageProcessorFast
  49  

train_real_world/transformers_4573/tests/models/vitmatte/test_modeling_vitmatte.py:
  19  
  20: from transformers import VitMatteConfig
  21: from transformers.testing_utils import (
  22      require_timm,

  26  )
  27: from transformers.utils import is_torch_available, is_vision_available
  28: from transformers.utils.import_utils import get_torch_major_and_minor_version
  29  

  37  
  38:     from transformers import VitDetConfig, VitMatteForImageMatting
  39  

  43  
  44:     from transformers import VitMatteImageProcessor
  45  

train_real_world/transformers_4573/tests/models/vitpose/test_image_processing_vitpose.py:
   19  
   20: from transformers.testing_utils import require_torch, require_vision
   21: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
   22  

   32  
   33:     from transformers import VitPoseImageProcessor
   34  
   35      if is_torchvision_available():
   36:         from transformers import VitPoseImageProcessorFast
   37  

  279  
  280:         from transformers.testing_utils import torch_device
  281  

train_real_world/transformers_4573/tests/models/vitpose/test_modeling_vitpose.py:
  21  
  22: from transformers import VitPoseBackboneConfig, VitPoseConfig
  23: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  24: from transformers.utils import is_torch_available, is_vision_available
  25: from transformers.utils.import_utils import get_torch_major_and_minor_version
  26  

  33  
  34:     from transformers import VitPoseForPoseEstimation
  35  

  39  
  40:     from transformers import VitPoseImageProcessor
  41  

train_real_world/transformers_4573/tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py:
  20  
  21: from transformers import VitPoseBackboneConfig
  22: from transformers.testing_utils import require_torch, torch_device
  23: from transformers.utils import is_torch_available
  24  

  32  
  33:     from transformers import VitPoseBackbone
  34  

train_real_world/transformers_4573/tests/models/vits/test_modeling_vits.py:
  22  
  23: from transformers import PreTrainedConfig, VitsConfig
  24: from transformers.testing_utils import (
  25      Expectations,

  33  )
  34: from transformers.trainer_utils import set_seed
  35  

  48  
  49:     from transformers import VitsModel, VitsTokenizer
  50  

train_real_world/transformers_4573/tests/models/vits/test_tokenization_vits.py:
  21  
  22: from transformers import VitsTokenizer
  23: from transformers.models.vits.tokenization_vits import VOCAB_FILES_NAMES
  24: from transformers.testing_utils import slow
  25  

train_real_world/transformers_4573/tests/models/vivit/test_image_processing_vivit.py:
  19  
  20: from transformers.testing_utils import require_torch, require_vision
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import VivitImageProcessor
  33  

train_real_world/transformers_4573/tests/models/vivit/test_modeling_vivit.py:
  23  
  24: from transformers import VivitConfig
  25: from transformers.models.auto import get_values
  26: from transformers.testing_utils import Expectations, require_torch, require_vision, slow, torch_device
  27: from transformers.utils import is_torch_available, is_vision_available
  28  

  37  
  38:     from transformers import MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING, VivitForVideoClassification, VivitModel
  39  

  41  if is_vision_available():
  42:     from transformers import VivitImageProcessor
  43  

train_real_world/transformers_4573/tests/models/vjepa2/test_modeling_vjepa2.py:
  21  
  22: from transformers import VJEPA2Config
  23: from transformers.testing_utils import (
  24      require_torch,

  28  )
  29: from transformers.utils import is_torch_available, is_vision_available
  30  

  42  
  43:     from transformers import VJEPA2ForVideoClassification, VJEPA2Model
  44  

  48  
  49:     from transformers import AutoVideoProcessor
  50  

train_real_world/transformers_4573/tests/models/voxtral/test_modeling_voxtral.py:
  18  
  19: from transformers import (
  20      AutoProcessor,

  24  )
  25: from transformers.testing_utils import (
  26      Expectations,

train_real_world/transformers_4573/tests/models/wav2vec2/test_feature_extraction_wav2vec2.py:
  21  
  22: from transformers import Wav2Vec2Config, Wav2Vec2FeatureExtractor
  23: from transformers.testing_utils import require_torch, slow
  24  

train_real_world/transformers_4573/tests/models/wav2vec2/test_modeling_wav2vec2.py:
  26  
  27: from transformers import Wav2Vec2Config, is_torch_available
  28: from transformers.testing_utils import (
  29      CaptureLogger,

  58  
  59:     from transformers import (
  60          Wav2Vec2FeatureExtractor,

  69      )
  70:     from transformers.models.wav2vec2.modeling_wav2vec2 import (
  71          WAV2VEC2_ADAPTER_PT_FILE,

  85  
  86:     from transformers import Wav2Vec2ProcessorWithLM
  87:     from transformers.models.wav2vec2_with_lm import processing_wav2vec2_with_lm
  88  

train_real_world/transformers_4573/tests/models/wav2vec2/test_processing_wav2vec2.py:
  18  
  19: from transformers.models.wav2vec2 import Wav2Vec2Processor
  20: from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES
  21  

train_real_world/transformers_4573/tests/models/wav2vec2/test_tokenization_wav2vec2.py:
   25  
   26: from transformers import (
   27      AddedToken,

   31  )
   32: from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES, Wav2Vec2CTCTokenizerOutput
   33: from transformers.testing_utils import get_tests_dir, require_torch, slow
   34  

  658          # ```
  659:         #        from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
  660          #        from datasets import load_dataset

train_real_world/transformers_4573/tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py:
  20  
  21: from transformers import Wav2Vec2BertConfig, is_torch_available
  22: from transformers.testing_utils import (
  23      is_flaky,

  43  
  44:     from transformers import (
  45          AutoFeatureExtractor,

  51      )
  52:     from transformers.models.wav2vec2.modeling_wav2vec2 import _sample_negative_indices
  53:     from transformers.models.wav2vec2_bert.modeling_wav2vec2_bert import (
  54          _compute_mask_indices,

train_real_world/transformers_4573/tests/models/wav2vec2_bert/test_processing_wav2vec2_bert.py:
  18  
  19: from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES
  20: from transformers.models.wav2vec2_bert import Wav2Vec2BertProcessor
  21  

train_real_world/transformers_4573/tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py:
  22  
  23: from transformers import Wav2Vec2ConformerConfig, is_torch_available
  24: from transformers.testing_utils import (
  25      is_flaky,

  45  
  46:     from transformers import (
  47          Wav2Vec2ConformerForAudioFrameClassification,

  55      )
  56:     from transformers.models.wav2vec2.modeling_wav2vec2 import _sample_negative_indices
  57:     from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import (
  58          Wav2Vec2ConformerGumbelVectorQuantizer,

train_real_world/transformers_4573/tests/models/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py:
  19  
  20: from transformers import Wav2Vec2PhonemeCTCTokenizer
  21: from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES
  22: from transformers.models.wav2vec2_phoneme.tokenization_wav2vec2_phoneme import Wav2Vec2PhonemeCTCTokenizerOutput
  23: from transformers.testing_utils import require_phonemizer
  24  

train_real_world/transformers_4573/tests/models/wav2vec2_with_lm/test_processing_wav2vec2_with_lm.py:
  27  
  28: from transformers import AutoFeatureExtractor, AutoProcessor
  29: from transformers.models.wav2vec2 import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor
  30: from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES
  31: from transformers.testing_utils import require_pyctcdecode, require_torch, require_torchaudio, slow
  32: from transformers.utils import is_pyctcdecode_available, is_torch_available
  33  

  40  
  41:     from transformers.models.wav2vec2_with_lm import Wav2Vec2ProcessorWithLM
  42:     from transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm import Wav2Vec2DecoderWithLMOutput
  43  
  44  if is_torch_available():
  45:     from transformers import Wav2Vec2ForCTC
  46  

train_real_world/transformers_4573/tests/models/wavlm/test_modeling_wavlm.py:
  21  
  22: from transformers import WavLMConfig, is_torch_available
  23: from transformers.testing_utils import require_torch, require_torchaudio, slow, torch_device
  24  

  37  
  38:     from transformers import (
  39          Wav2Vec2FeatureExtractor,

train_real_world/transformers_4573/tests/models/whisper/test_feature_extraction_whisper.py:
  24  
  25: from transformers import WhisperFeatureExtractor
  26: from transformers.testing_utils import (
  27      check_json_file_has_correct_format,

  30  )
  31: from transformers.utils.import_utils import is_torch_available
  32  

train_real_world/transformers_4573/tests/models/whisper/test_modeling_whisper.py:
  29  
  30: from transformers import WhisperConfig
  31: from transformers.testing_utils import (
  32      Expectations,

  42  )
  43: from transformers.utils import is_torch_available, is_torch_xpu_available, is_torchaudio_available
  44: from transformers.utils.import_utils import is_datasets_available
  45  

  58  
  59:     from transformers import (
  60          WhisperFeatureExtractor,

  67      )
  68:     from transformers.generation import (
  69          GenerateEncoderDecoderOutput,
  70      )
  71:     from transformers.generation.logits_process import LogitsProcessor
  72:     from transformers.models.whisper.modeling_whisper import WhisperDecoder, WhisperEncoder, sinusoids
  73  

train_real_world/transformers_4573/tests/models/whisper/test_processing_whisper.py:
  21  
  22: from transformers import WhisperTokenizer, WhisperTokenizerFast, is_speech_available
  23: from transformers.testing_utils import require_sentencepiece, require_torch, require_torchaudio
  24  

  28  if is_speech_available():
  29:     from transformers import WhisperFeatureExtractor, WhisperProcessor
  30  

train_real_world/transformers_4573/tests/models/whisper/test_tokenization_whisper.py:
  18  
  19: from transformers.models.whisper import WhisperTokenizer
  20: from transformers.models.whisper.tokenization_whisper import _combine_tokens_into_words, _find_longest_common_sequence
  21: from transformers.testing_utils import require_torch, slow
  22  

train_real_world/transformers_4573/tests/models/x_clip/test_modeling_x_clip.py:
  22  
  23: from transformers import XCLIPConfig, XCLIPTextConfig, XCLIPVisionConfig
  24: from transformers.testing_utils import (
  25      Expectations,

  31  )
  32: from transformers.utils import is_torch_available, is_vision_available
  33  

  47  
  48:     from transformers import XCLIPModel, XCLIPTextModel, XCLIPVisionModel
  49  

  51  if is_vision_available():
  52:     from transformers import XCLIPProcessor
  53  

train_real_world/transformers_4573/tests/models/xcodec/test_modeling_xcodec.py:
   27  from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor
   28: from transformers import AutoFeatureExtractor, XcodecConfig
   29: from transformers.testing_utils import (
   30      is_torch_available,

   40  
   41:     from transformers import DacConfig, HubertConfig, XcodecModel
   42  

  181  
  182:     # Copied from transformers.tests.encodec.test_modeling_encodecEncodecModelTest.test_determinism
  183      def test_determinism(self):

  208  
  209:     # Copied from transformers.tests.encodec.test_modeling_encodecEncodecModelTest.test_model_outputs_equivalence
  210      def test_model_outputs_equivalence(self):

  251  
  252: # Copied from transformers.tests.encodec.test_modeling_encodec.normalize
  253  def normalize(arr):

  258  
  259: # Copied from transformers.tests.encodec.test_modeling_encodec.compute_rmse
  260  def compute_rmse(arr1, arr2):

train_real_world/transformers_4573/tests/models/xglm/test_modeling_xglm.py:
  17  
  18: from transformers import XGLMConfig, is_torch_available
  19: from transformers.testing_utils import (
  20      Expectations,

  38  
  39:     from transformers import XGLMForCausalLM, XGLMModel, XGLMTokenizer
  40  

train_real_world/transformers_4573/tests/models/xglm/test_tokenization_xglm.py:
  17  from tests.test_tokenization_common import TokenizerTesterMixin
  18: from transformers.models.xglm.tokenization_xglm import XGLMTokenizer
  19: from transformers.testing_utils import require_tokenizers
  20  

train_real_world/transformers_4573/tests/models/xlm/test_modeling_xlm.py:
  16  
  17: from transformers import XLMConfig, is_torch_available
  18: from transformers.testing_utils import require_torch, slow, torch_device
  19  

  28  
  29:     from transformers import (
  30          XLMForMultipleChoice,

  37      )
  38:     from transformers.models.xlm.modeling_xlm import create_sinusoidal_embeddings
  39  

train_real_world/transformers_4573/tests/models/xlm/test_tokenization_xlm.py:
  20  
  21: from transformers.models.xlm.tokenization_xlm import VOCAB_FILES_NAMES, XLMTokenizer
  22: from transformers.testing_utils import slow
  23  

train_real_world/transformers_4573/tests/models/xlm_roberta/test_modeling_xlm_roberta.py:
  17  
  18: from transformers import is_torch_available
  19: from transformers.testing_utils import (
  20      require_sentencepiece,

  29  
  30:     from transformers import XLMRobertaModel
  31  

train_real_world/transformers_4573/tests/models/xlm_roberta/test_tokenization_xlm_roberta.py:
  16  
  17: from transformers import XLMRobertaTokenizer
  18: from transformers.testing_utils import require_sentencepiece, require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py:
  19  
  20: from transformers import XLMRobertaXLConfig, is_torch_available
  21: from transformers.testing_utils import require_torch, slow, torch_device
  22  

  31  
  32:     from transformers import (
  33          DataCollatorWithFlattening,

  41      )
  42:     from transformers.models.xlm_roberta_xl.modeling_xlm_roberta_xl import XLMRobertaXLEmbeddings
  43  

train_real_world/transformers_4573/tests/models/xlnet/test_modeling_xlnet.py:
  17  
  18: from transformers import XLNetConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  29  
  30:     from transformers import (
  31          XLNetForMultipleChoice,

train_real_world/transformers_4573/tests/models/xlnet/test_tokenization_xlnet.py:
  16  
  17: from transformers.models.xlnet.tokenization_xlnet import XLNetTokenizer
  18: from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers
  19  

train_real_world/transformers_4573/tests/models/xlstm/test_modeling_xlstm.py:
  19  
  20: from transformers import AutoTokenizer, is_torch_available, xLSTMConfig
  21: from transformers.testing_utils import (
  22      require_read_token,

  37  
  38:     from transformers import (
  39          xLSTMForCausalLM,

  41      )
  42:     from transformers.models.xlstm.modeling_xlstm import xLSTMBlock, xLSTMCache
  43  

train_real_world/transformers_4573/tests/models/xmod/test_modeling_xmod.py:
  17  
  18: from transformers import XLMRobertaTokenizer, is_torch_available
  19: from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device
  20  

  29  
  30:     from transformers import (
  31          DataCollatorWithFlattening,

  40      )
  41:     from transformers.models.xmod.modeling_xmod import XmodEmbeddings
  42  

train_real_world/transformers_4573/tests/models/yolos/test_image_processing_yolos.py:
  22  
  23: from transformers.testing_utils import require_torch, require_vision, slow
  24: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  25  

  34  
  35:     from transformers import YolosImageProcessor
  36  
  37      if is_torchvision_available():
  38:         from transformers import YolosImageProcessorFast
  39  

train_real_world/transformers_4573/tests/models/yolos/test_modeling_yolos.py:
  18  
  19: from transformers import YolosConfig
  20: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  21: from transformers.utils import is_torch_available, is_vision_available
  22  

  31  
  32:     from transformers import YolosForObjectDetection, YolosModel
  33  

  37  
  38:     from transformers import AutoImageProcessor
  39  

train_real_world/transformers_4573/tests/models/yoso/test_modeling_yoso.py:
  17  
  18: from transformers import YosoConfig, is_torch_available
  19: from transformers.testing_utils import require_torch, slow, torch_device
  20  

  28  
  29:     from transformers import (
  30          YosoForMaskedLM,

train_real_world/transformers_4573/tests/models/zamba/test_modeling_zamba.py:
  21  
  22: from transformers import AutoTokenizer, BitsAndBytesConfig, ZambaConfig, is_torch_available
  23: from transformers.testing_utils import (
  24      require_bitsandbytes,

  40  
  41:     from transformers import (
  42          ZambaForCausalLM,

  45      )
  46:     from transformers.models.zamba.modeling_zamba import (
  47          ZambaHybridDynamicCache,

train_real_world/transformers_4573/tests/models/zamba2/test_modeling_zamba2.py:
  21  
  22: from transformers import AutoTokenizer, BitsAndBytesConfig, Zamba2Config, is_torch_available
  23: from transformers.testing_utils import (
  24      Expectations,

  41  
  42:     from transformers import (
  43          Zamba2ForCausalLM,

  46      )
  47:     from transformers.models.zamba2.modeling_zamba2 import (
  48          Zamba2HybridDynamicCache,

train_real_world/transformers_4573/tests/models/zoedepth/test_image_processing_zoedepth.py:
  20  
  21: from transformers.testing_utils import require_torch, require_vision
  22: from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available
  23  

  30  if is_vision_available():
  31:     from transformers import ZoeDepthImageProcessor
  32  
  33      if is_torchvision_available():
  34:         from transformers import ZoeDepthImageProcessorFast
  35  

train_real_world/transformers_4573/tests/models/zoedepth/test_modeling_zoedepth.py:
  19  
  20: from transformers import Dinov2Config, ZoeDepthConfig
  21: from transformers.file_utils import is_torch_available, is_vision_available
  22: from transformers.testing_utils import require_torch, require_vision, slow, torch_device
  23: from transformers.utils.import_utils import get_torch_major_and_minor_version
  24  

  32  
  33:     from transformers import ZoeDepthForDepthEstimation
  34  

  38  
  39:     from transformers import ZoeDepthImageProcessor
  40  

train_real_world/transformers_4573/tests/optimization/test_optimization.py:
  19  
  20: from transformers import is_torch_available
  21: from transformers.testing_utils import require_torch
  22  

  27  
  28:     from transformers import (
  29          Adafactor,

train_real_world/transformers_4573/tests/peft_integration/test_peft_integration.py:
   23  
   24: from transformers import (
   25      AutoModelForCausalLM,

   33  )
   34: from transformers.testing_utils import (
   35      CaptureLogger,

   42  )
   43: from transformers.utils import check_torch_load_is_safe, is_torch_available
   44  

  558          """
  559:         from transformers import pipeline
  560  

  847  
  848:         from transformers import pipeline
  849  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_any_to_any.py:
   19  
   20: from transformers import MODEL_FOR_MULTIMODAL_LM_MAPPING, is_vision_available
   21: from transformers.pipelines import AnyToAnyPipeline, pipeline
   22: from transformers.testing_utils import (
   23      is_pipeline_test,

  228                                  "role": "assistant",
  229:                                 "content": "A digital embrace, a friendly face,\nHugging Face rises, setting the pace.\nFor AI's heart, a vibrant core,\nOpen source models, and so much more.\n\nFrom transformers deep, a powerful might,\nNLP's future, shining so bright.\nDatasets curated, a treasure trove found,\nFor researchers and builders, on fertile ground.\n\nA community thriving, a collaborative art,\nSharing knowledge, playing a vital part.\nSpaces to showcase, creations unfold,\nStories in code, bravely told.\n\nWith libraries sleek, and tools so refined,\nDemocratizing AI, for all humankind.\nFrom sentiment analysis to text generation's grace,\nHugging Face empowers, at a rapid pace.\n\nA platform of learning, a place to explore,\nUnlocking potential, and asking for more.\nSo let's give a cheer, for this innovative team,\nHugging Face's vision, a beautiful dream. \n",
  230                              },

train_real_world/transformers_4573/tests/pipelines/test_pipelines_audio_classification.py:
  20  
  21: from transformers import (
  22      MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING,

  24  )
  25: from transformers.pipelines import AudioClassificationPipeline, pipeline
  26: from transformers.testing_utils import (
  27      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_automatic_speech_recognition.py:
    21  
    22: from transformers import (
    23      MODEL_FOR_CTC_MAPPING,

    33  )
    34: from transformers.pipelines import AutomaticSpeechRecognitionPipeline, pipeline
    35: from transformers.pipelines.audio_utils import chunk_bytes_iter, ffmpeg_microphone_live
    36: from transformers.pipelines.automatic_speech_recognition import chunk_iter
    37: from transformers.testing_utils import (
    38      Expectations,

  1741      def test_slow_unfinished_sequence(self):
  1742:         from transformers import GenerationConfig
  1743  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_common.py:
   26  
   27: from transformers import (
   28      AutomaticSpeechRecognitionPipeline,

   37  )
   38: from transformers.pipelines import PIPELINE_REGISTRY, get_task
   39: from transformers.pipelines.base import Pipeline, _pad
   40: from transformers.testing_utils import (
   41      TOKEN,

   54  )
   55: from transformers.utils import direct_transformers_import, is_torch_available
   56: from transformers.utils import logging as transformers_logging
   57  

  350      def test_pipeline_dataset(self):
  351:         from transformers.pipelines.pt_utils import PipelineDataset
  352  

  364      def test_pipeline_iterator(self):
  365:         from transformers.pipelines.pt_utils import PipelineIterator
  366  

  379      def test_pipeline_iterator_no_len(self):
  380:         from transformers.pipelines.pt_utils import PipelineIterator
  381  

  396      def test_pipeline_batch_unbatch_iterator(self):
  397:         from transformers.pipelines.pt_utils import PipelineIterator
  398  

  412  
  413:         from transformers.pipelines.pt_utils import PipelineIterator
  414  

  428      def test_pipeline_chunk_iterator(self):
  429:         from transformers.pipelines.pt_utils import PipelineChunkIterator
  430  

  443      def test_pipeline_pack_iterator(self):
  444:         from transformers.pipelines.pt_utils import PipelinePackIterator
  445  

  476      def test_pipeline_pack_unbatch_iterator(self):
  477:         from transformers.pipelines.pt_utils import PipelinePackIterator
  478  

  512  
  513:         from transformers import AutoModelForCausalLM
  514  

  541  
  542:         from transformers import AutoModelForCausalLM
  543  

  558  
  559:         from transformers.pipelines import SUPPORTED_TASKS
  560  

  603  
  604:         from transformers.pipelines import get_supported_tasks
  605  

  627      def check_default_pipeline(self, task, set_seed_fn, check_models_equal_fn):
  628:         from transformers.pipelines import SUPPORTED_TASKS, pipeline
  629  

  886      def test_push_to_hub_dynamic_pipeline(self):
  887:         from transformers import BertConfig, BertForSequenceClassification, BertTokenizer
  888  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_depth_estimation.py:
  20  
  21: from transformers import MODEL_FOR_DEPTH_ESTIMATION_MAPPING, is_torch_available, is_vision_available
  22: from transformers.pipelines import DepthEstimationPipeline, pipeline
  23: from transformers.testing_utils import (
  24      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_document_question_answering.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING,

  22  )
  23: from transformers.pipelines import DocumentQuestionAnsweringPipeline, pipeline
  24: from transformers.pipelines.document_question_answering import apply_tesseract
  25: from transformers.testing_utils import (
  26      is_pipeline_test,

  44  
  45:     from transformers.image_utils import load_image
  46  else:

train_real_world/transformers_4573/tests/pipelines/test_pipelines_feature_extraction.py:
  18  
  19: from transformers import (
  20      FEATURE_EXTRACTOR_MAPPING,

  27  )
  28: from transformers.testing_utils import is_pipeline_test, nested_simplify, require_torch
  29  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_fill_mask.py:
  17  
  18: from transformers import MODEL_FOR_MASKED_LM_MAPPING, FillMaskPipeline, pipeline
  19: from transformers.pipelines import PipelineException
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_classification.py:
  19  
  20: from transformers import (
  21      MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,

  25  )
  26: from transformers.pipelines import ImageClassificationPipeline, pipeline
  27: from transformers.testing_utils import (
  28      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_feature_extraction.py:
  19  
  20: from transformers import (
  21      MODEL_MAPPING,

  27  )
  28: from transformers.testing_utils import is_pipeline_test, nested_simplify, require_torch
  29  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_segmentation.py:
  24  
  25: from transformers import (
  26      MODEL_FOR_IMAGE_SEGMENTATION_MAPPING,

  37  )
  38: from transformers.testing_utils import (
  39      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_text_to_text.py:
  17  
  18: from transformers import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING, is_vision_available
  19: from transformers.pipelines import ImageTextToTextPipeline, pipeline
  20: from transformers.testing_utils import (
  21      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_to_image.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_IMAGE_TO_IMAGE_MAPPING,

  24  )
  25: from transformers.testing_utils import (
  26      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_image_to_text.py:
  18  
  19: from transformers import MODEL_FOR_VISION_2_SEQ_MAPPING, is_vision_available
  20: from transformers.pipelines import ImageToTextPipeline, pipeline
  21: from transformers.testing_utils import (
  22      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_keypoint_matching.py:
  17  
  18: from transformers.models.auto.modeling_auto import MODEL_FOR_KEYPOINT_MATCHING_MAPPING
  19: from transformers.pipelines import KeypointMatchingPipeline, pipeline
  20: from transformers.testing_utils import (
  21      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_mask_generation.py:
  19  
  20: from transformers import (
  21      MODEL_FOR_MASK_GENERATION_MAPPING,

  25  )
  26: from transformers.pipelines import MaskGenerationPipeline
  27: from transformers.testing_utils import (
  28      Expectations,

  37  if is_torch_available():
  38:     from transformers import MODEL_FOR_MASK_GENERATION_MAPPING
  39  else:

train_real_world/transformers_4573/tests/pipelines/test_pipelines_object_detection.py:
  19  
  20: from transformers import (
  21      MODEL_FOR_OBJECT_DETECTION_MAPPING,

  27  )
  28: from transformers.testing_utils import (
  29      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_question_answering.py:
  18  
  19: from transformers import (
  20      MODEL_FOR_QUESTION_ANSWERING_MAPPING,

  23  )
  24: from transformers.data.processors.squad import SquadExample
  25: from transformers.pipelines import QuestionAnsweringArgumentHandler, pipeline
  26: from transformers.testing_utils import (
  27      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_summarization.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,

  21  )
  22: from transformers.testing_utils import is_pipeline_test, require_torch, slow, torch_device
  23: from transformers.tokenization_python import TruncationStrategy
  24  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_table_question_answering.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING,

  23  )
  24: from transformers.testing_utils import (
  25      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_text_classification.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,

  21  )
  22: from transformers.testing_utils import (
  23      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_text_generation.py:
   17  
   18: from transformers import (
   19      MODEL_FOR_CAUSAL_LM_MAPPING,

   23  )
   24: from transformers.testing_utils import (
   25      CaptureLogger,

  182  
  183:         from transformers.pipelines.pt_utils import KeyDataset
  184  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_text_to_audio.py:
  19  
  20: from transformers import (
  21      MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING,

  25  )
  26: from transformers.testing_utils import (
  27      is_pipeline_test,

  32  )
  33: from transformers.trainer_utils import set_seed
  34  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_text2text_generation.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,

  21  )
  22: from transformers.testing_utils import is_pipeline_test, require_torch
  23: from transformers.utils import is_torch_available
  24  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_token_classification.py:
  18  
  19: from transformers import (
  20      MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,

  25  )
  26: from transformers.pipelines import AggregationStrategy, TokenClassificationArgumentHandler
  27: from transformers.testing_utils import (
  28      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_translation.py:
  18  
  19: from transformers import (
  20      MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,

  26  )
  27: from transformers.testing_utils import is_pipeline_test, require_torch, slow
  28  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_video_classification.py:
  18  
  19: from transformers import MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING, VideoMAEImageProcessor
  20: from transformers.pipelines import VideoClassificationPipeline, pipeline
  21: from transformers.testing_utils import (
  22      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_visual_question_answering.py:
  18  
  19: from transformers import MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING, is_vision_available
  20: from transformers.pipelines import pipeline
  21: from transformers.testing_utils import (
  22      is_pipeline_test,

  37  
  38:     from transformers.pipelines.pt_utils import KeyDataset
  39  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_zero_shot_audio_classification.py:
  18  
  19: from transformers.pipelines import pipeline
  20: from transformers.testing_utils import is_pipeline_test, nested_simplify, require_torch, slow
  21  

train_real_world/transformers_4573/tests/pipelines/test_pipelines_zero_shot_image_classification.py:
  18  
  19: from transformers import is_vision_available
  20: from transformers.pipelines import pipeline
  21: from transformers.testing_utils import (
  22      compare_pipeline_output_to_hub_spec,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_zero_shot_object_detection.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING,

  22  )
  23: from transformers.testing_utils import (
  24      is_pipeline_test,

train_real_world/transformers_4573/tests/pipelines/test_pipelines_zero_shot.py:
  16  
  17: from transformers import (
  18      MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,

  22  )
  23: from transformers.testing_utils import (
  24      is_pipeline_test,

train_real_world/transformers_4573/tests/quantization/aqlm_integration/test_aqlm.py:
   23  
   24: from transformers import AqlmConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer, OPTForCausalLM, StaticCache
   25: from transformers.testing_utils import (
   26      backend_empty_cache,

   33  )
   34: from transformers.utils import is_aqlm_available, is_torch_available
   35  

  105  
  106:         from transformers.integrations import replace_with_aqlm_linear
  107  

train_real_world/transformers_4573/tests/quantization/autoawq/test_awq.py:
   18  
   19: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM
   20: from transformers.testing_utils import (
   21      backend_empty_cache,

   29  )
   30: from transformers.utils import is_torch_available
   31: from transformers.utils.quantization_config import AwqBackend
   32  

  147  
  148:         from transformers.integrations.awq import replace_with_awq_linear
  149  

train_real_world/transformers_4573/tests/quantization/autoround/test_auto_round.py:
  17  
  18: from transformers import AutoModelForCausalLM, AutoRoundConfig, AutoTokenizer
  19: from transformers.testing_utils import (
  20      backend_empty_cache,

  30  )
  31: from transformers.utils import is_torch_available
  32  

train_real_world/transformers_4573/tests/quantization/bitnet_integration/test_bitnet.py:
   17  
   18: from transformers import (
   19      AutoConfig,

   24  )
   25: from transformers.testing_utils import (
   26      backend_empty_cache,

   31  )
   32: from transformers.utils import is_torch_available
   33  

   74      def test_replace_with_bitlinear(self):
   75:         from transformers.integrations import BitLinear, replace_with_bitnet_linear
   76  

  110  
  111:         from transformers.integrations import pack_weights, unpack_weights
  112  

  124  
  125:         from transformers.integrations import BitLinear
  126  

  167  
  168:         from transformers.integrations import replace_with_bitnet_linear
  169  

train_real_world/transformers_4573/tests/quantization/bnb/test_4bit.py:
   19  
   20: from transformers import (
   21      AutoConfig,

   30  )
   31: from transformers.models.opt.modeling_opt import OPTAttention
   32: from transformers.testing_utils import (
   33      apply_skip_if_not_implemented,

  161          """
  162:         from transformers.integrations.accelerate import compute_module_sizes
  163:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  164:         from transformers.quantizers import AutoHfQuantizer
  165  

  234          """
  235:         from transformers import T5PreTrainedModel
  236  

  421          """
  422:         from transformers import T5ForConditionalGeneration
  423  

  447          """
  448:         from transformers import T5ForConditionalGeneration
  449  

  540          r"""
  541:         The aim of this test is to verify that the mixed 4bit is compatible with `pipeline` from transformers. Since
  542          we used pipeline for inference speed benchmarking we want to make sure that this feature does not break anything

train_real_world/transformers_4573/tests/quantization/bnb/test_mixed_int8.py:
   19  
   20: from transformers import (
   21      AutoConfig,

   30  )
   31: from transformers.models.opt.modeling_opt import OPTAttention
   32: from transformers.testing_utils import (
   33      apply_skip_if_not_implemented,

  151  
  152:         from transformers import AutoModelForMaskedLM, Blip2ForConditionalGeneration, MptForCausalLM, OPTForCausalLM
  153:         from transformers.quantizers.base import get_keys_to_not_convert
  154  

  216          """
  217:         from transformers import T5PreTrainedModel
  218  

  429          """
  430:         from transformers.integrations.accelerate import compute_module_sizes
  431:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  432:         from transformers.quantizers import AutoHfQuantizer
  433  

  497          """
  498:         from transformers import T5ForConditionalGeneration
  499  

  524  
  525:         from transformers import T5ForConditionalGeneration
  526  

  552  
  553:         from transformers import T5ForConditionalGeneration
  554  

  650          r"""
  651:         The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since
  652          we used pipeline for inference speed benchmarking we want to make sure that this feature does not break anything

train_real_world/transformers_4573/tests/quantization/compressed_tensors_integration/test_compressed_models.py:
    4  
    5: from transformers import AutoModelForCausalLM, AutoTokenizer
    6: from transformers.testing_utils import backend_empty_cache, require_compressed_tensors, require_torch, torch_device
    7: from transformers.utils import is_torch_available
    8: from transformers.utils.quantization_config import CompressedTensorsConfig
    9  

  194  
  195:         from transformers.utils.quantization_config import CompressedTensorsConfig
  196  

  217  
  218:         from transformers import AutoTokenizer
  219:         from transformers.utils.quantization_config import CompressedTensorsConfig
  220  

train_real_world/transformers_4573/tests/quantization/compressed_tensors_integration/test_compressed_tensors.py:
  3  
  4: from transformers import AutoModelForCausalLM, AutoTokenizer, CompressedTensorsConfig
  5: from transformers.testing_utils import backend_empty_cache, require_compressed_tensors, require_torch, torch_device
  6: from transformers.utils import is_torch_available
  7  

train_real_world/transformers_4573/tests/quantization/eetq_integration/test_eetq.py:
  18  
  19: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, EetqConfig, OPTForCausalLM
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

  28  )
  29: from transformers.utils import is_torch_available
  30  

  94          """
  95:         from transformers.integrations import replace_with_eetq_linear
  96:         from transformers.integrations.eetq import EetqLinear
  97  

train_real_world/transformers_4573/tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py:
   19  
   20: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FbgemmFp8Config, OPTForCausalLM
   21: from transformers.testing_utils import (
   22      backend_empty_cache,

   30  )
   31: from transformers.utils import (
   32      is_accelerate_available,

  149  
  150:         from transformers.integrations import FbgemmFp8Linear, replace_with_fbgemm_fp8_linear
  151  

  299          """
  300:         from transformers.integrations import FbgemmFp8Linear
  301  

  312          """
  313:         from transformers.integrations import FbgemmFp8Linear
  314  

train_real_world/transformers_4573/tests/quantization/finegrained_fp8/test_fp8.py:
   20  
   21: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FineGrainedFP8Config, OPTForCausalLM
   22: from transformers.quantizers.quantizer_finegrained_fp8 import FineGrainedFP8HfQuantizer
   23: from transformers.testing_utils import (
   24      backend_empty_cache,

   32  )
   33: from transformers.utils import is_torch_available
   34  

  138  
  139:         from transformers.integrations import FP8Linear, replace_with_fp8_linear
  140  

  327          """
  328:         from transformers.integrations import FP8Linear
  329:         from transformers.integrations.accelerate import compute_module_sizes
  330:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  331:         from transformers.quantizers import AutoHfQuantizer
  332  

  384          """
  385:         from transformers.integrations import FP8Linear
  386  

  396          """
  397:         from transformers.integrations import FP8Linear
  398  

train_real_world/transformers_4573/tests/quantization/fp_quant_integration/test_fp_quant.py:
  18  
  19: from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

train_real_world/transformers_4573/tests/quantization/ggml/test_ggml.py:
    18  
    19: from transformers import (
    20      AddedToken,

    26  )
    27: from transformers.testing_utils import (
    28      require_gguf,

    33  )
    34: from transformers.utils import is_gguf_available, is_torch_available
    35  

  1023          """Test that Deci GGUF config mapping is correctly applied."""
  1024:         from transformers.integrations.ggml import GGUF_CONFIG_MAPPING
  1025  

  1048          """Test that Deci architectures are mapped to GGUFLlamaConverter."""
  1049:         from transformers.integrations.ggml import GGUF_TO_FAST_CONVERTERS, GGUFLlamaConverter
  1050  

train_real_world/transformers_4573/tests/quantization/gptq/test_gptq.py:
  19  
  20: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPTQConfig
  21: from transformers.testing_utils import (
  22      is_torch_available,

  29  )
  30: from transformers.utils import is_gptqmodel_available, is_ipex_available
  31  

train_real_world/transformers_4573/tests/quantization/higgs/test_higgs.py:
  18  
  19: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, HiggsConfig, OPTForCausalLM
  20: from transformers.testing_utils import (
  21      backend_empty_cache,

  28  )
  29: from transformers.utils import is_torch_available
  30  

  95  
  96:         from transformers.integrations import HiggsLinear, replace_with_higgs_linear
  97  

train_real_world/transformers_4573/tests/quantization/hqq/test_hqq.py:
  20  
  21: from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig
  22: from transformers.testing_utils import (
  23      backend_empty_cache,

  31  )
  32: from transformers.utils import is_hqq_available, is_torch_available
  33  

train_real_world/transformers_4573/tests/quantization/mxfp4/test_mxfp4.py:
   20  
   21: from transformers import AutoTokenizer, GptOssForCausalLM, Mxfp4Config
   22: from transformers.testing_utils import (
   23      require_kernels,

   30  )
   31: from transformers.utils import (
   32      is_torch_available,

  115          with patch("transformers.quantizers.quantizer_mxfp4.is_torch_available", return_value=False):
  116:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  117  

  126          with _patch_no_accelerator():
  127:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  128  

  139          with patch("torch.cuda.get_device_capability", return_value=(7, 0)):
  140:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  141  

  152          with patch("torch.cuda.get_device_capability", return_value=(7, 0)):
  153:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  154  

  165          with patch("torch.cuda.get_device_capability", return_value=(7, 0)):
  166:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  167  

  181          with _patch_no_accelerator():
  182:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  183  

  204          ):
  205:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  206  

  218          ):
  219:             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  220  

  230          """Test trainability"""
  231:         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  232  

  244          """Test module conversion decision logic"""
  245:         from transformers.quantizers.quantizers_utils import should_convert_module
  246  

  258          """Test unpacking of quantized tensors"""
  259:         from transformers.integrations.mxfp4 import convert_moe_packed_tensors
  260  

  273          """Test quantization function"""
  274:         from transformers.integrations.mxfp4 import quantize_to_mxfp4
  275:         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  276  

  365          """Test device map validation"""
  366:         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer
  367  

  462          """
  463:         from transformers import AutoConfig, AutoModelForCausalLM
  464:         from transformers.integrations import Mxfp4GptOssExperts
  465:         from transformers.integrations.accelerate import compute_module_sizes
  466:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  467:         from transformers.quantizers import AutoHfQuantizer
  468  

train_real_world/transformers_4573/tests/quantization/quanto_integration/test_quanto.py:
   17  
   18: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, QuantoConfig
   19: from transformers.testing_utils import (
   20      require_accelerate,

   26  )
   27: from transformers.utils import is_optimum_quanto_available, is_torch_available
   28  

   36  
   37:     from transformers.integrations.quanto import replace_with_quanto_layers
   38  

  244          """
  245:         from transformers.integrations.accelerate import compute_module_sizes
  246:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  247:         from transformers.quantizers import AutoHfQuantizer
  248  

train_real_world/transformers_4573/tests/quantization/quark_integration/test_quark.py:
  15  
  16: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, QuarkConfig
  17: from transformers.testing_utils import (
  18      cleanup,

  26  )
  27: from transformers.utils.import_utils import is_quark_available
  28  

train_real_world/transformers_4573/tests/quantization/spqr_integration/test_spqr.py:
   20  
   21: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, SpQRConfig, StaticCache
   22: from transformers.testing_utils import (
   23      backend_empty_cache,

   30  )
   31: from transformers.utils import is_torch_available
   32  

  107  
  108:         from transformers.integrations import replace_with_spqr_linear
  109  

train_real_world/transformers_4573/tests/quantization/torchao_integration/test_torchao.py:
   22  
   23: from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
   24: from transformers.testing_utils import (
   25      Expectations,

   34  )
   35: from transformers.utils import is_torch_available, is_torchao_available
   36  

  547          """
  548:         from transformers import AutoConfig
  549:         from transformers.integrations.accelerate import compute_module_sizes
  550:         from transformers.modeling_utils import expand_device_map, get_total_byte_count
  551:         from transformers.quantizers import AutoHfQuantizer
  552  

train_real_world/transformers_4573/tests/quantization/vptq_integration/test_vptq.py:
   18  
   19: from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, VptqConfig
   20: from transformers.testing_utils import (
   21      backend_empty_cache,

   28  )
   29: from transformers.utils import is_torch_available
   30  

  127  
  128:         from transformers.integrations import replace_with_vptq_linear
  129  

train_real_world/transformers_4573/tests/repo_utils/test_check_copies.py:
   70  
   71: # Copied from transformers.models.bert.modeling_bert.bert_function
   72  def bert_copy_function(x):

   75  
   76: # Copied from transformers.models.bert.modeling_bert.BertAttention
   77  class BertCopyAttention(nn.Module):

   81  
   82: # Copied from transformers.models.bert.modeling_bert.BertModel with Bert->BertCopy all-casing
   83  class BertCopyModel(BertCopyPreTrainedModel):

  102  
  103:     # Copied from transformers.models.dummy_gpt2.modeling_dummy_gpt2.GPT2DummyModel.forward
  104      def forward(self, c):

  115  MOCK_DUMMY_ROBERTA_CODE_MATCH = """
  116: # Copied from transformers.models.dummy_bert_match.modeling_dummy_bert_match.BertDummyModel with BertDummy->RobertaBertDummy
  117  class RobertaBertDummyModel:

  129  
  130:     # Copied from transformers.models.dummy_gpt2.modeling_dummy_gpt2.GPT2DummyModel.forward
  131      def forward(self, c):

  151  
  152:     # Copied from transformers.models.dummy_gpt2.modeling_dummy_gpt2.GPT2DummyModel.forward
  153      def forward(self, c):

  170  MOCK_DUMMY_ROBERTA_CODE_NO_MATCH = """
  171: # Copied from transformers.models.dummy_bert_no_match.modeling_dummy_bert_no_match.BertDummyModel with BertDummy->RobertaBertDummy
  172  class RobertaBertDummyModel:

  184  
  185:     # Copied from transformers.models.dummy_gpt2.modeling_dummy_gpt2.GPT2DummyModel.forward
  186      def forward(self, c):

  204  EXPECTED_REPLACED_CODE = """
  205: # Copied from transformers.models.dummy_bert_no_match.modeling_dummy_bert_no_match.BertDummyModel with BertDummy->RobertaBertDummy
  206  class RobertaBertDummyModel:

  213  
  214:     # Copied from transformers.models.dummy_gpt2.modeling_dummy_gpt2.GPT2DummyModel.forward
  215      def forward(self, c):

train_real_world/transformers_4573/tests/repo_utils/test_tests_fetcher.py:
   24  
   25: from transformers.testing_utils import CaptureStdout
   26  

  139      with open(test_dir / "test_modeling_common.py", "w") as f:
  140:         f.write("from transformers.modeling_utils import PreTrainedModel\ncode")
  141  

  147              f.write(
  148:                 f"from transformers import {cls}Config, {cls}Model\nfrom ...test_modeling_common import ModelTesterMixin\n\ncode"
  149              )

  159      with open(glue_dir / "run_glue.py", "w") as f:
  160:         f.write("from transformers import BertModel\n\ncode")
  161  

  312                  f.write(
  313:                     "from transformers.utils import cached_file, is_torch_available\nfrom transformers.models.bert.configuration_bert import BertConfig\n"
  314                  )

  324                  f.write(
  325:                     "from transformers.utils import (\n    cached_file,\n    is_torch_available\n)\nfrom transformers.models.bert.configuration_bert import BertConfig\n"
  326                  )

  336                  f.write(
  337:                     "from transformers.utils import (\n    cached_file,\n    is_torch_available\n)\nfrom transformers import BertConfig\n"
  338                  )

  624                  f.write(
  625:                     "from transformers import T5Config, T5Model\nfrom ...test_modeling_common import ModelTesterMixin\n\ncode"
  626                  )

  667                  "tests/models/bert/test_modeling_bert.py",
  668:                 "from transformers import BertConfig, BertModel\nfrom ...test_modeling_common import ModelTesterMixin\n\ncode1",
  669                  repo,

  688                  "examples/pytorch/text-classification/run_glue.py",
  689:                 "from transformers import BertModeln\n\ncode1",
  690                  repo,

train_real_world/transformers_4573/tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py:
  27  import transformers
  28: from transformers import (
  29      AutoConfig,

  38  )
  39: from transformers.trainer import Trainer
  40: from transformers.training_args import TrainingArguments
  41: from transformers.utils import check_min_version
  42  

train_real_world/transformers_4573/tests/tensor_parallel/test_tensor_parallel.py:
  25  
  26: from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available
  27: from transformers.integrations.tensor_parallel import get_packed_weights, get_tensor_shard, repack_weights
  28: from transformers.testing_utils import (
  29      TestCasePlus,

train_real_world/transformers_4573/tests/tokenization/test_tokenization_fast.py:
  24  
  25: from transformers import AutoTokenizer, PreTrainedTokenizerFast
  26: from transformers.testing_utils import require_tokenizers
  27  

train_real_world/transformers_4573/tests/tokenization/test_tokenization_utils.py:
   23  
   24: from transformers import (
   25      AutoTokenizer,

   34  )
   35: from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer
   36: from transformers.testing_utils import (
   37      CaptureStderr,

  280  
  281:         from transformers.convert_slow_tokenizer import import_protobuf  # noqa: F401
  282  

train_real_world/transformers_4573/tests/trainer/test_data_collator.py:
  21  
  22: from transformers import (
  23      BertTokenizer,

  35  )
  36: from transformers.testing_utils import require_torch
  37: from transformers.utils import PaddingStrategy
  38  

train_real_world/transformers_4573/tests/trainer/test_trainer_callback.py:
  21  
  22: from transformers import (
  23      DefaultFlowCallback,

  33  )
  34: from transformers.testing_utils import require_torch
  35: from transformers.trainer_callback import ExportableState
  36  

  38  if is_torch_available():
  39:     from transformers.trainer import DEFAULT_CALLBACKS, TRAINER_STATE_NAME
  40  

train_real_world/transformers_4573/tests/trainer/test_trainer_distributed_loss.py:
   5  from tests.trainer.test_trainer import StoreLossCallback
   6: from transformers import (
   7      AutoModelForCausalLM,

  15  )
  16: from transformers.testing_utils import (
  17      TestCasePlus,

train_real_world/transformers_4573/tests/trainer/test_trainer_distributed_worker_seed.py:
   8  
   9: from transformers import (
  10      HfArgumentParser,

  14  )
  15: from transformers.testing_utils import (
  16      TestCasePlus,

train_real_world/transformers_4573/tests/trainer/test_trainer_distributed.py:
  17  
  18: from transformers import EvalPrediction, HfArgumentParser, TrainingArguments, is_torch_available
  19: from transformers.testing_utils import (
  20      TestCasePlus,

  27  )
  28: from transformers.training_args import ParallelMode
  29: from transformers.utils import logging
  30  

  39  
  40:     from transformers import Trainer
  41  

train_real_world/transformers_4573/tests/trainer/test_trainer_fsdp.py:
  15  
  16: from transformers import is_torch_available
  17: from transformers.testing_utils import (
  18      TestCasePlus,

  34  
  35:     from transformers import (
  36          AutoModelForCausalLM,

train_real_world/transformers_4573/tests/trainer/test_trainer_jit_checkpoint.py:
   20  
   21: from transformers import TrainingArguments, is_torch_available
   22: from transformers.testing_utils import require_torch
   23  

   25  if is_torch_available():
   26:     from transformers.trainer_jit_checkpoint import CheckpointManager, JITCheckpointCallback
   27  

   42          """Helper method to create a trainer with JIT checkpointing enabled."""
   43:         from transformers import Trainer
   44  

  149          """Test the checkpoint execution logic with sentinel file."""
  150:         from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
  151  

  177          """Test that sentinel file is cleaned up after successful checkpoint."""
  178:         from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
  179  

train_real_world/transformers_4573/tests/trainer/test_trainer_seq2seq.py:
  13  # limitations under the License.
  14: from transformers import (
  15      AutoModelForSeq2SeqLM,

  23  )
  24: from transformers.testing_utils import TestCasePlus, require_sentencepiece, require_torch, slow
  25: from transformers.utils import is_datasets_available, is_torch_available
  26  

train_real_world/transformers_4573/tests/trainer/test_trainer_tpu.py:
  23  
  24: from transformers import EvalPrediction, HfArgumentParser, TrainingArguments, is_torch_available
  25: from transformers.utils import logging
  26  

  35  
  36:     from transformers import Trainer
  37  

train_real_world/transformers_4573/tests/trainer/test_trainer_utils.py:
  20  
  21: from transformers import Trainer, TrainingArguments
  22: from transformers.data.data_collator import default_data_collator
  23: from transformers.testing_utils import require_accelerate, require_torch
  24: from transformers.trainer_utils import RemoveColumnsCollator, find_executable_batch_size
  25: from transformers.utils import is_torch_available
  26  

  32  
  33:     from transformers.modeling_outputs import SequenceClassifierOutput
  34:     from transformers.tokenization_utils_base import BatchEncoding
  35:     from transformers.trainer_pt_utils import (
  36          DistributedLengthGroupedSampler,

train_real_world/transformers_4573/tests/trainer/test_trainer.py:
    35  
    36: from transformers import (
    37      AutoFeatureExtractor,

    53  )
    54: from transformers.hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS
    55: from transformers.testing_utils import (
    56      ENDPOINT_STAGING,

   105  )
   106: from transformers.trainer_utils import (
   107      PREFIX_CHECKPOINT_DIR,

   111  )
   112: from transformers.training_args import OptimizerNames
   113: from transformers.utils import (
   114      SAFE_WEIGHTS_INDEX_NAME,

   121  )
   122: from transformers.utils.hp_naming import TrialShortNamer
   123  

   138      import transformers.optimization
   139:     from transformers import (
   140          AutoModelForCausalLM,

   152      )
   153:     from transformers.trainer_pt_utils import AcceleratorConfig
   154  

  1255  
  1256:         from transformers.optimization import Adafactor, AdafactorSchedule
  1257  

  1914  
  1915:             from transformers.models.llama import modeling_llama
  1916  

train_real_world/transformers_4573/tests/utils/test_activations.py:
  16  
  17: from transformers import is_torch_available
  18: from transformers.testing_utils import require_torch
  19  

  23  
  24:     from transformers.activations import gelu_new, gelu_python, get_activation
  25  

train_real_world/transformers_4573/tests/utils/test_add_new_model_like.py:
  21  
  22: from transformers.cli.add_new_model_like import ModelInfos, _add_new_model_like_internal
  23: from transformers.testing_utils import require_torch
  24  

train_real_world/transformers_4573/tests/utils/test_attention_visualizer.py:
  19  
  20: from transformers.testing_utils import require_read_token, require_torch
  21: from transformers.utils.attention_visualizer import AttentionMaskVisualizer
  22  

train_real_world/transformers_4573/tests/utils/test_audio_utils.py:
  19  
  20: from transformers.audio_utils import (
  21      amplitude_to_db,

  32  )
  33: from transformers.testing_utils import is_librosa_available, require_librosa
  34  

train_real_world/transformers_4573/tests/utils/test_auto_docstring.py:
  18  
  19: âŸª 7520 characters skipped âŸ«n\n              Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n              heads.\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")\n        >>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")\n\n        >>> prompt = "Hey, are you conscious? Can you talk to me?"\n        >>> inputs = tokenizer(prompt, return_tensors="pt")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        "Hey, are you conscious? Can you talk to me?\\nI\'m not conscious, but I can talk to you."\n        ```"""
  20  

  24  
  25: âŸª 6518 characters skipped âŸ«oftmax, used to compute the weighted average in the self-attention\n              heads.\n\n        Example of single-label classification:\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoTokenizer, LlamaForSequenceClassification\n\n        >>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")\n        >>> model = LlamaForSequenceClassification.from_pretrained("meta-llama/Llama-2-7b-hf")\n\n        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")\n\n        >>> with torch.no_grad():\n        ...     logits = model(**inputs).logits\n\n        >>> predicted_class_id = logits.argmax().item()\n        >>> model.config.id2label[predicted_class_id]\n        ...\n\n        >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n        >>> num_labels = len(model.config.id2label)\n        >>> model = LlamaForSequenceClassification.from_pretrained("meta-llama/Llama-2-7b-hf", num_labels=num_labels)\n\n        >>> labels = torch.tensor([1])\n        >>> loss = model(**inputs, labels=labels).loss\n        >>> round(loss.item(), 2)\n        ...\n        ```\n\n        Example of multi-label classification:\n\n        ```python\n        >>> import torch\n        >>> from transformers import AutoTokenizer, LlamaForSequenceClassification\n\n        >>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")\n        >>> model = LlamaForSequenceClassification.from_pretrained("meta-llama/Llama-2-7b-hf", problem_type="multi_label_classification")\n\n        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")\n\n        >>> with torch.no_grad():\n        ...     logits = model(**inputs).logits\n\n        >>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n        >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n        >>> num_labels = len(model.config.id2label)\n        >>> model = LlamaForSequenceClassification.from_pretrained(\n        ...     "meta-llama/Llama-2-7b-hf", num_labels=num_labels, problem_type="multi_label_classification"\n        ... )\n\n        >>> labels = torch.sum(\n        ...     to
  26  

train_real_world/transformers_4573/tests/utils/test_backbone_utils.py:
  18  
  19: from transformers import DetrConfig, MaskFormerConfig, ResNetBackbone, ResNetConfig, TimmBackbone
  20: from transformers.testing_utils import require_torch, slow
  21: from transformers.utils.backbone_utils import (
  22      BackboneMixin,

  26  )
  27: from transformers.utils.import_utils import is_torch_available
  28  

  32  
  33:     from transformers import BertPreTrainedModel
  34  

train_real_world/transformers_4573/tests/utils/test_cache_utils.py:
   21  
   22: from transformers import set_seed
   23: from transformers.generation.configuration_utils import ALL_CACHE_IMPLEMENTATIONS
   24: from transformers.testing_utils import (
   25      CaptureStderr,

   39  )
   40: from transformers.utils import is_hqq_available, is_optimum_quanto_available, is_torch_greater_or_equal
   41  

   45  
   46:     from transformers import (
   47          AutoModelForCausalLM,

   58      )
   59:     from transformers.integrations.executorch import export_with_dynamic_cache
   60  

  754  
  755:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  756  

  772  
  773:         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM
  774  

  785          # Set generation config on the model for the hybrid cache model
  786:         from transformers.generation.configuration_utils import GenerationConfig
  787  

train_real_world/transformers_4573/tests/utils/test_chat_parsing_utils.py:
  17  
  18: from transformers import AutoTokenizer
  19: from transformers.testing_utils import require_jmespath
  20: from transformers.utils.chat_parsing_utils import recursive_parse
  21  

train_real_world/transformers_4573/tests/utils/test_chat_template_utils.py:
  17  
  18: from transformers.utils import DocstringParsingException, TypeHintParsingException, get_json_schema
  19  

train_real_world/transformers_4573/tests/utils/test_configuration_utils.py:
  25  
  26: from transformers import AutoConfig, BertConfig, Florence2Config, GPT2Config
  27: from transformers.configuration_utils import PreTrainedConfig
  28: from transformers.testing_utils import TOKEN, TemporaryHubRepo, is_staging_test, require_torch
  29  

train_real_world/transformers_4573/tests/utils/test_convert_slow_tokenizer.py:
  4  
  5: from transformers.convert_slow_tokenizer import SpmConverter
  6: from transformers.testing_utils import get_tests_dir
  7  

train_real_world/transformers_4573/tests/utils/test_core_model_loading.py:
   19  
   20: from transformers import PretrainedConfig
   21: from transformers.conversion_mapping import get_checkpoint_conversion_mapping, register_checkpoint_conversion_mapping
   22: from transformers.core_model_loading import (
   23      Chunk,

   34  )
   35: from transformers.utils.import_utils import is_triton_available
   36  

  388          if is_triton_available():
  389:             from transformers.integrations.finegrained_fp8 import Fp8Dequantize
  390          else:

train_real_world/transformers_4573/tests/utils/test_deprecation.py:
  20  
  21: from transformers import __version__, is_torch_available
  22: from transformers.testing_utils import require_torch_accelerator, torch_device
  23: from transformers.utils.deprecation import deprecate_kwarg
  24  

train_real_world/transformers_4573/tests/utils/test_doc_samples.py:
  20  import transformers
  21: from transformers.testing_utils import require_torch, slow
  22  

train_real_world/transformers_4573/tests/utils/test_dynamic_module_utils.py:
  18  
  19: from transformers.dynamic_module_utils import get_imports
  20  

train_real_world/transformers_4573/tests/utils/test_expectations.py:
  2  
  3: from transformers.testing_utils import Expectations
  4  

train_real_world/transformers_4573/tests/utils/test_feature_extraction_utils.py:
  24  
  25: from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor
  26: from transformers.feature_extraction_utils import BatchFeature
  27: from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test, require_torch
  28: from transformers.utils import is_torch_available
  29  

train_real_world/transformers_4573/tests/utils/test_file_utils.py:
  21  
  22: # Try to import everything from transformers to ensure every object can be loaded.
  23: from transformers import *  # noqa F406
  24: from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_torch
  25: from transformers.utils import ContextManagers, find_labels, is_torch_available
  26  

  28  if is_torch_available():
  29:     from transformers import BertForPreTraining, BertForQuestionAnswering, BertForSequenceClassification
  30  

train_real_world/transformers_4573/tests/utils/test_generic.py:
  20  
  21: from transformers.configuration_utils import PreTrainedConfig
  22: from transformers.modeling_outputs import BaseModelOutput, CausalLMOutputWithPast
  23: from transformers.testing_utils import require_torch
  24: from transformers.utils import (
  25      can_return_tuple,

train_real_world/transformers_4573/tests/utils/test_hf_argparser.py:
  29  
  30: from transformers import HfArgumentParser, TrainingArguments
  31: from transformers.hf_argparser import make_choice_type_function, string_to_bool
  32: from transformers.testing_utils import require_torch
  33  

train_real_world/transformers_4573/tests/utils/test_hub_utils.py:
  23  
  24: from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, cached_file, has_file, list_repo_templates
  25  

train_real_world/transformers_4573/tests/utils/test_image_processing_utils.py:
  22  
  23: from transformers import AutoImageProcessor, ViTImageProcessor, ViTImageProcessorFast
  24: from transformers.image_processing_utils import get_size_dict
  25: from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test
  26  

train_real_world/transformers_4573/tests/utils/test_image_utils.py:
  22  from tests.pipelines.test_pipelines_document_question_answering import INVOICE_URL
  23: from transformers import is_torch_available, is_vision_available
  24: from transformers.image_utils import (
  25      ChannelDimension,

  30  )
  31: from transformers.testing_utils import is_flaky, require_torch, require_vision
  32  

  39  
  40:     from transformers import ImageFeatureExtractionMixin
  41:     from transformers.image_utils import get_image_size, infer_channel_dimension_format, load_image
  42  

train_real_world/transformers_4573/tests/utils/test_import_structure.py:
  7  
  8: from transformers.utils.import_utils import (
  9      Backend,

train_real_world/transformers_4573/tests/utils/test_import_utils.py:
   2  
   3: from transformers.testing_utils import run_test_using_subprocess
   4: from transformers.utils.import_utils import clear_import_cache
   5  

  22      # Import and verify module exists
  23:     from transformers.models.auto import modeling_auto
  24  

train_real_world/transformers_4573/tests/utils/test_logging.py:
  20  import transformers.models.roberta.tokenization_roberta
  21: from transformers import logging
  22: from transformers.testing_utils import CaptureLogger, mockenv, mockenv_context
  23: from transformers.utils.logging import disable_progress_bar, enable_progress_bar
  24  

train_real_world/transformers_4573/tests/utils/test_masking_utils.py:
  16  
  17: from transformers.testing_utils import (
  18      cleanup,

  28  
  29:     from transformers import DynamicCache, LlamaConfig
  30:     from transformers.cache_utils import DynamicSlidingWindowLayer
  31:     from transformers.masking_utils import (
  32          create_bidirectional_mask,

train_real_world/transformers_4573/tests/utils/test_model_card.py:
  20  
  21: from transformers.modelcard import ModelCard, TrainingSummary
  22  

train_real_world/transformers_4573/tests/utils/test_model_debugging_utils.py:
  22  
  23: from transformers import is_torch_available
  24: from transformers.model_debugging_utils import model_addition_debugger_context
  25  

train_real_world/transformers_4573/tests/utils/test_model_output.py:
  20  
  21: from transformers import AlbertForMaskedLM
  22: from transformers.testing_utils import require_torch
  23: from transformers.utils import ModelOutput, is_torch_available
  24  

train_real_world/transformers_4573/tests/utils/test_modeling_rope_utils.py:
  18  
  19: from transformers import LlamaConfig
  20: from transformers.testing_utils import is_torch_available, require_torch, torch_device
  21  

  25  
  26:     from transformers import ROPE_INIT_FUNCTIONS
  27:     from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding
  28  

train_real_world/transformers_4573/tests/utils/test_modeling_utils.py:
    36  
    37: from transformers import (
    38      AutoConfig,

    61  )
    62: from transformers.modeling_flash_attention_utils import is_flash_attn_available
    63: from transformers.models.mistral.modeling_mistral import MistralModel
    64: from transformers.testing_utils import (
    65      TOKEN,

    81  )
    82: from transformers.utils import (
    83      SAFE_WEIGHTS_INDEX_NAME,

    87  )
    88: from transformers.utils.import_utils import (
    89      is_flash_attn_2_available,

   109  
   110:     from transformers import (
   111          AutoModelForCausalLM,

   124      )
   125:     from transformers.conversion_mapping import MergeModulelist, WeightConverter, get_model_conversion_mapping
   126:     from transformers.modeling_attn_mask_utils import (
   127          AttentionMaskConverter,

   131      )
   132:     from transformers.modeling_utils import (
   133          FLASH_ATTN_KERNEL_FALLBACK,

   137      )
   138:     from transformers.pytorch_utils import isin_mps_friendly
   139  

  1968              import argparse
  1969:             from transformers import AutoModelForCausalLM
  1970:             from transformers.utils import is_torch_accelerator_available
  1971  

train_real_world/transformers_4573/tests/utils/test_offline.py:
   18  
   19: from transformers import BertConfig, BertModel, BertTokenizer, pipeline
   20: from transformers.testing_utils import TestCasePlus, require_torch
   21  

   34          load = """
   35: from transformers import BertConfig, BertModel, BertTokenizer, pipeline
   36          """

   69          load = """
   70: from transformers import BertConfig, BertModel, BertTokenizer, pipeline
   71          """

  109          load = """
  110: from transformers import BertConfig, BertModel, BertTokenizer
  111          """

  141          load = """
  142: from transformers import pipeline
  143          """

  163          load = """
  164: from transformers import AutoModel
  165          """

train_real_world/transformers_4573/tests/utils/test_skip_decorators.py:
  35  
  36: from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device
  37  

train_real_world/transformers_4573/tests/utils/test_tokenization_utils.py:
  23  
  24: from transformers import AutoTokenizer, BertTokenizer, BertTokenizerFast, GPT2TokenizerFast, is_tokenizers_available
  25: from transformers.testing_utils import TOKEN, TemporaryHubRepo, is_staging_test, require_tokenizers
  26: from transformers.tokenization_python import ExtensionsTrie, Trie
  27  

train_real_world/transformers_4573/tests/utils/test_versions_utils.py:
  17  
  18: from transformers.testing_utils import TestCasePlus
  19: from transformers.utils.versions import require_version, require_version_core
  20  

train_real_world/transformers_4573/tests/utils/test_video_utils.py:
  20  
  21: from transformers import is_torch_available, is_vision_available
  22: from transformers.image_processing_utils import get_size_dict
  23: from transformers.image_utils import SizeDict
  24: from transformers.processing_utils import VideosKwargs
  25: from transformers.testing_utils import (
  26      require_av,

  33  )
  34: from transformers.video_utils import group_videos_by_shape, make_batched_videos, reorder_videos
  35  

  42  
  43:     from transformers import BaseVideoProcessor
  44:     from transformers.video_utils import VideoMetadata, load_video
  45  

train_real_world/transformers_4573/tests/utils/import_structures/failing_export.py:
  16  
  17: from transformers.utils.import_utils import requires
  18  

train_real_world/transformers_4573/tests/utils/import_structures/import_structure_raw_register_with_versions.py:
  16  
  17: from transformers.utils.import_utils import requires
  18  

train_real_world/transformers_4573/tests/utils/import_structures/import_structure_raw_register.py:
  16  
  17: from transformers.utils.import_utils import requires
  18  

train_real_world/transformers_4573/tests/utils/import_structures/import_structure_register_with_comments.py:
  16  
  17: from transformers.utils.import_utils import requires
  18  

train_real_world/transformers_4573/tests/utils/import_structures/import_structure_register_with_duplicates.py:
  16  
  17: from transformers.utils.import_utils import requires
  18  

train_real_world/transformers_4573/utils/check_config_attributes.py:
  19  
  20: from transformers.configuration_utils import PreTrainedConfig
  21: from transformers.utils import direct_transformers_import
  22  

train_real_world/transformers_4573/utils/check_config_docstrings.py:
  18  
  19: from transformers.utils import direct_transformers_import
  20  

train_real_world/transformers_4573/utils/check_copies.py:
  45  
  46: from transformers.utils import direct_transformers_import, logging
  47  

train_real_world/transformers_4573/utils/check_docstrings.py:
  51  
  52: from transformers.utils import direct_transformers_import
  53: from transformers.utils.auto_docstring import (
  54      ImageProcessorArgs,

train_real_world/transformers_4573/utils/check_inits.py:
  323      # This is to make sure the transformers module imported is the one in the repo.
  324:     from transformers.utils import direct_transformers_import
  325  

train_real_world/transformers_4573/utils/check_pipeline_typing.py:
  2  
  3: from transformers.pipelines import SUPPORTED_TASKS, Pipeline
  4  

train_real_world/transformers_4573/utils/check_repo.py:
  42  
  43: from transformers import is_torch_available
  44: from transformers.models.auto.auto_factory import get_values
  45: from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES
  46: from transformers.models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES
  47: from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES
  48: from transformers.models.auto.processing_auto import PROCESSOR_MAPPING_NAMES
  49: from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES
  50: from transformers.testing_utils import _COMMON_MODEL_NAMES_MAP
  51: from transformers.utils import ENV_VARS_TRUE_VALUES, direct_transformers_import
  52  

train_real_world/transformers_4573/utils/create_dummy_models.py:
   32  
   33: from transformers import (
   34      CONFIG_MAPPING,

   44  )
   45: from transformers.feature_extraction_utils import FeatureExtractionMixin
   46: from transformers.file_utils import is_torch_available
   47: from transformers.image_processing_utils import BaseImageProcessor
   48: from transformers.models.auto.configuration_auto import AutoConfig, model_type_to_module_name
   49: from transformers.models.fsmt import configuration_fsmt
   50: from transformers.processing_utils import ProcessorMixin, transformers_module
   51: from transformers.tokenization_utils_base import PreTrainedTokenizerBase
   52  

  825  
  826:     from transformers import (
  827          BertConfig,

train_real_world/transformers_4573/utils/deprecate_models.py:
  17  
  18: from transformers import CONFIG_MAPPING, logging
  19: from transformers import __version__ as current_version
  20  

train_real_world/transformers_4573/utils/extract_warnings.py:
  8  
  9: from transformers import logging
  10  

train_real_world/transformers_4573/utils/fetch_hub_objects_for_ci.py:
    5  
    6: from transformers.testing_utils import _run_pipeline_tests, _run_staging
    7: from transformers.utils.import_utils import is_mistral_common_available
    8  

  185  
  186:             from transformers import AutoTokenizer
  187:             from transformers.tokenization_mistral_common import MistralCommonBackend
  188  

train_real_world/transformers_4573/utils/models_to_deprecate.py:
  30  
  31: from transformers.models.auto.configuration_auto import DEPRECATED_MODELS, MODEL_NAMES_MAPPING
  32  

train_real_world/transformers_4573/utils/modular_model_converter.py:
  37  
  38: from transformers import logging
  39: from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES
  40  

train_real_world/transformers_4573/utils/modular_model_detector.py:
  118  import transformers
  119: from transformers import AutoModel, AutoTokenizer
  120: from transformers.utils import enable_tf32
  121: from transformers.utils import logging as transformers_logging
  122  

train_real_world/transformers_4573/utils/patch_helper.py:
  48  def get_release_branch_name():
  49:     """Derive branch name from transformers version."""
  50      major, minor, *_ = transformers.__version__.split(".")

train_real_world/transformers_4573/utils/print_env.py:
  23  import transformers
  24: from transformers import is_torch_hpu_available, is_torch_xpu_available
  25  

train_real_world/transformers_4573/utils/tests_fetcher.py:
  296          # Add imports via `define_import_structure` after the #35167 as we remove explicit import in `__init__.py`
  297:         from transformers.utils.import_utils import define_import_structure
  298  

  541  # (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
  542: # \s*from\s+transformers(\S*)\s+import\s+([^\n]+) -> Line only contains from transformers.xxx import yyy and we catch
  543  #           .xxx and yyy

  547  # (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
  548: # \s*from\s+transformers(\S*)\s+import\s+\(([^\)]+)\) -> Line continues with from transformers.xxx import (yyy) and we
  549  # catch .xxx and yyy. yyy will take multiple lines otherwise there wouldn't be parenthesis.

  670                  # Add imports via `define_import_structure` after the #35167 as we remove explicit import in `__init__.py`
  671:                 from transformers.utils.import_utils import define_import_structure
  672  

train_real_world/transformers_4573/utils/update_metadata.py:
  41  
  42: from transformers.utils import direct_transformers_import
  43  

train_real_world/transformers_4573/utils/update_tiny_models.py:
  32  import transformers
  33: from transformers import AutoFeatureExtractor, AutoImageProcessor, AutoTokenizer
  34: from transformers.image_processing_utils import BaseImageProcessor
  35  

train_real_world/transformers_4573/utils/test_module/custom_configuration.py:
  1: from transformers import PreTrainedConfig
  2  

train_real_world/transformers_4573/utils/test_module/custom_feature_extraction.py:
  1: from transformers import Wav2Vec2FeatureExtractor
  2  

train_real_world/transformers_4573/utils/test_module/custom_image_processing.py:
  1: from transformers import CLIPImageProcessor
  2  

train_real_world/transformers_4573/utils/test_module/custom_modeling.py:
  2  
  3: from transformers import PreTrainedModel
  4  

train_real_world/transformers_4573/utils/test_module/custom_pipeline.py:
  2  
  3: from transformers import Pipeline
  4  

train_real_world/transformers_4573/utils/test_module/custom_processing.py:
  1: from transformers import ProcessorMixin
  2  

train_real_world/transformers_4573/utils/test_module/custom_tokenization_fast.py:
  1: from transformers import BertTokenizerFast
  2  

train_real_world/transformers_4573/utils/test_module/custom_tokenization.py:
  1: from transformers import BertTokenizer
  2  

train_real_world/transformers_4573/utils/test_module/custom_video_processing.py:
  1: from transformers import LlavaOnevisionVideoProcessor
  2  
